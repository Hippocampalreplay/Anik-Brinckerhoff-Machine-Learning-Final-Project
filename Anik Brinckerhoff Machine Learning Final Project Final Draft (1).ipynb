{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18fee450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import PIL\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from imblearn.over_sampling import(RandomOverSampler)\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dcc6ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data set is found here on Kaggle: \n",
    "# https://www.kaggle.com/datasets/tourist55/alzheimers-dataset-4-class-of-images\n",
    "\n",
    "from PIL import Image\n",
    "testImage = Image.open('Desktop/Alzheimer_s Dataset/test/MildDemented/26 (19).jpg')\n",
    "\n",
    "# I print out one of the images to see what the size of the image is in pixels and its qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f0660f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JPEG\n",
      "(176, 208)\n",
      "L\n"
     ]
    }
   ],
   "source": [
    "print(testImage.format)\n",
    "print(testImage.size)\n",
    "print(testImage.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5364b876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   4   3   1   0   0   2\n",
      "    4   3   5   5   0   0   1   0   0   3   0  11   0   0  11   0   0   3\n",
      "    0   0   1   2   2   2   0   0   0   7   3   0   3   3   0   0   3   0\n",
      "    0   1   0   7   2   0   7  22  39  48  44  29  11   0   0   0   1   2\n",
      "    1   1   1   2   2   0   8   0   7   1   8   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   2   3   2   0   0\n",
      "    0   0   0   6   5   0   1  12   1   0   0   4   0   0   0   0  23  72\n",
      "   92  99 105 105  96  84  74  69   7   1   0   0  11   7   0   4   6  12\n",
      "    8   0   8  44  52  68  64  70  80  91  99  97  86  75  78  77  66  39\n",
      "    8   0   0   2   0   0   4   0   2   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   3   1   0\n",
      "    0   0  18   7   0   0   0   3   0  12  27  77 111 118 108  75  61  81\n",
      "   75  85  96 102 101 100 103 107  89  92  88  88  88  61  19   0   0  69\n",
      "  106  86  92  92  57  53  88  88  90  94  96  92  84  77  62  69  78  81\n",
      "   70  47  18   0  18   6   0   0   0   6   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   2   0   0   0   2   2\n",
      "    4   7   0   3   0  17   0   0  15  82 117 137 140 124 101  67  46  52\n",
      "   45  45  44  42  44  52  64  73  81  97  97  93 103 104  83  65  55 110\n",
      "  140  93  58  58  59  76  79  75  70  64  53  39  30  28  34  31  35  49\n",
      "   65  70  61  50  55  42  14  15   0   3   0   8   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   4   0   0   0   2   0\n",
      "    0   0   0  23   0   6  14  91 105 119 140 158 167 156 128  84  43  25\n",
      "   35  33  34  39  48  56  60  61  84  92  98 106 108  88  67  66  39  64\n",
      "  122 113  67  74  94  97  93  73  54  46  42  35  26  21  32  31  30  27\n",
      "   27  35  49  59  56  66  58  54  14   3   0   5   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   3   0   0\n",
      "    0   6   0  79 114 118  85 104 112 136 164 166 157 126  86  57  42  35\n",
      "   13  27  53  83 106 112 102  91  75  67  88 137 150 107  79  94  52  39\n",
      "   95 118  96 124 154 145 120  82  45  36  43  43  34  26  19  23  30  35\n",
      "   37  33  26  20  35  52  65  77  65  49  13   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   2   0   2\n",
      "   30  61 123 117  93  85  83 101 130 172 173 169 157 125  84  56  40  26\n",
      "   30  55  95 134 158 157 141 126 111  79  67  98 123 103  86  98  47  45\n",
      "   99 129 129 153 160 147 134 104  73  57  46  34  28  29  38  30  24  30\n",
      "   42  48  42  34  54  69  91  98 111  96  60  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   1   2   0   0  15\n",
      "   72 127  88  71  44  46  99 144 156 150 172 170 170 158 129 100  64  27\n",
      "   35  63 107 148 173 179 173 166 138 115  87  93 126 134 106  81  48  72\n",
      "  119 136 146 164 164 168 142 137 127 105  67  36  36  53  57  60  60  55\n",
      "   47  44  47  52  97 129 160 134 125 101  95  72   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   9   1   0  10   1   2   1  16   0   0   8  86\n",
      "   81  64  50  31  27  60 106 138 153 158 165 174 177 167 149 122  84  51\n",
      "   32  61 107 141 159 183 202 201 173 121 110 109  94 117 122  69  72 102\n",
      "  129 141 159 184 193 186 169 161 153 139 106  76  78  99 126 126 113  93\n",
      "   70  47  51  78 109 137 158 155 143 128  98  67  77  51   5   0   0  12\n",
      "    0   0   0   4   5   0   0   2   3   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   6   0   0   3   2   0  19   0   0   0  12 105 103  86\n",
      "   55  85  46  33  33  57  89 115 135 150 152 159 163 160 151 131  98  68\n",
      "   44  53  79 106 130 163 192 199 200 154 131 113  91 104 111  78  93 122\n",
      "  149 164 181 203 211 206 187 175 165 153 124  99 111 141 170 170 156 132\n",
      "  102  70  63  81 123 149 166 159 143 125  97  70  54  74  71  61  11   0\n",
      "    0   3   0   0   2   4   0   0   0   4   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0  19   0   0  14   0   0  25   7  78  97  99  92 149\n",
      "  126  83  62  44  34  48  71  89 103 114 117 125 136 143 141 125  97  73\n",
      "   57  57  71  87  99 129 171 194 200 177 155 131 104  99  98  86 110 135\n",
      "  160 173 185 200 206 202 199 180 167 156 129 108 126 164 194 196 186 165\n",
      "  135  99  83  92 134 159 175 163 137 112  87  66  68  64  58  72  69  61\n",
      "   22   0   0   3   6   0   0   5   4   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0  18   0   0  16   0 115  97  89  80 116 125 146\n",
      "  139 149  97  63  34  38  57  67  64  60  61  73  93 107 108  96  81  72\n",
      "   78  87 108 114 104 119 164 199 205 200 180 152 125 103  98 105 138 159\n",
      "  179 189 194 201 205 203 188 166 150 140 118 103 127 169 201 205 196 178\n",
      "  153 117  96  99 131 161 182 168 130  93  67  54  58  53  52  67  80  84\n",
      "   68  53  11   2   0   0   5   7   1   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   1   7   0 125  94  68  82  94 127 152 178\n",
      "  162 144 122  83  46  37  43  43  37  32  32  42  58  67  62  56  65  80\n",
      "  114 126 148 153 142 152 187 213 222 221 201 173 147 117 108 124 146 163\n",
      "  185 197 201 204 207 210 190 166 145 132 119 117 150 191 216 218 206 185\n",
      "  160 124  97  94 121 156 187 178 133  83  53  43  49  52  57  52  52  57\n",
      "   75  89  60  13   0   2  10   0   0   7   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   3   0   0  19   0 115  97  75  58  85 118 154 174 182\n",
      "  167 151 125  96  64  45  36  36  46  59  72  70  68  58  39  33  60  94\n",
      "  150 152 164 173 179 197 213 213 189 194 195 191 173 136 112 116 127 147\n",
      "  176 197 204 204 207 213 217 196 171 154 146 154 183 213 219 221 206 184\n",
      "  161 127  97  89 114 150 189 192 151  96  58  45  47  38  35  24  34  37\n",
      "   54  60  75  57  23   0   0   2   5   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   3   0  19   0 102 106  89  58  85 120 151 163 176 186\n",
      "  175 152 125  98  68  52  50  64  93 120 136 124 111  93  66  56  84 124\n",
      "  177 180 188 194 203 222 221 199 148 147 161 179 170 136 108  98 112 135\n",
      "  172 202 209 202 199 204 214 209 198 188 187 195 207 214 215 217 202 181\n",
      "  161 128  95  83 114 144 184 201 176 123  76  54  32  34  45  28  30  19\n",
      "   38  45  54  82  68  11   0   7  13   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   2   0   0  80 105  94  70  71 101 121 157 186 204 192\n",
      "  166 140 131  96  61  53  71 102 140 167 164 153 144 132 110  99 124 161\n",
      "  194 209 224 222 220 229 219 188 153 131 131 146 139 119 103  95  89 114\n",
      "  155 188 192 177 167 168 175 189 201 208 217 225 221 209 222 223 208 185\n",
      "  162 127  89  72 116 139 177 205 193 144  91  62  57  44  47  36  54  41\n",
      "   43  29  43  64  78  61  20   0   0  10   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   8   0   0\n",
      "    4   4   0   6   0  20  64  90  84  64  58  65  99 128 154 186 213 198\n",
      "  165 156 143 118  78  68 103 143 165 178 183 177 169 160 151 151 165 180\n",
      "  202 216 231 229 224 230 220 192 148 122 100  98  98  93  93 100  92 111\n",
      "  148 179 175 146 126 125 144 162 190 211 215 210 211 217 218 216 213 190\n",
      "  149 116  89  64 105 139 172 194 199 171 123  92  69  55  48  58  72  75\n",
      "   71  68  74  54  61  87  72  18   0   0   0   3   3   0   0   4   3   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0  10   0   3   8\n",
      "    0   1   8   0  16  57  84  78  76  92 101  95 105 130 158 186 208 204\n",
      "  178 161 156 130  91  83 116 152 173 185 199 195 191 188 185 186 198 211\n",
      "  214 220 225 214 198 198 194 176 148 118  90  78  72  70  82  99  80  90\n",
      "  117 140 131 106 103 119 132 157 189 210 215 210 204 201 204 210 217 200\n",
      "  159 124  95  70  96 124 162 192 206 194 153 111  74  63  62  75  90  97\n",
      "   99 103 102  93  79  72  69  55  22   0   0   0   1   3   0   0   0   6\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   7   0  12   0\n",
      "    8   0   0  18  62  88  97  87  93 122 139 135 125 139 161 177 191 205\n",
      "  198 173 161 133  96  92 130 168 193 211 223 219 218 218 217 217 223 231\n",
      "  226 222 217 197 165 145 133 117 107  94  86  86  80  68  69  80  57  57\n",
      "   73  91  81  65  82 117 152 173 192 198 198 199 194 187 190 204 223 217\n",
      "  182 148 117  91  92 107 146 184 206 217 187 131  88  78  79  95 112 121\n",
      "  129 137 136 131 104  74  71  79  57  19   0   2   5   0   0   1   2   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   2   0\n",
      "    0   1   0  70  94  84  88 111 132 142 153 166 152 153 162 162 167 196\n",
      "  204 175 157 127  92  95 137 176 205 228 239 234 232 232 231 227 227 230\n",
      "  227 219 215 200 164 128  99  73  57  61  78 100 101  81  64  62  43  35\n",
      "   45  65  63  54  78 119 156 174 183 175 170 173 170 159 174 191 218 225\n",
      "  203 176 146 117 101  99 130 168 195 225 209 147 110  93  88 105 124 133\n",
      "  141 149 152 145 128 103  82  72  69  68   9   1   0   0   3   1   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   7   0   5   0\n",
      "    0  24  62  80  91  64  67 113 149 155 164 182 174 173 176 168 167 192\n",
      "  195 161 139 112  87 102 145 176 196 217 233 231 232 235 235 231 229 231\n",
      "  226 218 219 216 193 163 128  96  48  50  67  93 100  83  66  60  45  33\n",
      "   39  60  64  56  71 100 125 145 158 154 152 157 157 149 153 170 201 220\n",
      "  214 198 172 142 120 104 120 150 181 222 220 169 131 103  87 103 127 137\n",
      "  141 146 148 149 149 134 101  71  64  74  56   8   0   6  15   0   0  11\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  11   0   9\n",
      "    3  89  88  82  72  57  57  86 125 153 169 176 182 192 200 198 200 208\n",
      "  190 153 111  90  79 108 154 176 186 203 216 219 227 235 235 229 226 227\n",
      "  225 216 216 213 201 186 158 124  62  60  73  94  99  81  63  58  47  34\n",
      "   34  49  53  44  47  60  80  95 105 104 106 119 135 143 137 157 194 220\n",
      "  223 216 197 171 138 117 116 138 172 214 224 197 149 112  88 103 128 140\n",
      "  144 150 144 155 160 147 123  96  68  48  67  44  12   0   0   2   5   1\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  25   0\n",
      "  130  66 116  83  74  71  65  68  91 123 144 148 165 188 203 213 230 226\n",
      "  193 161 112  89  77 107 150 167 175 193 208 215 227 232 223 206 194 191\n",
      "  201 196 193 183 171 167 146 113  67  65  76  92  91  72  58  58  44  34\n",
      "   30  34  37  34  34  39  62  74  85  84  78  79  91 103 125 152 197 226\n",
      "  228 224 214 196 152 130 117 131 167 201 217 218 168 128 100 111 134 144\n",
      "  152 163 152 158 157 149 141 126  94  62  59  75  55   6   0   4   8   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   3   3   0 132\n",
      "  112 101  93 101  92  87  77  70  73  89 108 120 136 166 184 203 233 229\n",
      "  192 170 142 109  84 102 137 150 159 180 210 219 229 228 207 176 154 145\n",
      "  164 167 167 153 140 141 125  92  66  61  63  70  64  53  56  69  44  38\n",
      "   32  30  33  38  44  48  91 113 140 147 126  96  78  76 112 146 198 228\n",
      "  226 222 216 204 158 139 118 128 163 190 207 225 183 143 115 122 139 147\n",
      "  159 174 164 155 149 151 151 138 120 108  72  73  74  60  23   0   0   8\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0  15   0   0   4   4   0   4   5 122 109\n",
      "   78 127 108 121 135 137 116  99  95  76  64  78 120 145 166 202 222 217\n",
      "  209 186 161 132 101  86 113 138 149 187 213 229 233 219 191 146 110 101\n",
      "  135 128 120 114 108  96  78  65  50  42  41  48  50  52  64  81  59  32\n",
      "   23  38  41  28  36  61 112 144 165 169 163 139 108  93 121 155 195 220\n",
      "  225 217 203 192 152 135 114 111 138 182 214 225 203 152 114 122 146 158\n",
      "  167 178 177 165 152 148 149 148 145 143 119  90  63  54  80  20   0   1\n",
      "    1   0   0   6   5   0   0   3   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   4   0   9  13   0  15   0   0  96 108  98\n",
      "  116 119 132 139 150 155 143 134 130 102  71  64  94 133 163 198 220 224\n",
      "  218 189 160 136 105  80  98 125 142 180 206 226 234 221 186 130  83  67\n",
      "   67  75  88 101 111 114 108 101  84  65  47  40  36  41  63  89  65  42\n",
      "   30  38  43  40  53  76 115 149 176 184 178 151 117 102 121 157 199 221\n",
      "  220 207 191 180 161 142 118 114 140 182 212 223 216 179 137 122 137 164\n",
      "  183 191 170 163 158 157 160 160 157 154 131 109  77  79  59  60  19   0\n",
      "    0   6   5   0   0   7   7   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0  11   0   8   0   0  10   0  12   6 122  82  85\n",
      "  120 100 132 139 148 154 151 149 149 123  82  60  73 120 156 192 220 233\n",
      "  230 198 163 147 121  86  88 115 137 172 204 223 231 216 178 118  70  57\n",
      "   82  99 119 132 141 149 150 147 130  96  59  38  28  34  61  91  73  73\n",
      "   82  94  97  87  78  75 110 149 183 198 192 160 121 104 115 156 203 223\n",
      "  215 196 177 165 147 129 110 113 145 188 218 229 229 202 158 130 145 181\n",
      "  193 183 159 161 166 171 175 174 170 166 152 117  85  69  37  91  84  15\n",
      "    4   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0  15   0   0   6   0   0   0  69  83  88 106\n",
      "  114 116 136 141 137 143 145 145 148 138 112  91  81 113 141 186 223 237\n",
      "  238 213 175 162 142  99  89 116 141 171 210 223 223 206 169 117  85  85\n",
      "  127 146 159 159 158 165 169 167 160 118  76  57  50  50  65  85  74  99\n",
      "  130 151 154 138 106  79 100 143 185 207 203 165 121 101 109 154 204 225\n",
      "  214 188 164 150 130 117 109 123 160 200 226 236 233 210 175 158 176 199\n",
      "  188 158 161 171 181 188 191 190 183 176 162 126 124  86  72  80  74   0\n",
      "    6   0   0  10  10   0   0   7   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   6   0   4  14   0   0  15 103  62  90 113\n",
      "  114 134 135 127 137 145 152 150 150 155 149 132 107 110 123 177 222 233\n",
      "  236 223 191 170 147 102  87 119 153 183 218 225 220 199 164 119 100 111\n",
      "  138 161 177 175 176 187 190 183 162 124  95  96 101  92  84  86  76 107\n",
      "  140 157 164 158 130 100  96 137 182 210 209 171 125 103 118 161 209 228\n",
      "  215 185 154 135 129 124 129 152 186 215 232 237 232 217 201 199 207 205\n",
      "  184 162 184 195 205 208 209 206 197 186 169 142 134 104  96  65  49  13\n",
      "    0   1   5   0   0   4   5   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   8   0   9   0   0   0  15  98  85  86  78  95\n",
      "  122 128 120 104 135 147 162 158 150 160 162 143 128 113 112 170 218 227\n",
      "  232 224 202 165 138 100  89 130 172 203 222 229 224 203 165 119 102 118\n",
      "  150 178 198 198 201 209 201 182 147 120 113 138 153 135 108  96  89 120\n",
      "  152 167 174 169 142 112  99 132 172 203 210 178 136 118 146 179 215 229\n",
      "  216 186 151 128 125 129 147 178 209 228 236 239 231 227 228 231 221 204\n",
      "  195 197 208 215 218 214 212 209 197 183 176 161 124 122  92  73  56  86\n",
      "   12   3   0   0   0   0   0   2   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0  15   0   0   0   8  94 119  88 109  96 110\n",
      "  138 122 108 114 134 148 166 163 152 165 168 143 132 120 118 168 216 228\n",
      "  234 222 199 153 132 113 114 155 194 219 225 233 229 210 172 125 110 128\n",
      "  165 190 204 200 202 208 191 160 135 118 126 163 178 153 120 105  98 130\n",
      "  165 185 194 187 159 129 116 140 170 200 216 195 163 151 177 197 219 227\n",
      "  217 193 159 134 128 140 166 200 224 234 237 239 229 229 232 234 226 215\n",
      "  212 217 209 211 205 193 189 187 174 157 160 160 145 145  99  70  49  70\n",
      "   79  30   0   2   5   0   0   7   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   5   0  24 106 115  76 105  87 115 131\n",
      "  132 110  86 126 143 154 172 170 161 179 184 156 128 128 129 171 216 234\n",
      "  239 221 192 144 135 135 143 181 209 226 229 235 232 214 177 134 124 147\n",
      "  175 196 204 197 203 215 199 166 130 119 134 171 181 151 118 106  94 122\n",
      "  155 179 194 196 180 159 140 156 178 208 229 218 194 187 195 206 218 224\n",
      "  219 200 170 146 150 163 190 219 235 234 232 233 226 222 219 223 230 232\n",
      "  224 214 196 193 181 165 160 159 146 128 134 125 139 109  80  50  70  70\n",
      "  125  78  22   0   0   7   4   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   9   0  16   0 126 116  81 101 103 115 127 135\n",
      "  127 103 101 125 145 164 174 170 172 182 180 168 136 137 153 188 220 234\n",
      "  234 231 195 161 135 144 177 209 231 245 234 233 229 214 185 159 152 158\n",
      "  185 201 198 189 199 203 182 160 141 132 150 184 185 145 110 102 102 124\n",
      "  154 183 202 207 196 183 180 180 187 205 224 233 227 218 223 223 226 231\n",
      "  226 209 191 181 183 200 218 225 227 230 232 231 225 225 221 222 233 240\n",
      "  229 211 195 177 160 153 146 131 110  96 100 103 100  81  56  44  53  69\n",
      "   80  89   5   0   9   0   0   9   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0  21   0   0 106  87  89  73  95 113 127 131\n",
      "  121  98  94 111 132 154 172 176 178 185 189 188 152 150 172 212 234 231\n",
      "  226 231 217 191 167 171 195 221 237 244 234 229 223 212 195 180 178 184\n",
      "  195 199 189 186 203 207 183 160 135 158 190 207 186 143 110  97  99 124\n",
      "  155 180 199 212 219 220 209 204 203 210 222 231 231 227 230 230 235 242\n",
      "  239 225 209 200 216 224 230 229 228 231 234 234 220 222 221 221 227 231\n",
      "  217 199 172 161 151 145 132 105  74  55  62  63  62  53  42  37  44  53\n",
      "   54  83  91   0  16   0   3   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   5   0   0   3 111  76  72  82  98 121 133 128\n",
      "  116  95  84  91 114 136 164 180 184 187 198 208 188 182 200 234 245 231\n",
      "  227 240 238 225 210 206 218 235 241 238 236 228 221 215 209 205 206 210\n",
      "  208 196 176 176 202 212 194 178 166 192 211 200 169 138 114 100  95 121\n",
      "  152 171 185 203 225 240 227 221 215 217 224 232 234 234 224 224 229 236\n",
      "  235 224 213 208 233 235 234 229 227 229 229 227 217 223 224 221 221 218\n",
      "  202 183 153 144 135 124 104  75  47  32  39  45  53  61  66  66  62  58\n",
      "   61  79 103  20   0  16   0   3   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0  16   0  99  73  76  63  95 103 127 136 128\n",
      "  122 111  97  94 104 121 152 178 188 189 200 215 222 213 216 230 235 228\n",
      "  230 241 241 243 238 232 236 243 239 227 233 230 227 224 220 217 216 217\n",
      "  216 197 170 172 200 216 211 208 207 207 199 179 156 134 113  97  92 119\n",
      "  148 163 171 186 208 223 220 212 207 212 224 232 233 229 232 230 231 234\n",
      "  233 226 221 221 225 227 227 226 229 232 228 221 222 229 230 222 214 205\n",
      "  188 169 150 133 110  86  63  43  34  34  52  67  88 107 120 118 104  89\n",
      "   90  61  90   0   0   0   0   7   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   8   0 134  94  58  79  47  91 104 126 135 133\n",
      "  139 139 129 123 114 124 150 181 197 199 204 213 228 226 220 214 217 225\n",
      "  228 224 232 242 247 245 244 245 235 221 223 229 234 232 226 219 213 210\n",
      "  211 201 184 186 208 221 220 223 213 192 174 165 147 117  94  86  97 124\n",
      "  153 169 177 187 200 209 211 199 189 194 210 224 225 221 233 228 224 223\n",
      "  219 214 214 218 220 221 221 222 228 235 234 228 228 234 232 220 206 192\n",
      "  171 151 134 112  80  50  30  24  35  48  79  97 121 139 150 150 135 119\n",
      "   92  82  69   8   0   0  11   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0 104  99  64  71  63  74 106 129 141 144\n",
      "  151 151 145 145 141 146 165 193 212 215 213 215 222 230 227 215 217 230\n",
      "  228 215 225 233 241 243 243 239 230 221 215 225 232 231 225 218 209 201\n",
      "  200 209 208 209 222 226 220 219 203 171 148 140 118  84  75  89 109 135\n",
      "  164 182 190 200 211 218 220 202 184 181 194 210 218 219 224 219 214 210\n",
      "  205 201 203 209 221 219 213 211 217 227 233 233 229 234 231 216 199 179\n",
      "  152 128  95  78  57  40  34  41  61  80 101 117 133 145 154 159 151 138\n",
      "  101 106  56  81  16   0   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   3  10  88  53  90  61  55  69  94 124 146 153\n",
      "  155 150 150 162 169 175 190 210 225 229 226 221 221 230 234 228 227 232\n",
      "  233 228 225 224 227 232 232 227 223 223 220 224 225 223 220 216 205 193\n",
      "  194 215 222 220 228 232 223 216 204 169 141 131 111  84  80  97 122 145\n",
      "  169 180 185 197 214 226 232 219 201 192 194 204 215 221 225 223 222 221\n",
      "  217 212 214 219 215 215 210 206 210 219 226 227 228 233 231 217 197 172\n",
      "  136 106  70  61  54  52  57  67  86 101 113 126 140 150 164 173 165 149\n",
      "  127  90  80  84  88   0   3   5   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0 132  78  74  57  42  51  51  68 105 140 156\n",
      "  157 151 159 182 185 194 208 221 232 237 233 227 222 227 233 233 228 226\n",
      "  234 244 228 219 216 221 222 217 217 224 230 227 220 214 216 215 203 188\n",
      "  193 218 224 217 226 236 230 222 208 178 156 154 145 119  97  92 128 149\n",
      "  167 169 167 179 203 222 236 231 222 211 205 207 216 224 214 215 218 220\n",
      "  218 212 213 217 209 213 215 214 216 221 223 221 227 234 233 220 200 170\n",
      "  129  94  75  68  63  64  66  72  83  93 114 128 144 160 179 188 174 151\n",
      "  122  98  71  66  71  11   5   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4\n",
      "    0  14   0   0  17   0  24  92  75  63  55  32  53  53  55  79 111 135\n",
      "  150 162 175 185 196 213 229 234 236 238 236 231 228 225 225 227 228 228\n",
      "  233 240 235 226 215 210 212 217 220 221 224 224 220 217 219 219 208 194\n",
      "  195 208 225 232 226 217 214 216 215 196 177 167 156 137 120 112 133 150\n",
      "  159 159 155 150 165 195 228 232 232 227 221 217 215 214 205 217 226 225\n",
      "  221 219 215 209 216 228 221 216 223 214 207 222 228 229 229 226 215 187\n",
      "  143 107  91  79  68  63  63  64  69  75 106 116 136 165 191 198 179 155\n",
      "  135 128  96  57  61  25   0  13   0   1   0   3   0   0   4   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4   3\n",
      "    9   0   2   0   0   2  89 111  72  47  40  29  42  39  43  52  78 112\n",
      "  134 147 167 189 198 204 211 218 228 235 231 223 223 219 218 223 226 227\n",
      "  232 237 238 232 223 218 216 217 218 218 215 222 227 230 232 230 219 207\n",
      "  203 215 228 229 219 209 207 211 222 209 193 180 167 148 128 115 127 141\n",
      "  145 138 132 130 150 183 223 230 234 228 220 219 222 225 223 231 235 231\n",
      "  225 223 219 214 221 235 230 223 227 219 210 218 226 226 226 224 215 188\n",
      "  145 108  84  77  70  67  62  56  55  57 100 116 144 175 199 205 188 167\n",
      "  145 130 121  99  81  59  20   0   0   0   0   0   0   0   4   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   12   0  14   3   9  11 117  93  54  36  43  57  65  61  55  43  53  88\n",
      "  118 134 158 185 201 197 194 203 222 238 240 234 237 229 224 225 226 224\n",
      "  223 224 227 225 222 219 218 219 220 221 205 215 224 225 222 218 212 207\n",
      "  221 226 229 223 213 211 216 223 214 205 191 177 163 144 116  93  63  73\n",
      "   71  62  63  80 122 168 219 233 241 234 223 221 226 231 234 239 240 236\n",
      "  231 229 226 222 223 239 237 225 224 220 215 218 224 224 224 225 220 197\n",
      "  155 120  94  81  65  53  47  47  57  68 102 123 152 178 196 201 191 179\n",
      "  162 131 126 117  76  79  75   0   0   7   2   0   2   5   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3   0\n",
      "    0   0   0   0  17   0  92  55  44  49  73 103 109 108  95  71  59  80\n",
      "  112 137 157 173 178 171 164 169 187 210 228 236 246 236 228 227 226 222\n",
      "  217 215 220 219 218 217 216 217 219 221 215 223 228 226 219 215 217 221\n",
      "  229 226 219 212 213 223 235 241 219 210 197 184 175 155 117  83  27  45\n",
      "   53  55  67  92 137 182 215 232 242 235 224 222 226 228 242 238 229 218\n",
      "  209 202 196 192 205 227 235 225 217 217 219 220 224 224 225 228 228 212\n",
      "  177 145 117 102  82  69  65  73  92 110 136 155 177 192 200 204 202 199\n",
      "  178 143 126 121  74  76  87   1   0   6   7   0   4   7   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4   4\n",
      "    0   0   0   5   0  12  78  56  64  81 104 129 134 136 128 104  80  81\n",
      "  112 148 163 162 163 158 150 147 155 179 213 239 240 231 224 226 228 225\n",
      "  221 219 226 224 221 219 218 217 217 217 221 227 233 232 225 219 221 225\n",
      "  221 217 209 209 221 236 240 236 220 207 191 185 182 164 124  89  56  86\n",
      "  111 123 135 147 169 196 219 231 235 227 223 229 234 232 238 217 187 158\n",
      "  138 126 121 119 160 191 224 231 222 219 221 218 221 222 224 229 233 225\n",
      "  200 176 141 131 122 118 118 124 136 148 163 179 196 203 203 203 202 200\n",
      "  186 166 141 136 100  76  71  14   0   0   0   0   0   6   0   7   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    9   0   7  21   0  95  84  87  92 110 128 137 146 149 139 123  97  85\n",
      "  104 135 148 141 148 143 136 132 137 161 203 237 243 236 232 233 232 228\n",
      "  225 224 225 222 221 223 226 226 222 218 224 229 238 244 239 227 219 217\n",
      "  218 219 217 221 235 245 237 221 214 193 174 171 169 152 123 101 100 127\n",
      "  145 153 164 175 190 210 235 238 233 226 232 242 239 225 187 156 113  80\n",
      "   61  54  55  60 101 136 195 232 232 225 222 214 218 220 222 226 231 230\n",
      "  218 203 171 159 146 143 146 154 166 177 189 202 214 216 212 208 203 198\n",
      "  187 177 148 134 117  88  73  61  28   0   1   1   0   4   0   6   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    3   0   0   0   7  84  83 113 105 126 152 150 162 160 149 136 113  92\n",
      "   83  87  95  98  96  89  86  94 113 145 186 218 243 238 235 235 232 226\n",
      "  224 226 222 219 218 223 227 224 212 201 207 211 223 240 248 243 235 231\n",
      "  227 231 230 228 235 241 232 216 213 185 161 153 143 124 112 112 127 148\n",
      "  160 166 182 200 218 236 243 242 236 233 240 238 206 168 105  80  51  35\n",
      "   29  28  32  37  56  81 149 211 228 228 225 214 219 222 223 224 227 232\n",
      "  230 225 207 189 171 166 172 183 196 206 223 225 223 216 210 208 204 200\n",
      "  191 179 161 134 123 107  83  88  84   2   5   7   0   4   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   5   2\n",
      "    0  16   0   2 128  71  77 125 108 135 171 162 173 163 163 149 127  99\n",
      "   64  40  41  55  57  48  49  72 110 152 191 218 224 222 224 227 226 223\n",
      "  224 229 229 224 220 222 221 208 185 166 149 151 167 197 224 236 240 242\n",
      "  234 237 232 221 220 227 227 219 196 164 135 120 100  77  78  95 122 151\n",
      "  173 189 209 222 228 236 236 237 235 236 239 218 156  93  51  37  26  29\n",
      "   34  33  30  30  36  47 111 185 215 224 228 218 222 225 226 224 226 233\n",
      "  237 237 229 216 207 211 220 225 225 226 222 212 194 176 169 172 174 173\n",
      "  197 186 190 159 137 123  78  67 112   1   0   4   0   7   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16\n",
      "    0   3   0   8 104  99 103 112 130 154 172 178 177 175 164 150 133 116\n",
      "   99  86  87  94 101  99 101 109 123 148 185 216 218 218 223 229 228 221\n",
      "  220 224 228 216 218 221 198 162 127  99  74  68  67 100 158 202 229 249\n",
      "  249 246 235 228 212 221 242 221 203 161 119  97  80  66  71  88 118 158\n",
      "  179 200 232 230 220 238 241 235 238 246 229 170  93  40  37  35  32  28\n",
      "   26  28  33  37  31  44  81 150 210 223 217 222 222 226 228 228 230 234\n",
      "  233 229 220 227 232 232 232 233 231 227 202 166 127 113 116 126 139 150\n",
      "  166 183 195 185 157 123  97  84  82   0   0   0   3   1   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0\n",
      "    2   0  22   0  99  78 104 117 140 165 181 184 178 172 158 153 149 147\n",
      "  142 137 140 147 142 142 143 142 142 156 190 221 237 234 234 233 226 217\n",
      "  214 218 221 213 217 213 176 126  84  55  43  42  38  46  76 119 182 244\n",
      "  248 249 237 235 226 226 241 230 206 177 151 144 136 117 101  96 118 160\n",
      "  180 195 226 231 218 224 240 244 251 239 188 114  58  36  34  34  32  29\n",
      "   27  28  31  35  36  45  77 142 202 217 212 219 226 230 233 233 236 240\n",
      "  239 235 222 225 227 228 229 224 211 197 167 127  80  57  57  75 104 129\n",
      "  151 162 174 177 169 146 114  90  85   0   1   0   2   1   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0  17   0   4  82  94 105 122 149 174 189 190 181 172 158 155 152 151\n",
      "  147 144 146 151 149 154 156 150 143 153 187 219 235 234 234 233 228 224\n",
      "  226 232 230 224 223 204 150  91  51  29  26  32  31  28  28  47 110 184\n",
      "  229 248 241 241 240 231 236 234 227 203 179 168 162 147 126 111 117 161\n",
      "  185 197 224 233 223 223 244 252 251 217 143  68  34  35  32  32  32  31\n",
      "   29  28  30  32  42  47  74 138 200 219 218 226 223 227 229 230 234 239\n",
      "  237 232 227 227 228 227 225 211 183 158 117  87  53  35  30  39  60  81\n",
      "  116 132 153 167 168 154 124  99  89   2   2   0   1   1   0   1   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    7   0   0   0  85 111 109 127 152 173 187 192 186 177 175 169 159 151\n",
      "  145 143 145 148 151 157 159 154 150 166 199 229 222 226 230 231 230 231\n",
      "  237 244 243 234 225 191 125  68  43  34  34  36  37  37  32  28  60 111\n",
      "  186 238 247 244 244 233 231 226 238 224 202 182 165 147 128 114 120 163\n",
      "  195 209 221 222 226 241 245 248 227 171 102  52  33  32  30  32  34  34\n",
      "   32  30  30  31  37  40  68 135 202 226 226 235 219 222 225 226 231 235\n",
      "  232 226 227 229 229 223 211 186 151 122  90  70  50  41  35  30  34  42\n",
      "   72 102 135 152 152 142 123 107  91   4   3   0   1   0   0   2   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   6\n",
      "    4   0   0  19  77 106 116 132 152 167 180 191 192 187 176 173 167 161\n",
      "  157 155 154 153 151 153 153 151 156 176 205 226 224 230 235 232 227 225\n",
      "  228 231 237 228 215 175 104  50  35  35  38  35  30  32  32  26  42  76\n",
      "  132 215 248 244 243 237 231 214 214 221 224 211 183 151 126 112 129 165\n",
      "  202 213 203 194 214 249 239 239 199 122  61  41  35  26  30  33  36  36\n",
      "   34  32  31  30  27  31  61 131 203 227 225 230 222 224 226 228 234 238\n",
      "  234 226 224 228 226 210 184 152 118  94  84  61  38  30  28  25  26  31\n",
      "   42  73 106 126 137 139 128 114  91   4   3   0   1   0   0   2   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  10\n",
      "    0  12  22  96  80 103 120 136 151 161 173 187 192 190 181 184 185 181\n",
      "  176 169 156 145 145 148 150 154 167 189 211 222 227 235 239 232 223 219\n",
      "  220 222 227 219 209 171 100  45  28  27  26  32  33  33  31  23  29  51\n",
      "   87 177 231 244 243 243 238 210 188 200 214 212 186 152 129 122 137 167\n",
      "  197 197 175 171 203 241 241 233 178  91  35  30  34  28  29  32  35  36\n",
      "   34  31  30  30  29  33  63 135 207 229 222 223 217 220 222 226 234 240\n",
      "  235 227 229 232 222 194 156 121  93  76  64  44  28  27  29  27  26  29\n",
      "   33  50  70  92 121 140 131 109  89   2   2   0   1   1   0   1   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0\n",
      "    8   0 110 113 100  97 116 133 149 157 166 178 182 178 189 196 202 203\n",
      "  202 196 183 168 173 181 188 194 205 220 231 234 226 235 238 229 220 220\n",
      "  224 226 229 220 210 175 107  53  35  31  23  34  35  31  33  28  25  33\n",
      "   59 131 199 240 246 245 244 214 177 174 179 185 172 144 125 123 134 162\n",
      "  181 169 155 179 219 239 249 214 144  70  31  29  35  33  28  30  33  33\n",
      "   31  29  28  28  35  36  65 137 210 231 221 220 207 210 214 221 232 239\n",
      "  236 227 234 230 210 170 126  91  68  56  42  33  30  36  38  32  28  31\n",
      "   32  41  50  62  89 112 108  89  85   0   1   0   2   1   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   5   0\n",
      "    9   6 123 117  75  89 108 128 146 153 161 170 170 163 160 171 183 193\n",
      "  205 215 214 206 205 216 225 226 226 229 229 225 231 239 240 228 218 219\n",
      "  226 229 235 223 210 175 111  61  44  42  32  33  19  11  25  37  37  38\n",
      "   46  96 170 236 248 244 246 218 171 157 158 173 170 140 113 105 127 155\n",
      "  168 148 149 202 246 248 252 187 104  50  34  34  33  31  26  28  30  30\n",
      "   28  26  26  26  32  33  61 133 206 228 218 216 202 205 210 219 233 242\n",
      "  239 231 231 220 191 145 100  66  45  35  40  33  31  34  32  28  32  42\n",
      "   31  43  48  46  55  73  77  67  82   0   0   0   3   1   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    2   0 115  99  77  57  88 115 125 145 166 182 191 172 164 156 168 189\n",
      "  201 212 220 219 228 224 226 232 233 228 228 233 236 239 231 225 226 221\n",
      "  220 230 223 235 214 171 100  41  36  38  31  27  23  24  30  36  38  38\n",
      "   35  54 142 210 247 233 244 236 198 168 144 164 162 122 104  96 102 131\n",
      "  140 142 176 220 241 243 235 166  78  34  31  29  25  32  35  35  34  32\n",
      "   29  29  31  33  32  29  64 142 211 232 221 208 208 206 208 215 227 236\n",
      "  239 238 219 194 160 124  84  51  44  53  39  37  35  36  34  31  31  33\n",
      "   34  37  44  42  40  54  64  57  74  23   0   0   5   7   0   3   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    2   0 115  97  77  58  73 101 114 135 157 176 192 180 175 161 166 186\n",
      "  202 216 223 218 230 233 229 220 222 233 236 232 234 238 230 223 225 221\n",
      "  220 229 224 233 209 167  99  41  35  37  30  30  30  30  29  30  32  34\n",
      "   33  43 117 192 244 233 243 247 237 202 154 141 119  83  75  71  71  92\n",
      "  115 146 191 227 241 246 225 147  68  34  28  26  27  31  36  36  34  31\n",
      "   27  26  27  29  28  27  67 149 216 235 226 220 233 230 228 230 235 237\n",
      "  234 230 205 177 139 104  71  48  49  61  59  58  55  52  50  48  42  36\n",
      "   45  41  46  50  50  58  62  51  76  64  40   1   2   3   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    2   0 113  93  73  57  72 102 116 135 153 171 194 189 181 176 187 199\n",
      "  201 210 224 227 219 220 199 170 175 210 230 226 232 238 232 224 224 221\n",
      "  221 230 228 234 210 168 100  41  35  38  30  34  36  34  29  26  28  31\n",
      "   36  37  86 163 236 237 240 254 252 226 171 130  90  60  66  68  58  77\n",
      "  118 173 219 239 244 246 211 124  58  40  31  30  38  37  33  33  32  30\n",
      "   27  26  28  29  26  31  79 163 223 235 228 229 238 236 235 237 238 236\n",
      "  231 226 197 166 125  91  66  53  59  71  89  94  94  89  87  86  76  63\n",
      "   68  49  43  45  45  49  51  44  70 104 100   1   0   0   1   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    2   0 108  84  65  52  76 108 126 145 159 175 202 204 198 196 207 216\n",
      "  214 218 223 219 199 181 141 104 112 160 202 217 229 239 236 228 228 226\n",
      "  225 233 231 239 217 177 105  41  35  40  33  34  36  34  31  30  32  34\n",
      "   36  37  58 125 211 238 236 243 239 237 199 150  95  62  74  80  74 104\n",
      "  160 212 238 246 244 236 186  97  47  45  34  33  44  41  29  29  30  29\n",
      "   28  29  33  35  27  43 101 182 231 232 225 229 223 225 229 236 240 240\n",
      "  236 232 204 176 138 108  87  75  74  80 104 117 126 123 122 123 116 103\n",
      "   97  65  44  37  28  29  36  38  59 112 124   0   1   0   3   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0\n",
      "    2   0 102  75  56  47  69 104 125 146 158 173 204 212 218 212 218 227\n",
      "  227 224 211 189 171 137  91  63  71 111 161 195 218 235 237 231 233 232\n",
      "  229 235 228 240 225 189 116  46  34  38  37  33  30  30  34  38  40  40\n",
      "   32  39  40  88 174 233 236 232 234 249 227 182 113  63  69  78  92 141\n",
      "  207 238 239 247 244 221 147  69  34  42  34  31  41  37  29  30  31  30\n",
      "   30  31  34  37  28  59 127 202 237 231 223 228 221 225 231 237 241 240\n",
      "  236 232 215 196 170 147 126 108  95  89 102 121 137 140 138 139 136 130\n",
      "  123  92  69  54  36  29  35  38  57  97 118   0   6   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3   0\n",
      "    3   0  99  69  51  45  67  98 116 134 142 154 186 197 210 220 236 230\n",
      "  204 186 177 164 128  93  60  51  56  76 117 157 194 218 230 229 234 235\n",
      "  232 234 224 236 227 202 135  60  38  33  38  33  28  29  35  40  42  42\n",
      "   33  44  39  68 132 215 239 239 237 249 237 206 137  71  69  82 115 170\n",
      "  235 250 239 251 246 205 110  54  31  42  40  34  37  35  34  34  34  32\n",
      "   30  29  31  34  33  80 154 217 240 233 226 229 229 232 235 235 232 225\n",
      "  217 212 214 208 198 185 168 146 123 109 112 126 143 153 153 149 148 148\n",
      "  144 125 113 100  79  65  57  48  66  82 108   0   8   1   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0\n",
      "    3   0  99  68  52  49  66  92 104 120 128 141 178 193 204 215 226 208\n",
      "  168 145 136 127  80  58  43  45  50  60  91 126 162 194 214 221 231 234\n",
      "  230 231 223 231 226 215 162  86  49  31  37  35  33  32  33  35  36  37\n",
      "   35  43  37  56  85 173 226 247 243 242 235 231 176 102  89 102 152 191\n",
      "  242 252 238 248 230 171  78  48  32  41  47  38  32  33  34  34  33  31\n",
      "   28  28  30  32  49 106 180 226 237 232 229 229 228 230 231 228 221 211\n",
      "  202 197 200 205 211 210 200 181 158 143 137 138 150 169 176 169 165 168\n",
      "  162 153 150 143 127 113  93  67  72  74 105   1   1   3   0   2   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n",
      "    3   0 101  69  55  53  56  80  93 112 126 147 192 212 223 202 183 169\n",
      "  155 142 114  77  51  41  37  42  47  60  89 118 139 175 202 214 227 232\n",
      "  229 228 226 230 225 225 183 108  61  32  34  37  38  36  31  28  29  31\n",
      "   31  33  29  42  44 127 201 242 253 242 238 254 213 132 108 118 183 201\n",
      "  239 248 234 235 203 132  55  42  29  36  47  36  23  26  30  30  30  29\n",
      "   28  28  31  34  64 127 197 231 233 230 228 227 222 224 226 225 219 210\n",
      "  203 199 187 199 214 221 217 204 185 171 155 146 153 179 193 186 179 182\n",
      "  173 168 167 162 154 145 119  83  70  70 106   7   0   2   0   5   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  78  62  56  63  73  80  95 114 129 152 189 222 205 174 151 149\n",
      "  143 112  73  48  52  56  71  84  80  74  92 121 134 150 174 197 215 225\n",
      "  226 224 225 233 231 230 215 148  71  40  30  37  36  36  41  35  27  31\n",
      "   33  22  27  36  40  76 151 214 249 242 237 245 234 185 141 135 196 223\n",
      "  243 240 243 244 186 102  38  35  33  33  35  35  33  30  35  30  27  33\n",
      "   35  22  23  46 101 168 222 227 218 227 238 238 229 228 223 211 196 183\n",
      "  176 174 183 185 193 205 212 207 195 187 177 172 182 199 205 207 200 185\n",
      "  186 181 179 171 160 151 125  88  79  75  85   5   5   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  78  64  64  75  91 103 111 121 151 187 200 191 156 153 143 118\n",
      "   85  62  60  68 100 109 124 135 128 113 111 120 141 150 165 184 201 210\n",
      "  209 205 211 222 224 230 227 171  90  45  37  43  40  33  33  31  29  33\n",
      "   39  34  33  34  31  47 107 172 234 239 235 240 246 213 172 163 191 226\n",
      "  246 243 247 233 164  87  41  38  35  34  34  33  30  27  33  28  26  35\n",
      "   39  28  38  69 140 194 233 228 215 222 231 230 241 233 219 203 189 178\n",
      "  171 168 167 168 175 190 204 209 205 199 201 198 210 222 218 209 193 172\n",
      "  175 163 156 148 135 126 108  82  65  72  88   5   1   0   1   3   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    1   0  78  69  77  95 113 123 132 147 177 202 193 167 145 143 127  92\n",
      "   62  61  89 117 138 147 159 164 159 143 125 113 122 124 134 152 176 196\n",
      "  208 214 210 218 219 226 235 200 122  61  31  37  35  25  22  29  35  38\n",
      "   35  38  33  33  33  31  70 138 214 242 240 237 252 234 194 182 195 232\n",
      "  243 238 243 212 134  71  38  36  34  34  34  34  32  30  29  26  26  36\n",
      "   39  33  59 106 174 212 236 226 216 226 238 238 243 234 219 204 191 180\n",
      "  172 168 162 161 165 180 201 215 218 215 220 218 225 226 209 191 171 149\n",
      "  152 141 142 142 125 103  80  59  54  71  93   4   0   0   3   3   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "    1   0  80  76  91 114 128 133 151 178 191 183 168 160 143 111  77  65\n",
      "   72  94 124 147 156 161 163 162 162 155 132 108  88  90 101 121 149 180\n",
      "  211 232 227 227 219 219 234 221 154  85  33  36  36  28  20  29  38  36\n",
      "   27  34  27  31  41  29  49 110 180 235 248 238 249 237 204 197 210 239\n",
      "  237 228 234 190 107  59  32  32  32  33  35  36  36  36  26  26  28  34\n",
      "   33  35  80 145 199 221 233 223 219 231 243 244 236 235 230 221 209 197\n",
      "  189 185 176 173 174 184 203 219 221 216 227 222 219 204 176 155 137 117\n",
      "  113  98 100 105  86  59  40  28  52  73  94   2   0   0   1   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    1   0  82  81  98 123 138 146 165 184 184 166 149 143 114  79  60  78\n",
      "  107 127 150 171 180 179 173 166 165 159 133 103  70  79  97 118 137 161\n",
      "  194 222 233 230 222 219 232 231 178 108  53  43  42  38  26  30  36  27\n",
      "   29  34  23  28  43  30  32  74 134 208 247 244 246 237 220 219 226 243\n",
      "  234 228 227 170  86  47  31  32  33  34  34  35  36  37  24  28  29  30\n",
      "   30  44 103 176 222 232 234 223 219 227 235 235 235 238 237 229 217 209\n",
      "  207 208 200 199 197 199 209 219 215 205 229 221 211 184 146 119  98  75\n",
      "   84  60  52  53  39  27  28  33  54  70  88   0   0   3   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   1  83  81  95 116 141 157 166 161 160 156 134 106  76  77  96 126\n",
      "  142 150 171 199 193 189 180 172 167 154 123  92  74  90 115 137 145 154\n",
      "  178 204 224 224 226 225 231 235 197 136  70  39  32  37  29  32  39  29\n",
      "   32  34  26  30  45  41  38  57 103 171 231 247 242 235 231 232 236 241\n",
      "  236 231 211 143  67  36  34  36  36  35  33  32  32  34  23  30  30  29\n",
      "   41  72 134 200 231 233 230 223 221 227 233 234 238 235 228 216 206 204\n",
      "  209 214 220 223 220 217 220 224 218 207 225 218 206 175 134 103  76  48\n",
      "   56  37  35  42  36  31  35  38  57  64  79   0   0   6   5   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n",
      "    0   0  82  76  82  99 133 148 152 139 129 121  99  73  67  94 125 144\n",
      "  154 167 187 202 189 182 173 169 165 152 127 106 105 116 136 153 155 157\n",
      "  176 200 223 224 232 230 225 233 216 171 100  44  24  34  29  33  43  35\n",
      "   31  33  33  35  42  48  51  52  74 119 190 234 234 235 241 236 244 236\n",
      "  232 220 174 101  48  32  35  36  37  36  32  31  32  33  23  30  28  33\n",
      "   66 114 169 215 232 230 226 221 223 230 238 243 245 234 220 210 207 208\n",
      "  210 210 217 224 226 221 222 228 226 216 223 216 204 176 141 119  97  70\n",
      "   62  54  61  71  65  57  48  37  62  63  76   0   2   3   3   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3   1\n",
      "    0   0  80  70  72  84 123 128 135 131 107  75  60  60 101 126 142 147\n",
      "  167 201 214 205 185 174 166 167 168 163 151 142 146 145 152 158 157 158\n",
      "  177 201 233 231 237 230 218 230 229 199 145  69  35  40  31  31  41  34\n",
      "   33  35  39  35  30  40  47  38  44  68 146 214 228 240 254 244 250 232\n",
      "  224 202 136  66  35  33  31  34  36  35  33  32  34  37  23  30  26  37\n",
      "   86 147 194 223 240 235 228 222 222 228 237 243 254 240 226 221 225 226\n",
      "  220 212 198 209 214 211 214 224 226 220 229 220 206 181 156 148 138 117\n",
      "  128 117 116 116 107  98  88  72  68  66  79   0   1   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3   0\n",
      "    4   2  73  71  70  52  76  78  77  69  60  59  68  78 127 136 148 171\n",
      "  194 197 194 202 188 184 179 175 172 169 167 165 167 166 165 164 162 165\n",
      "  179 195 220 228 233 230 226 225 228 231 176 111  50  32  33  29  27  33\n",
      "   36  35  33  33  34  37  39  41  41  43  82 159 221 241 242 244 252 241\n",
      "  220 170  97  47  35  36  33  33  30  27  24  25  28  31  32  29  31  61\n",
      "  118 167 198 220 228 224 223 227 230 231 235 239 242 226 218 220 209 191\n",
      "  192 207 213 213 213 214 215 217 223 229 223 213 208 195 173 160 154 144\n",
      "  142 148 152 146 139 135 130 126  99  98  76  10   0   0   3   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4   0\n",
      "    1   2  77  73  66  47  46  51  54  54  58  73  97 116 138 153 169 188\n",
      "  203 201 199 208 197 195 191 188 184 180 176 173 169 167 166 162 159 166\n",
      "  187 208 230 229 226 224 223 224 223 221 194 137  75  47  39  33  31  34\n",
      "   32  32  32  33  35  38  40  42  33  35  54  99 157 205 231 240 242 225\n",
      "  191 129  65  39  40  38  35  34  32  29  27  27  30  32  35  30  40  76\n",
      "  127 178 213 228 229 221 216 220 226 230 235 240 223 208 197 192 181 166\n",
      "  163 171 189 189 193 200 208 215 219 221 221 211 209 202 186 177 170 157\n",
      "  164 165 161 154 148 145 139 132 112  60  28   0   0   0   3   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4   0\n",
      "    0   2  81  73  63  48  66  79  93 100 103 110 126 139 152 176 198 212\n",
      "  214 196 179 181 176 175 173 171 169 168 167 166 174 171 166 157 151 158\n",
      "  183 208 235 226 217 216 221 222 218 212 204 158 100  59  39  32  30  31\n",
      "   29  30  32  34  36  39  40  41  34  38  38  49  95 163 211 225 217 195\n",
      "  151  88  40  36  44  37  35  35  34  32  30  30  32  34  36  29  53  93\n",
      "  133 184 221 222 217 214 216 224 229 228 225 225 227 213 197 184 170 154\n",
      "  143 140 149 148 149 160 180 200 213 219 222 214 216 217 208 204 198 186\n",
      "  195 191 184 176 172 166 152 138 117  17   0   0   3   0   1   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0\n",
      "    0   1  81  69  62  61  97 112 129 136 135 136 143 151 176 194 204 206\n",
      "  201 180 160 158 163 160 154 147 143 143 146 149 147 146 143 140 139 151\n",
      "  181 207 226 217 211 214 221 221 217 213 204 173 122  72  42  33  32  31\n",
      "   29  30  32  34  36  37  38  38  40  45  40  37  66 123 170 188 181 152\n",
      "  112  68  38  37  41  32  34  35  35  33  31  30  32  33  33  30  68 116\n",
      "  148 197 231 221 217 217 220 225 224 220 221 225 226 215 196 173 149 125\n",
      "  103  88  92 101 115 135 160 185 202 208 217 213 219 223 217 218 220 213\n",
      "  214 208 199 192 188 179 157 136 113   0   0   4   4   0   0   3   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2\n",
      "    2   1  79  62  62  80 112 122 135 144 152 164 181 195 198 197 183 170\n",
      "  168 158 146 146 128 121 109  95  85  83  87  92  84  87  97 112 131 157\n",
      "  190 216 211 207 209 218 223 220 218 219 211 191 148  95  56  40  37  36\n",
      "   31  32  33  34  35  35  35  34  39  40  38  39  52  82 119 144 144 105\n",
      "   73  55  38  31  32  28  30  32  34  33  31  30  30  31  28  35  83 133\n",
      "  163 206 233 218 219 219 220 217 212 213 226 241 233 223 203 172 134  96\n",
      "   65  46  58  84 117 144 165 183 193 194 208 207 215 218 212 216 226 225\n",
      "  221 211 194 182 176 169 152 134 119   0   2   7   1   0   1   1   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4\n",
      "    2   0  77  56  59  90 117 129 147 164 178 188 196 202 187 183 164 149\n",
      "  146 133 111 100  67  64  55  44  34  30  33  38  44  48  64  95 133 168\n",
      "  198 218 207 206 212 222 224 218 217 222 218 203 167 117  70  43  34  33\n",
      "   33  33  33  33  33  33  33  33  37  34  34  38  41  53  83 113 114  72\n",
      "   45  40  31  24  27  29  27  30  33  33  31  29  29  29  28  44  88 129\n",
      "  156 189 205 190 195 203 215 221 216 212 218 228 245 237 219 187 139  92\n",
      "   62  51  91 115 140 155 170 189 205 211 213 209 215 218 214 220 230 230\n",
      "  225 210 186 164 155 154 149 142 130   3   0   0   0   5   1   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   5\n",
      "    0   0  79  53  51  86 112 127 151 174 186 185 174 166 164 164 150 137\n",
      "  136 121  92  75  73  77  79  75  65  57  53  52  48  47  62 100 147 184\n",
      "  207 217 219 214 217 224 223 215 212 218 220 206 176 132  82  44  29  29\n",
      "   32  32  31  30  31  32  34  35  34  34  35  36  34  41  66  91  93  62\n",
      "   42  37  31  31  34  30  26  29  33  34  32  30  29  29  31  57  92 125\n",
      "  157 185 194 188 190 199 214 226 226 217 212 213 236 230 220 195 147 100\n",
      "   84  91 140 155 166 171 182 205 223 229 227 217 219 224 225 231 235 227\n",
      "  217 205 182 157 144 143 146 146 121   0   0   0   0   5   0   2   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   4\n",
      "    0   0  81  52  45  78 119 130 149 170 186 189 181 172 158 155 136 120\n",
      "  124 124 113 108 121 130 137 135 121 103  88  81  62  57  69 110 163 203\n",
      "  222 227 234 223 219 224 222 212 208 212 220 207 182 143  93  49  32  33\n",
      "   31  30  29  29  30  33  36  38  26  34  38  33  28  36  55  71  81  64\n",
      "   52  44  38  44  45  31  26  30  33  35  33  31  30  29  34  67 100 134\n",
      "  178 212 224 228 226 219 215 218 221 222 224 227 232 228 225 209 167 126\n",
      "  123 144 149 168 187 197 210 225 228 221 234 219 218 225 231 237 233 217\n",
      "  200 195 179 157 140 136 137 138 101   0   6   1   3   0   0  11   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   11   0  62  57  59 103 132 142 161 184 200 198 181 165 153 147 134 122\n",
      "  123 135 142 142 142 146 150 141 132 136 128 103  87  56  58 115 176 204\n",
      "  214 222 223 220 223 215 193 188 203 214 219 215 197 158 105  59  35  29\n",
      "   30  29  30  32  34  35  33  31  29  35  35  36  39  37  49  75  70  60\n",
      "   46  37  41  47  41  28  30  34  32  33  39  32  25  32  41  92 136 163\n",
      "  193 213 212 208 210 214 217 218 221 225 228 228 232 227 220 204 179 156\n",
      "  149 154 167 184 206 219 217 210 209 213 232 223 220 220 221 229 224 205\n",
      "  200 195 183 158 137 141 141 124  80   2   6   0   0   2   1   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    6   0  70  76  74 112 137 150 173 196 207 199 178 161 145 142 135 131\n",
      "  138 150 155 153 169 172 173 161 148 147 137 112  95  62  53  96 159 201\n",
      "  222 233 222 218 221 213 192 187 201 211 225 223 207 170 117  69  41  32\n",
      "   35  34  34  36  38  38  36  34  31  37  37  38  41  39  51  77  70  59\n",
      "   42  31  32  39  37  29  26  30  27  27  31  25  22  32  67 109 144 168\n",
      "  201 225 227 222 204 212 219 220 217 217 221 224 229 227 223 212 191 171\n",
      "  164 168 184 199 217 225 220 211 208 210 236 230 231 233 232 237 233 215\n",
      "  210 205 193 165 139 133 125 104  76   0   4   0   1   3   1   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   2  58  79  85 124 143 161 188 209 213 200 179 164 154 154 154 160\n",
      "  172 183 183 177 195 195 193 178 160 154 141 117  98  69  52  80 142 201\n",
      "  232 240 224 220 223 219 201 198 212 221 226 226 214 180 130  82  50  36\n",
      "   35  34  33  34  36  35  33  31  31  36  35  37  41  39  51  76  76  64\n",
      "   46  32  30  36  38  35  29  34  31  28  31  27  29  44  74 112 143 166\n",
      "  200 224 223 215 206 215 223 223 216 211 213 218 230 231 230 223 207 191\n",
      "  186 189 208 218 229 231 224 217 215 215 221 218 224 228 224 227 223 208\n",
      "  214 211 202 177 147 134 119  94  75   0   2   0   3   3   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   6   4\n",
      "    0   0  23  52  84 132 146 166 193 211 213 201 187 177 181 181 185 195\n",
      "  208 213 208 199 205 203 199 182 159 149 135 112  89  69  52  68 128 195\n",
      "  228 231 222 217 222 221 209 209 224 232 221 224 213 180 135  90  57  39\n",
      "   36  34  33  33  34  34  32  30  30  33  32  34  38  37  47  72  79  67\n",
      "   50  37  33  35  38  39  34  39  35  31  32  30  37  56  87 130 164 187\n",
      "  218 237 231 221 218 220 224 224 218 212 213 218 235 236 236 229 218 209\n",
      "  208 211 219 225 229 225 220 220 221 222 221 218 225 229 222 222 221 211\n",
      "  211 210 208 191 165 150 132 106  81   0   1   0   3   2   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4   6\n",
      "    0   0   0  18  77 130 145 164 189 204 207 203 198 196 207 206 209 218\n",
      "  226 225 217 210 213 209 204 186 160 146 132 112  92  79  61  66 117 185\n",
      "  220 221 217 210 215 216 206 208 221 227 223 225 214 181 137  96  61  39\n",
      "   40  38  36  36  37  37  36  34  31  34  31  33  38  36  45  68  76  64\n",
      "   48  38  34  33  33  34  32  36  31  26  28  27  36  55  94 139 175 194\n",
      "  218 234 231 225 225 218 215 218 220 218 220 224 235 237 235 229 222 222\n",
      "  226 231 223 226 223 214 211 217 221 222 224 218 222 222 212 208 209 202\n",
      "  204 207 212 204 182 166 147 122  92   3   2   0   1   0   0   2   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    7   3   0   0  75 118 141 159 182 199 204 205 206 208 219 216 216 221\n",
      "  225 223 220 219 224 219 216 201 175 161 150 133 127 117  97  93 131 191\n",
      "  225 227 221 213 215 215 206 206 216 219 229 232 220 187 146 105  67  40\n",
      "   38  36  33  33  35  36  35  34  36  37  33  34  39  37  45  66  77  62\n",
      "   48  41  39  36  33  33  31  34  29  26  29  28  35  53  92 134 164 176\n",
      "  197 216 220 220 223 212 206 213 219 220 223 229 231 233 231 225 222 228\n",
      "  236 240 232 234 228 217 215 222 226 223 226 214 212 208 192 183 179 171\n",
      "  188 194 209 210 192 174 151 125  99   6   1   0   1   0   0   3   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    8   3   0   0  79 101 135 154 180 199 207 208 207 207 207 202 201 205\n",
      "  208 210 216 224 226 224 226 218 199 190 185 173 168 164 149 141 164 207\n",
      "  232 232 229 219 220 220 211 211 218 219 227 233 225 196 158 119  78  47\n",
      "   33  31  29  29  31  33  33  32  39  39  33  34  38  36  42  63  79  61\n",
      "   45  41  41  39  35  34  34  36  30  28  34  33  36  50 106 144 170 183\n",
      "  206 226 228 224 223 212 209 215 218 215 218 226 230 233 231 223 220 226\n",
      "  231 232 235 239 235 226 226 233 232 224 233 217 210 202 180 162 150 138\n",
      "  153 166 193 206 195 176 150 122  98   3   0   0   2   0   0   2   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   8   0\n",
      "    4   0   0   0  82  89 132 152 181 203 212 210 204 201 184 179 178 182\n",
      "  186 193 207 222 221 221 228 227 215 212 213 205 186 188 182 176 190 217\n",
      "  229 225 230 219 220 221 214 214 221 222 219 229 226 202 168 131  88  55\n",
      "   35  33  31  31  34  36  36  36  38  37  31  31  36  32  38  58  77  56\n",
      "   37  33  36  34  31  30  33  34  28  28  35  34  34  46 109 146 174 191\n",
      "  219 236 230 220 227 220 218 222 218 209 211 221 233 236 233 223 218 220\n",
      "  222 219 227 233 233 228 230 236 232 221 221 202 192 182 156 131 112  96\n",
      "  119 138 175 200 195 178 151 123  93   0   0   0   4   0   0   1   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0\n",
      "    1   0   5   0  56  92 124 159 192 204 207 204 181 153 142 131 130 143\n",
      "  157 171 193 214 212 215 217 214 209 210 217 225 220 215 210 210 212 216\n",
      "  221 225 221 224 222 214 211 215 221 224 223 224 220 199 172 148 107  58\n",
      "   37  31  26  28  34  36  31  25  25  29  30  29  30  39  50  57  66  61\n",
      "   48  34  27  29  32  31  32  37  33  29  25  20  33  61 122 159 177 176\n",
      "  192 215 228 234 221 222 223 220 215 209 205 203 229 234 226 218 222 219\n",
      "  209 206 204 213 224 232 232 227 219 214 207 205 196 175 142 104  71  53\n",
      "   84 139 180 191 197 186 152 122  92   0   0   6   1   5   0   4   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    2   0   7   0  65  87 124 160 193 201 194 176 142 110  92  74  76 106\n",
      "  140 163 188 212 217 217 217 216 217 220 224 227 231 223 216 215 219 222\n",
      "  224 224 224 226 223 216 214 218 222 222 212 208 199 180 161 150 119  75\n",
      "   36  32  28  30  34  36  33  29  26  29  30  28  28  34  44  51  64  59\n",
      "   48  35  31  35  38  37  28  33  31  28  28  25  41  71 122 155 175 181\n",
      "  198 215 220 221 220 217 215 215 216 215 212 209 218 221 212 206 214 215\n",
      "  207 205 210 214 220 223 222 218 213 209 210 212 208 186 148 106  73  55\n",
      "   83 130 168 180 183 173 146 124  88   3   0   1   0   4   0   8   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    3   0   8   0  80  84 130 167 201 204 182 145  99  62  47  44  63 104\n",
      "  140 164 191 217 218 215 214 216 222 226 227 227 228 222 215 214 218 222\n",
      "  222 220 224 224 220 215 216 220 220 217 215 208 196 176 159 149 118  73\n",
      "   35  33  31  32  34  35  35  33  27  29  31  30  28  32  41  50  61  56\n",
      "   43  31  28  32  35  33  25  30  28  27  28  28  46  77 130 158 178 193\n",
      "  215 225 220 217 221 216 212 212 217 222 222 220 216 216 204 198 208 211\n",
      "  204 202 207 211 218 223 224 222 218 215 189 199 205 194 164 129 102  88\n",
      "   73 109 150 173 177 163 133 107  60   1   1   0   0   5   0   5   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    3   0   7   0  89  87 126 162 197 199 169 123  75  44  40  70 110 140\n",
      "  157 174 197 217 209 209 210 213 218 222 225 226 213 212 210 207 209 214\n",
      "  216 216 220 219 215 212 215 220 219 214 216 214 208 188 165 147 110  62\n",
      "   36  35  34  32  32  32  33  33  29  31  33  33  31  33  44  56  66  58\n",
      "   44  29  25  28  29  27  26  30  28  25  26  27  46  78 134 157 177 198\n",
      "  224 234 227 222 225 221 217 215 216 221 226 229 230 228 212 203 211 213\n",
      "  204 201 208 213 221 228 231 228 221 215 204 210 213 203 178 144 115  98\n",
      "   67  85 121 153 163 146 105  64  20   0   0   1   0   7   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "    1   0   4   0  83  94 121 151 180 177 142  97  61  43  64 110 149 159\n",
      "  167 187 202 202 201 205 210 211 211 215 222 229 209 216 217 211 208 212\n",
      "  217 218 217 217 214 212 216 221 221 216 212 216 215 196 170 150 113  67\n",
      "   38  37  34  31  30  30  30  31  33  33  35  35  31  32  44  59  77  67\n",
      "   49  33  28  31  32  29  27  32  29  27  29  30  50  81 125 149 166 185\n",
      "  214 231 229 227 225 227 226 219 211 212 221 230 242 240 225 214 220 220\n",
      "  211 208 222 223 224 227 228 225 218 213 213 213 212 208 194 170 142 123\n",
      "   89  75  82 102 112 105  66  16   0   0   0   0   0   6   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   2\n",
      "    0   0   1   0  56  95 132 149 161 146 106  65  48  49 105 136 157 161\n",
      "  177 204 210 196 201 206 211 211 210 214 224 233 224 232 233 225 220 224\n",
      "  226 224 220 220 218 215 217 222 224 222 229 228 221 199 173 155 121  77\n",
      "   40  36  32  30  30  31  30  30  36  33  34  34  29  28  42  61  83  70\n",
      "   48  31  26  31  33  31  27  32  31  31  33  35  54  85 117 147 162 171\n",
      "  198 224 232 231 222 228 230 221 208 204 214 225 237 241 231 223 229 228\n",
      "  221 220 232 228 222 221 222 223 222 220 200 199 201 207 209 198 178 161\n",
      "  130  91  64  52  47  55  37   0   3   3   1   0   0   1   1   1   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   3\n",
      "    0   0   0   5  19  89 116 123 124 104  69  46  56  79 140 154 168 180\n",
      "  197 214 216 208 207 208 208 209 211 217 226 233 235 239 237 230 231 237\n",
      "  235 225 223 225 222 216 214 219 222 223 240 231 217 195 174 159 121  72\n",
      "   41  35  29  29  33  36  35  33  33  30  31  33  30  31  50  73  92  75\n",
      "   50  29  23  28  31  28  27  33  32  31  33  32  49  79 116 153 168 164\n",
      "  185 219 233 234 221 227 229 223 212 208 215 223 225 235 233 229 235 234\n",
      "  229 231 231 225 219 216 215 212 206 200 191 193 199 206 209 200 183 169\n",
      "  160 122  88  53  26  36  34   0   2   6   0   0   2   0   0   2   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   3\n",
      "    0   1   0   8   0  82  69  74  76  64  44  44  80 121 155 166 187 208\n",
      "  215 212 214 222 212 208 204 205 211 220 227 231 234 234 229 225 234 244\n",
      "  238 223 225 227 224 215 210 213 218 220 225 214 200 185 173 161 117  60\n",
      "   41  33  27  28  35  41  40  36  29  26  29  34  34  39  61  88 106  87\n",
      "   58  33  25  29  31  28  29  34  32  30  28  25  40  68 114 158 172 160\n",
      "  176 212 230 229 223 227 229 226 219 217 220 225 219 233 235 233 238 237\n",
      "  233 236 228 224 219 214 205 188 168 153 156 166 182 199 208 206 198 190\n",
      "  168 146 126  85  41  45  46   8   0   1   0   2   8   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0  57  38  30  44  62  80 111 142 167 186 206 215\n",
      "  217 217 211 203 208 186 179 191 206 223 232 226 231 227 225 221 222 236\n",
      "  240 223 222 225 226 224 217 212 209 209 213 199 180 169 169 155 110  63\n",
      "   31  31  30  29  29  29  30  30  26  25  27  31  31  39  64  90 106  80\n",
      "   47  27  23  28  30  29  30  23  31  31  33  32  33  67 122 147 166 168\n",
      "  177 203 225 233 229 231 231 224 210 202 210 223 228 230 230 231 238 245\n",
      "  242 232 232 229 202 180 178 163 143 145 140 141 148 164 184 200 207 207\n",
      "  195 150 143 109  54  44  68   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0  72  51  40  59  93 122 143 154 174 200 223 223\n",
      "  208 192 182 176 155 147 151 164 182 211 232 231 226 224 226 225 225 237\n",
      "  237 219 221 223 224 222 217 213 211 212 210 196 176 166 166 153 109  64\n",
      "   37  36  35  33  31  30  29  29  31  31  35  39  38  41  62  86  99  75\n",
      "   44  25  23  29  32  32  29  23  30  29  32  31  32  65 115 145 168 172\n",
      "  181 204 222 226 228 230 232 228 217 208 212 221 231 234 234 230 230 233\n",
      "  231 225 216 200 175 159 152 132 110 103  98 112 132 151 166 183 201 214\n",
      "  199 163 153 122  79  62  74   6   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0  71  56  53  77 118 151 166 170 207 216 218 202\n",
      "  178 159 148 144 133 133 144 155 168 196 222 227 230 229 234 235 234 240\n",
      "  236 217 221 222 224 224 222 222 223 225 219 203 182 171 169 154 111  69\n",
      "   38  38  37  35  34  32  31  31  30  31  37  41  38  38  55  76  91  69\n",
      "   41  24  23  30  34  35  28  23  30  28  32  32  31  62 113 147 175 180\n",
      "  187 208 222 222 223 225 228 230 226 219 219 223 226 231 233 230 226 226\n",
      "  226 225 208 171 142 130 113  92  75  63  50  67  92 117 138 160 180 194\n",
      "  190 170 157 129 100  67  60   5   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0  78  70  80 113 146 167 185 199 213 202 186 170\n",
      "  155 137 118 103  80  77  99 132 155 182 215 233 233 231 237 239 235 237\n",
      "  231 214 216 217 219 221 224 227 231 233 227 210 187 175 170 150 106  66\n",
      "   32  33  33  34  34  34  34  33  27  28  34  39  37  38  55  76  87  67\n",
      "   41  25  23  29  34  35  30  25  32  28  33  34  32  59 112 148 175 177\n",
      "  183 205 221 220 221 219 222 228 229 226 225 227 225 229 232 232 229 225\n",
      "  221 217 198 145 112  97  69  49  45  38  35  39  52  78 112 141 160 167\n",
      "  170 165 146 114  93  43  25   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0  86  72  85 127 159 168 180 200 177 170 160 151\n",
      "  135 108  78  57  46  33  59 113 146 166 198 226 225 222 228 232 228 230\n",
      "  228 215 212 212 212 214 217 220 223 224 217 199 179 169 162 139  95  55\n",
      "   28  29  30  32  32  32  31  31  28  29  33  38  38  42  62  86  87  68\n",
      "   44  27  24  28  32  33  32  29  34  30  35  37  33  58 109 146 170 168\n",
      "  173 199 219 221 226 221 219 223 226 225 224 225 227 226 228 232 232 225\n",
      "  212 203 178 129 100  84  52  33  34  32  43  37  34  48  78 114 143 159\n",
      "  156 154 124  84  65   9   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0  87  66  71 112 150 160 163 171 155 150 139 117\n",
      "   86  55  38  34  43  24  47 101 135 153 183 209 219 215 224 231 229 232\n",
      "  234 225 219 217 216 216 217 216 215 213 201 185 170 166 160 135  89  51\n",
      "   30  31  31  32  31  29  27  26  26  25  29  33  33  39  62  87  89  72\n",
      "   48  31  26  28  31  32  33  31  36  30  35  39  34  55 104 144 170 167\n",
      "  174 204 227 230 235 227 221 221 222 220 218 219 214 213 218 229 235 230\n",
      "  218 210 180 149 126 113  91  72  67  66  56  51  42  37  46  74 111 138\n",
      "  137 130  94  55  42   0   0   6   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0  69  62  65  88 120 141 144 140 129 116  96  70\n",
      "   44  28  32  44  31  23  44  85 119 152 188 208 220 217 228 237 236 237\n",
      "  238 231 224 222 221 221 221 219 215 211 194 181 170 170 164 136  88  50\n",
      "   31  32  33  34  33  31  29  28  25  24  27  31  29  33  54  78  91  74\n",
      "   51  35  28  30  32  34  33  31  36  29  35  40  33  52  89 135 169 170\n",
      "  180 212 234 235 235 228 223 223 222 218 217 218 210 210 217 230 236 232\n",
      "  227 226 202 186 162 148 145 133 121 120 100  97  88  75  66  67  78  89\n",
      "   92  83  59  41  41   0   0   9   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0  19  36  46  48  65  94 106  99  80  65  49  44\n",
      "   43  45  53  63  71  72  83  99 120 159 194 203 221 219 230 240 236 234\n",
      "  233 225 219 218 218 219 221 220 215 211 191 179 171 171 165 133  83  44\n",
      "   29  31  34  36  37  36  35  34  31  30  33  35  32  33  52  74  91  75\n",
      "   53  37  30  31  34  36  32  31  35  28  34  39  32  50  69 120 161 168\n",
      "  180 212 232 230 230 226 224 225 225 221 220 222 225 225 230 236 234 226\n",
      "  224 228 215 205 172 156 168 165 151 153 160 154 145 135 120  97  69  49\n",
      "   46  40  33  38  49   4   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   1  55  44  36  48  20  45  40  40  42  48\n",
      "   62  86 113 132 134 131 127 128 140 165 194 214 224 232 236 234 234 237\n",
      "  236 231 225 217 214 212 212 225 231 220 206 186 172 171 162 125  75  41\n",
      "   35  34  33  36  38  38  35  32  31  34  36  33  31  38  56  73  87  67\n",
      "   44  32  32  36  35  31  37  38  36  31  32  37  40  39  57 113 155 158\n",
      "  167 202 228 229 226 235 231 226 229 227 223 229 228 232 231 228 231 235\n",
      "  227 214 213 215 211 201 195 193 188 180 170 172 167 159 153 140 107  73\n",
      "   40  33  39  43  47   9   5   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0  43  27  28  41  41  48  46  47  57  81\n",
      "  111 136 147 149 147 147 148 153 165 185 207 222 220 227 231 229 229 232\n",
      "  232 229 218 217 224 227 224 230 232 219 207 183 167 166 152 111  64  37\n",
      "   34  32  31  33  35  36  34  32  31  32  35  35  31  35  55  77  87  71\n",
      "   50  37  33  34  33  31  35  36  34  30  30  34  35  33  50 103 150 162\n",
      "  172 198 216 217 220 225 220 218 228 232 235 244 228 234 234 229 228 231\n",
      "  227 218 215 219 220 217 215 213 207 199 200 199 190 178 169 155 122  91\n",
      "   45  35  32  18  19   0   2   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   2   7  45  53  66  82 108  98 104 102 107 121\n",
      "  140 153 155 152 172 173 175 178 183 190 199 205 222 227 230 228 228 231\n",
      "  232 231 220 222 232 234 225 226 228 219 218 190 172 169 147  97  52  35\n",
      "   32  30  29  30  32  33  33  32  28  26  30  34  31  32  54  81  92  78\n",
      "   58  43  36  33  33  32  33  36  35  31  31  33  33  30  40  89 142 170\n",
      "  184 198 209 211 227 230 223 221 228 227 224 230 214 224 227 220 215 219\n",
      "  221 218 226 229 230 228 224 217 206 196 174 176 175 171 169 157 128  99\n",
      "   46  39  34   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   4  30  89 105 123 150 136 135 137 138 139\n",
      "  144 158 179 194 207 208 209 209 208 206 205 204 225 228 230 230 229 231\n",
      "  233 234 232 229 232 227 213 214 222 221 227 196 177 173 144  85  42  30\n",
      "   32  31  29  29  30  32  33  34  30  25  29  35  33  34  58  88  97  80\n",
      "   58  42  35  35  34  33  32  36  37  35  35  37  35  31  34  77 133 174\n",
      "  194 203 211 217 232 236 231 230 231 218 201 200 200 214 221 213 207 212\n",
      "  218 220 217 215 211 206 198 187 173 162 143 148 153 156 155 141 111  84\n",
      "   41  42  48   7   0   0   2   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   6  11 107 122 144 158 156 161 163 163 162\n",
      "  165 178 199 216 210 212 214 216 218 218 216 215 224 225 228 229 229 229\n",
      "  232 235 236 229 228 223 210 210 221 222 218 189 172 168 137  78  37  29\n",
      "   34  33  32  31  31  31  33  35  37  31  32  37  38  42  65  93  95  75\n",
      "   49  34  31  34  35  34  29  34  36  35  36  39  38  34  38  72 124 169\n",
      "  194 204 214 223 225 228 226 227 227 210 193 193 204 218 226 219 213 217\n",
      "  222 223 209 201 190 181 172 160 148 141 140 138 134 130 122 104  76  53\n",
      "   37  41  56   9   2   4   3   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   3   7   0  85 122 152 162 176 194 192 192 196\n",
      "  202 204 201 197 209 209 210 213 215 216 214 212 219 220 225 231 233 232\n",
      "  235 240 231 221 223 226 220 220 225 222 205 180 164 158 130  81  45  36\n",
      "   35  36  36  34  32  31  32  34  34  31  30  33  40  54  77  97 101  82\n",
      "   57  38  32  33  34  34  27  31  34  33  35  38  38  35  45  73 117 158\n",
      "  185 200 213 222 225 226 220 217 213 195 183 190 208 220 227 223 219 222\n",
      "  221 217 202 185 165 150 137 122 110 104  92  84  75  69  65  57  43  31\n",
      "   40  37  53   1   0   3   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0  26 104 135 150 163 171 173 180 190\n",
      "  199 201 197 193 214 212 211 211 211 209 204 200 203 205 213 224 229 230\n",
      "  234 240 231 216 215 224 226 227 228 221 198 175 155 143 119  81  50  39\n",
      "   35  37  38  36  32  30  29  31  27  29  30  34  52  82 109 122 131 118\n",
      "   94  68  46  35  33  35  30  34  35  34  35  39  39  36  48  74 112 148\n",
      "  178 201 216 224 228 229 222 214 202 177 164 174 201 212 219 220 223 226\n",
      "  220 209 188 166 141 123 107  89  76  71  55  49  45  47  53  54  50  46\n",
      "   49  36  51   0   0   1   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   5   4   2   0 101 124 140 143 141 150 158 158\n",
      "  154 156 167 177 187 186 185 186 187 184 178 173 182 184 195 209 217 219\n",
      "  224 231 238 216 209 217 222 226 227 219 193 171 146 127 103  72  45  31\n",
      "   35  38  39  37  32  28  27  28  27  33  37  45  73 116 146 156 164 156\n",
      "  135 100  64  40  34  37  36  39  39  37  37  40  40  38  47  73 109 144\n",
      "  176 205 223 230 222 226 224 218 202 170 153 161 194 204 213 219 228 234\n",
      "  225 211 199 178 154 139 126 111  99  96  89  85  85  91  96  92  84  77\n",
      "   55  38  55   3   0   5   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   7   0   3 103 143 136 142 146 140 136 133\n",
      "  129 129 140 155 151 150 149 148 149 154 161 166 163 171 172 174 182 187\n",
      "  201 224 228 235 230 217 216 227 229 219 199 165 144 133 103  66  42  30\n",
      "   22  26  31  35  36  37  37  37  31  27  40  81 129 163 180 186 197 189\n",
      "  179 161 122  73  44  37  33  41  33  31  43  37  28  39  39  63 103 137\n",
      "  168 207 228 220 226 231 232 216 184 155 147 155 178 205 228 234 234 234\n",
      "  221 203 202 194 180 165 152 143 138 136 130 135 141 144 142 137 131 128\n",
      "  106  68   4   2   0   3   1   1   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0  27   0   3   0 108 145 139 145 147 141 136 131\n",
      "  123 115 117 124 123 126 130 134 136 135 132 130 138 146 146 149 157 164\n",
      "  179 203 218 230 232 220 217 226 229 222 205 166 143 134 110  75  49  33\n",
      "   30  30  32  35  37  36  31  27  33  61 104 147 182 204 211 210 217 215\n",
      "  213 203 173 126  83  59  35  37  34  36  42  38  34  39  42  66 104 136\n",
      "  167 206 230 225 222 226 229 219 189 157 151 161 194 213 231 236 236 235\n",
      "  227 216 189 188 188 187 185 179 172 166 168 165 161 160 161 160 156 152\n",
      "  122  57   8   3   0   2   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0  18 133 130 149 151 148 145 143\n",
      "  137 127 121 121 127 130 134 141 145 143 136 130 127 133 131 132 140 146\n",
      "  161 184 212 228 236 228 223 229 231 227 202 160 134 128 108  76  48  28\n",
      "   33  31  29  32  36  37  33  28  55 110 168 199 214 225 227 220 220 224\n",
      "  227 226 212 176 121  77  43  31  31  36  35  35  38  35  37  61  98 127\n",
      "  156 197 225 225 227 224 225 218 190 160 158 175 208 217 228 233 232 228\n",
      "  226 224 204 205 208 214 219 217 208 200 207 204 202 203 201 188 166 149\n",
      "  121  23   0   0   0   2   0   3   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   9  17   0   2   7 139 163 155 155 154 154 155\n",
      "  152 145 137 132 145 141 140 144 150 153 150 146 140 143 139 137 143 147\n",
      "  158 179 211 224 232 228 227 231 229 222 196 155 129 123 102  70  42  24\n",
      "   33  31  29  30  34  41  49  53 119 171 215 224 223 232 234 226 220 223\n",
      "  224 225 227 208 156 105  66  36  28  34  30  35  41  31  31  56  93 120\n",
      "  146 188 221 226 235 225 222 218 195 169 172 194 216 217 223 228 226 220\n",
      "  220 224 227 222 218 222 228 230 226 219 216 215 215 215 208 188 160 139\n",
      "  115   0   0   0   0   2   0   4   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   8   0   0   0 140 163 179 179 179 179\n",
      "  178 175 169 163 171 166 163 164 169 172 169 165 160 164 159 158 163 164\n",
      "  171 189 214 219 222 221 225 229 223 213 193 156 133 125 101  67  42  26\n",
      "   35  35  32  28  29  43  66  85 134 173 208 218 222 231 233 226 240 239\n",
      "  233 229 236 233 197 155 103  58  34  33  29  35  41  30  35  62 100 124\n",
      "  146 187 223 231 231 223 224 227 213 194 195 211 225 225 229 233 229 222\n",
      "  221 226 228 221 215 217 222 223 219 213 197 191 183 175 167 158 146 139\n",
      "  107   0   5   0   3   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   8   0   0   0   9   0 152 150 179 184 189 192\n",
      "  195 200 201 198 197 197 199 202 203 199 191 184 184 190 188 189 195 194\n",
      "  197 211 223 222 219 219 225 230 225 216 192 155 134 126 102  69  45  30\n",
      "   34  34  30  23  26  49  86 115 106 136 177 210 227 232 233 234 248 247\n",
      "  241 235 239 240 220 194 137  90  51  38  31  29  31  30  44  73 112 133\n",
      "  149 186 222 231 219 217 221 230 229 220 218 223 235 239 241 239 233 229\n",
      "  228 228 226 223 219 218 215 207 194 184 169 161 148 136 127 118 111 107\n",
      "   64   0   1   3   0   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0  14   8   6   0   7 146 159 151 163 174 179\n",
      "  186 196 202 203 202 206 213 217 216 211 203 197 208 214 215 218 224 218\n",
      "  216 226 227 224 219 217 220 226 227 224 196 156 133 129 111  81  55  38\n",
      "   32  32  30  31  46  82 129 164 154 167 199 230 231 213 212 228 225 233\n",
      "  239 239 238 235 224 212 172 135  87  60  48  33  30  44  59  90 129 147\n",
      "  157 188 221 230 216 212 210 212 220 227 229 227 234 242 242 232 226 227\n",
      "  226 222 219 218 216 212 203 188 172 161 143 129 109  94  86  78  68  60\n",
      "   12   0   0   3   0   2   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0  10   0   4   0  10   1 128 142 157 171 183 187\n",
      "  191 198 204 204 211 216 223 227 227 225 223 222 219 226 227 230 234 225\n",
      "  219 226 219 218 214 209 209 215 222 226 206 163 137 137 125  98  70  49\n",
      "   36  35  38  50  79 127 180 216 236 227 233 238 205 158 153 181 202 219\n",
      "  239 248 244 235 227 223 202 176 126  90  72  46  40  66  76 108 147 162\n",
      "  168 196 226 234 223 213 196 188 199 219 229 228 226 236 234 219 211 217\n",
      "  218 211 198 198 197 193 186 175 166 159 127  99  63  42  39  45  46  44\n",
      "    0   7   0  10   0   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   1   0   7   0  14  98 135 146 165 173 173\n",
      "  177 176 171 175 176 202 219 218 220 227 216 195 185 202 223 233 229 220\n",
      "  213 212 216 212 209 211 218 227 233 235 209 182 155 143 134 112  78  53\n",
      "   44  39  53  87 132 184 214 215 231 236 243 223 160  98  94 126 141 180\n",
      "  218 232 234 239 242 240 235 228 204 163 123 100  94  94 108 125 147 168\n",
      "  191 214 229 236 229 217 204 200 207 218 227 231 237 243 244 235 221 210\n",
      "  201 194 178 183 183 175 169 167 162 156 125  66  56  23  35  50  31   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   6   0   0   5   0   0  52 103 127 149 163 168\n",
      "  171 165 161 168 163 192 217 223 223 222 211 195 188 201 218 228 229 226\n",
      "  224 225 225 221 218 219 223 227 229 229 216 194 170 155 141 121  96  80\n",
      "   58  67  98 137 173 210 230 227 226 222 212 182 124  71  65  87 104 133\n",
      "  170 197 216 228 232 232 236 241 238 218 193 170 154 143 133 148 170 190\n",
      "  209 222 225 222 231 223 214 212 219 227 232 233 238 238 231 215 199 186\n",
      "  174 165 158 162 162 160 160 160 153 143 116  79  52  29  39  41  43   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   4   0   1   4   0   0  18  80 111 134 155 169\n",
      "  178 174 174 187 184 206 229 239 236 230 224 219 210 216 225 232 236 239\n",
      "  241 243 232 229 226 225 226 227 227 226 221 207 188 170 150 130 118 114\n",
      "  112 129 164 194 210 225 232 223 229 208 175 133  85  51  46  57  65  79\n",
      "  109 147 180 202 217 227 237 242 244 238 229 218 205 194 167 179 194 207\n",
      "  218 225 222 215 234 228 222 222 228 232 233 231 239 231 215 195 179 167\n",
      "  153 141 137 137 138 143 153 157 149 138 108  91  51  33  42  33  51   1\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   5   0   5   4   1  53  98 120 142 163\n",
      "  183 189 196 212 222 227 236 242 240 236 237 243 231 230 231 233 237 240\n",
      "  241 242 233 230 226 223 222 223 224 225 224 219 209 192 168 149 147 153\n",
      "  183 193 216 229 229 231 230 217 218 187 141  98  68  54  49  48  53  57\n",
      "   79 112 138 158 186 213 232 234 234 232 232 229 220 209 196 204 209 209\n",
      "  211 215 219 219 227 221 216 216 220 223 222 219 222 210 190 172 161 153\n",
      "  139 126 113 108 108 119 138 149 146 137 109  93  59  31  41  35  40   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   6   0   0   7   0  10  89 110 128 147\n",
      "  174 191 203 217 231 224 222 227 230 230 233 238 235 231 227 226 227 229\n",
      "  230 230 233 229 222 216 213 214 218 221 222 225 224 214 195 181 185 196\n",
      "  222 224 236 242 238 237 231 215 180 154 117  90  78  70  56  42  55  65\n",
      "   89 114 124 133 165 201 230 235 239 240 240 237 225 212 212 219 221 213\n",
      "  205 206 212 217 214 209 204 205 209 212 209 205 191 179 162 149 145 140\n",
      "  126 110  86  77  73  87 110 127 132 130 115  86  70  29  40  39  17   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   2   2   0   0   8   0   0  84 113 133 146\n",
      "  172 197 209 217 222 217 216 222 229 230 228 227 234 230 224 222 223 226\n",
      "  230 232 235 230 221 212 207 207 212 217 214 218 223 222 212 206 212 223\n",
      "  229 228 236 241 235 230 218 196 151 134 117 111 108  93  65  41  51  72\n",
      "  108 136 144 152 183 217 236 238 236 229 227 229 230 227 217 226 230 222\n",
      "  212 207 208 208 211 209 209 212 214 210 199 190 165 159 148 142 141 136\n",
      "  120 102  67  57  52  64  84 103 115 122 116  84  78  34  43  32   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   1  10   0  56 105 139 149\n",
      "  174 203 218 221 220 222 223 224 227 229 227 223 224 219 213 208 209 215\n",
      "  224 231 232 229 223 215 210 210 215 219 214 214 216 219 218 217 222 230\n",
      "  228 227 235 237 227 220 207 187 156 141 132 135 132 108  75  53  52  74\n",
      "  108 137 153 169 196 221 231 232 225 212 205 212 226 234 218 223 226 224\n",
      "  220 218 213 207 212 215 220 224 219 200 174 155 131 131 130 130 132 126\n",
      "  107  88  58  50  46  54  68  83 100 113 108  91  77  48  49  12   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   2   0   0  10   0   0  11   0  13  81 129 142\n",
      "  166 200 217 219 219 225 225 217 213 217 219 217 203 198 190 182 182 190\n",
      "  204 214 227 226 224 219 216 216 221 226 226 220 217 220 222 222 225 230\n",
      "  227 228 236 236 227 224 218 204 175 156 143 144 136 110  81  66  66  78\n",
      "   97 117 136 157 179 196 213 223 228 220 210 210 217 223 218 217 216 216\n",
      "  222 226 222 213 207 213 222 226 213 181 140 112  90  95 100 106 110 103\n",
      "   83  63  52  45  43  49  57  68  86 103  98 101  73  60  55   0   0   4\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   7 115 153\n",
      "  180 200 199 206 211 214 219 222 218 205 188 176 163 166 162 159 158 150\n",
      "  156 177 193 215 223 217 212 206 207 219 220 216 211 209 213 220 228 233\n",
      "  232 229 226 227 230 228 219 209 196 188 167 148 134 107  77  66  84  72\n",
      "   75  89 104 125 143 144 159 193 215 209 208 223 232 226 219 218 216 217\n",
      "  220 223 218 211 211 221 230 224 200 163 127 105  74  63  62  67  64  63\n",
      "   54  37  36  32  32  37  43  52  70  89  94  85  77  53  13   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   8 105 151\n",
      "  149 174 201 214 213 212 210 203 190 174 157 147 145 142 136 140 147 139\n",
      "  134 143 168 202 223 222 220 215 211 215 215 215 216 219 222 224 225 225\n",
      "  218 221 224 225 225 223 219 215 201 193 172 153 139 112  84  75  90  79\n",
      "   71  68  77 104 120 112 148 160 182 205 213 215 225 238 232 227 222 222\n",
      "  222 219 213 208 209 216 223 219 200 171 144 128 127 109  97  86  72  65\n",
      "   57  42  39  33  29  31  33  38  51  64  72  69  61  35   6   1   7   2\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  15 114 145\n",
      "  166 190 226 187 210 206 197 183 165 148 136 129 128 128 123 124 125 113\n",
      "  105 114 147 195 228 231 231 227 216 211 209 213 220 228 232 230 224 219\n",
      "  213 218 222 221 217 213 213 215 195 187 167 151 137 111  86  82  99  86\n",
      "   70  56  62  94 110  98 134 143 173 210 224 218 219 231 242 231 225 226\n",
      "  223 213 207 207 214 218 221 216 203 184 165 155 150 141 140 135 118 104\n",
      "   88  69  59  49  39  34  33  34  40  46  52  49  38  13   0   4   8   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0  15   0   1 120\n",
      "  152 172 177 207 189 185 176 162 147 134 127 125 122 127 122 110  96  80\n",
      "   86 111 142 195 229 229 229 228 215 204 209 213 220 229 234 233 227 221\n",
      "  217 213 207 201 195 190 192 196 176 169 151 137 125  98  78  79  99  80\n",
      "   67  66  78 106 125 121 147 164 185 197 203 208 212 214 238 226 221 225\n",
      "  222 211 207 210 223 225 225 220 208 192 176 167 148 147 157 162 151 140\n",
      "  126 108  89  74  55  42  38  38  39  40  37  24  12   0   0   2   5   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  12\n",
      "  134 160 184 165 168 164 155 143 131 122 117 115 113 112  97  81  70  66\n",
      "   86 121 152 198 223 219 221 224 216 207 215 214 216 222 229 232 231 228\n",
      "  216 196 175 165 162 162 166 171 164 157 141 128 112  83  64  69  92  68\n",
      "   68  92 111 128 147 156 178 182 180 168 160 168 188 205 226 219 218 223\n",
      "  223 217 215 219 223 226 227 224 212 194 175 162 158 151 153 153 145 142\n",
      "  140 130 111  93  66  45  37  36  36  34  30   4   0   0   0   2   5   1\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0  10   0\n",
      "  128 135 159 158 164 155 142 129 118 110 103  98  89  75  52  47  60  77\n",
      "  104 136 176 209 224 219 224 230 228 225 222 217 212 214 222 229 230 229\n",
      "  208 176 143 132 136 142 150 157 163 156 140 126 107  73  54  60  89  68\n",
      "   82 122 142 149 163 177 178 166 157 151 137 130 153 186 216 217 220 223\n",
      "  224 225 224 224 217 221 225 224 212 190 165 148 152 144 145 147 141 140\n",
      "  138 126 115  98  71  47  36  36  36  33  37   6   0   5   3   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  30   0   0\n",
      "    0 156 137 144 143 129 110  95  87  80  72  66  52  42  30  38  68  97\n",
      "  130 164 201 221 227 225 233 238 236 238 227 219 212 213 219 224 224 222\n",
      "  189 153 117 107 113 123 135 146 157 151 138 125 105  68  49  58  84  75\n",
      "   98 136 152 156 165 172 148 136 123 111 101 105 130 157 210 220 224 221\n",
      "  221 227 225 218 218 222 227 225 212 185 155 134 114 106 110 117 117 118\n",
      "  114 100  94  83  63  44  36  40  42  40  32  10   2   4   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0   1   1\n",
      "    7 119 131 124 105  87  64  49  44  40  34  28  25  30  35  51  80 110\n",
      "  150 193 212 224 226 225 234 235 231 235 228 221 214 214 219 221 218 214\n",
      "  169 133  98  88  93 103 117 132 146 142 131 122 102  66  48  58  72  75\n",
      "  102 132 144 151 156 154 129 122  88  46  43  85 127 143 207 222 227 218\n",
      "  216 224 220 207 225 229 232 228 212 182 148 125  82  65  57  58  61  72\n",
      "   79  72  65  61  49  36  33  40  45  44   7   0   0   0   0   4   9   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0   5\n",
      "    0  17  73  75  53  45  38  35  32  28  28  32  26  44  71  98 122 151\n",
      "  185 212 227 221 217 218 222 227 231 235 217 216 213 217 227 230 214 194\n",
      "  151 125  90  64  59  77 108 132 133 129 136 115  85  61  36  39  52  66\n",
      "   91 117 133 134 128 122 104  71  38  34  58  98 141 171 218 229 230 216\n",
      "  204 201 197 189 195 210 220 220 217 202 160 117  80  58  38  34  40  44\n",
      "   47  50  55  37  42  35  40  41  59   3   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   4   0   0\n",
      "    0   1  36  48  41  34  29  30  30  27  27  30  48  66  94 126 158 188\n",
      "  212 225 214 207 202 204 211 217 223 229 223 221 217 217 225 225 206 182\n",
      "  145 116  79  56  54  69  93 112 126 123 129 103  69  46  31  43  54  51\n",
      "   61  88 113 115  96  78  52  47  49  63  87 118 155 183 214 228 231 214\n",
      "  194 182 172 164 168 188 209 223 226 212 177 143 101  85  71  69  70  68\n",
      "   65  64  66  40  49  45  36  43  62   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0   2   0\n",
      "    0   3  11  40  47  39  33  35  40  44  49  54  77  97 126 156 188 211\n",
      "  214 205 181 177 178 190 203 213 223 231 225 224 220 220 226 226 205 179\n",
      "  145 113  75  52  47  56  72  86 112 110 115  86  51  33  28  50  65  49\n",
      "   43  57  74  73  55  38  29  48  77 104 123 146 176 200 216 224 219 194\n",
      "  168 154 147 142 153 172 201 224 228 211 184 164 143 134 126 125 125 122\n",
      "  120 120 109  75  66  48  26  42  67   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4   0\n",
      "    7   8   0  37  40  30  24  28  39  51  64  75  93 119 152 180 204 213\n",
      "  197 174 155 155 166 187 202 208 214 221 217 221 221 222 231 235 216 190\n",
      "  155 123  84  57  44  43  54  69  91  87  92  68  40  30  31  55  81  64\n",
      "   48  41  37  35  38  46  69  87 112 134 152 172 197 216 225 216 193 162\n",
      "  141 136 140 142 156 168 194 216 218 201 186 183 174 169 163 158 154 151\n",
      "  150 151 143 129 100  65  40  44  52   7   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   3   0   4   0\n",
      "    4   0   0  15  26  25  30  42  57  71  85  97 103 134 173 199 211 205\n",
      "  182 159 147 152 170 191 197 189 184 188 204 214 220 221 231 239 225 202\n",
      "  163 135 100  71  47  36  45  63  65  59  65  50  36  34  33  52  89  69\n",
      "   49  38  33  39  64  90 118 124 135 151 172 196 216 227 228 205 169 139\n",
      "  129 137 147 153 164 171 188 204 202 190 187 196 194 195 192 185 176 167\n",
      "  158 152 142 165 128  90  66  40  14   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0   1   0\n",
      "    1   0   0   3  10  22  45  70  87  97 108 118 125 152 187 206 202 181\n",
      "  158 144 140 149 170 188 184 165 157 162 193 210 219 218 225 235 227 208\n",
      "  171 147 117  89  60  39  42  59  46  38  45  38  33  36  31  45  80  59\n",
      "   45  51  66  83 109 133 139 141 148 163 188 214 228 230 224 194 154 129\n",
      "  126 138 149 154 174 180 191 196 187 172 170 176 176 184 191 192 188 179\n",
      "  166 154 142 167 121  84  51  17   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4\n",
      "    0   0   8   0   0   9  42  73  91 102 115 128 150 164 187 200 186 156\n",
      "  138 136 142 150 168 180 166 142 137 149 187 209 221 217 220 232 229 215\n",
      "  187 163 136 111  80  49  40  48  40  32  41  35  32  35  29  42  60  51\n",
      "   60  88 113 125 138 153 151 159 169 179 196 216 226 228 218 185 143 116\n",
      "  112 123 137 146 165 177 189 190 175 155 142 138 126 136 146 154 161 164\n",
      "  157 147 149 140  94  68  13   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3   0   5\n",
      "    0   0  14   0   0  15  48  77  95 111 132 151 159 164 180 193 179 151\n",
      "  139 147 158 162 173 175 152 123 120 136 184 209 224 219 220 232 234 222\n",
      "  204 178 150 128  96  58  37  36  42  36  45  38  33  34  29  43  44  54\n",
      "   85 125 143 142 145 156 167 180 191 193 197 209 220 224 216 181 135 101\n",
      "   92 102 122 138 138 158 179 183 171 150 129 117 113 118 122 127 139 152\n",
      "  154 149 144 109  78  73   0   0   8   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0  11   0   8  68 110 115 141 153 166 170 178 182 173 160\n",
      "  156 160 158 164 174 169 136 105 112 140 182 208 221 215 219 236 240 227\n",
      "  221 202 169 151 111  57  46  45  38  36  29  30  39  36  36  48  42  65\n",
      "   95 117 134 150 166 177 189 196 200 199 200 209 218 222 210 176 137 108\n",
      "   83  73  91 120 141 143 160 183 184 161 139 133 120 119 119 121 126 135\n",
      "  145 152 114 108  69  13   0   4   8   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0  14   0  10  88 107 125 117 158 158 163 172 179 174 160\n",
      "  150 149 160 160 159 143 110  93 120 162 195 212 221 217 221 233 235 226\n",
      "  226 220 187 156 111  69  64  58  54  51  42  40  47  44  43  55  50  71\n",
      "  100 124 143 161 179 190 196 204 210 209 209 212 215 214 199 176 148 123\n",
      "   97  78  76  84 120 127 147 173 184 176 165 160 156 147 136 129 128 128\n",
      "  127 125 104  81  42   8   0   0   2   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0  25   0   1 116 119 135 134 150 150 153 159 160 155\n",
      "  149 148 146 133 119 106  95 104 143 185 212 213 213 213 218 226 230 230\n",
      "  228 236 207 168 123  93  92  81  75  77  71  68  69  58  48  52  54  71\n",
      "   98 123 143 160 175 185 189 198 207 210 212 213 212 208 200 188 168 145\n",
      "  123 101  81  68  90 103 128 156 179 189 187 182 179 167 154 145 139 127\n",
      "  108  92  80  44  11   2   1   0   0   2   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   3   0   1   0   0 127 143 127 138 140 145 152 153 147\n",
      "  137 130 105  86  75  86 112 142 171 191 208 192 180 182 192 203 217 229\n",
      "  226 237 218 189 152 124 120 107  97 105 105 103 100  78  52  44  55  69\n",
      "   94 121 141 151 159 165 169 179 190 200 210 218 219 215 210 202 184 162\n",
      "  147 132 108  84  65  83 110 139 168 186 188 181 170 165 161 159 152 130\n",
      "   98  73  44  15   0   0   2   0   0   1   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   6   0   5   3   0   0 121 132 133 140 148 148 139 120\n",
      "   99  83  67  65  78 110 151 178 187 186 177 150 129 131 145 164 191 215\n",
      "  223 233 223 215 189 155 144 129 120 128 127 124 121  98  65  49  59  70\n",
      "   94 125 146 152 153 154 156 165 177 191 208 222 227 224 208 203 187 171\n",
      "  163 153 126  97  48  66  95 127 157 176 176 167 156 154 153 152 142 120\n",
      "   89  66  10   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0  14   1   3   3   2   0 109 126 127 120 102  83  71\n",
      "   66  64  75  98 131 161 182 189 181 171 134 107  84  85 102 128 162 191\n",
      "  212 226 225 228 211 179 166 149 145 146 135 127 129 113  83  65  56  63\n",
      "   87 122 147 153 151 151 148 157 171 187 205 217 216 209 191 188 180 173\n",
      "  169 155 123  91  38  52  82 119 150 164 162 155 147 138 127 118 108  91\n",
      "   68  51   0   0   0   0   0   3   2   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   1   0   0   3   6   0  14   3  75  78  69  50  39  52\n",
      "   79 100 119 143 169 182 180 168 146 127  88  69  52  53  70  97 130 155\n",
      "  183 213 220 220 208 193 188 168 164 159 140 130 135 123  91  68  49  51\n",
      "   73 110 138 146 146 147 146 158 175 194 209 212 198 182 160 157 155 157\n",
      "  154 136 101  72  34  42  69 110 141 150 148 146 126 108  87  75  69  58\n",
      "   38  21   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   1   0   0   4   0   6   0   0   4  24  39  40  46  74\n",
      "  113 141 152 162 168 166 157 139 106  76  52  40  30  32  48  72 101 121\n",
      "  155 200 213 205 195 198 204 181 173 167 147 137 142 127  88  59  48  47\n",
      "   65 101 130 140 141 143 150 165 187 208 220 215 190 167 132 125 124 130\n",
      "  129 109  78  54  34  36  60 103 133 138 136 138  99  77  54  46  47  39\n",
      "   16   0   8   0   0   0   2   0   0   2   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   7  25  46  65  89\n",
      "  119 144 149 154 139 131 101  91  61  47  50  42  37  38  41  49  68  86\n",
      "  121 150 185 195 180 164 169 183 174 176 161 144 141 126  93  68  42  55\n",
      "   63  88 126 134 134 151 158 180 193 197 207 211 183 143 111 100  95  96\n",
      "   86  65  49  45  32  38  47  65  99 127 122 101  73  74  65  42  15   0\n",
      "    0   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   1   6  21  55  96\n",
      "  121 127 122 121 106  87  57  47  46  57  62  58  51  43  32  31  45  63\n",
      "   91 117 153 176 175 162 156 158 166 174 164 148 143 126  94  72  47  59\n",
      "   62  83 124 140 138 148 150 172 191 199 204 198 165 128  90  76  66  65\n",
      "   59  43  32  30  34  39  42  42  51  65  69  65  55  38  17   4   1   1\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   2   0   0  21  63\n",
      "   85  84  69  71  61  41  47  65  74  65  87  87  81  64  40  26  32  44\n",
      "   61  80 115 151 170 167 154 145 156 165 159 145 138 118  87  68  53  65\n",
      "   61  72 115 141 141 144 146 170 195 207 205 185 148 114  72  55  41  39\n",
      "   37  30  25  26  40  40  38  33  30  31  35  39  28   9   0   0   0   6\n",
      "    5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   4   2   0   0  11\n",
      "   36  50  63  32  34  63  86  75  86 109 109 112 108  92  66  45  36  36\n",
      "   44  56  85 126 160 172 170 165 160 163 153 140 134 113  80  61  55  71\n",
      "   61  58  94 126 133 138 146 169 196 209 200 170 131 102  57  41  28  27\n",
      "   31  31  30  30  38  31  29  35  39  35  28  25   3   1   0   0   2   3\n",
      "    2   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   1   0   0\n",
      "   13  38  43  61  78  85  87  97 112 123 122 123 120 109  90  68  47  34\n",
      "   32  37  57  93 132 162 184 195 178 171 151 137 133 112  76  54  54  74\n",
      "   61  44  67  97 113 127 141 164 189 198 184 152 114  87  38  28  21  23\n",
      "   28  31  31  31  30  26  25  30  29  21  12   9   0   0   4   5   1   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   9   0   0   4   4   0\n",
      "    0  16   6  54  76  95 101 144 137 119 127 123 116 107  98  84  62  43\n",
      "   28  27  37  60  94 133 173 200 187 174 148 132 128 104  65  41  51  72\n",
      "   59  34  44  65  85 107 135 160 182 183 167 141 105  76  28  25  23  26\n",
      "   30  32  32  30  34  38  37  25   7   0   0   1   2   0   0   0   0   2\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   9   0   0   0   3   0\n",
      "    0   0   5   0   0  85 102 132 112 134 116 108  94  84  82  82  73  60\n",
      "   45  39  37  46  68 101 142 173 184 172 148 131 122  92  51  30  49  68\n",
      "   55  31  33  42  57  82 125 153 173 168 153 133  98  63  29  31  33  34\n",
      "   33  33  32  31  25  33  30  13   0   0   0   0   7   0   0   0   0   4\n",
      "    2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   2\n",
      "    1   0   0  20   0   8   4 105 107 105  96  85  68  55  58  70  74  70\n",
      "   70  60  50  49  59  83 117 144 183 174 154 137 122  88  48  29  48  65\n",
      "   52  30  30  32  40  64 112 143 161 153 140 124  88  49  31  35  38  36\n",
      "   32  31  30  29   0   4   0   0   0   8   4   0   0   1   4   2   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   1   2   0   0  15  46  71  56  52  46  43  47  59  74  84\n",
      "   91  81  68  66  65  61  84 127 157 162 155 143 126  88  45  26  37  71\n",
      "   57  25  31  40  44  63 101 129 152 149 128  98  62  34  40  42  43  38\n",
      "   29  18   8   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   2   3   0   0   0   5  11  17  22  29  35  44  59  78  92\n",
      "   94  93  80  73  81  85  91 104 119 147 160 144 109  67  44  47  41  71\n",
      "   61  32  33  35  41  65 103 131 148 132  97  64  39  23  30  30  28  22\n",
      "   12   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   2   2   0   0   0   0   1  13  20  23  31  45  57\n",
      "   80 111 119 101  90  86  80  79 109 131 144 139 117  72  28  10  46  69\n",
      "   63  44  41  37  45  73  88 115 128 106  68  42  36  38   7   7   5   2\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   2   0   0   0   3   5   4   1   0   0   5   6   3   1   4   8\n",
      "    6  53  84  88  92  94  90  89 104  73  21   0   0   0   0   4   0  10\n",
      "    7   0   0   0   2  26  62  76  77  51  16   0   0   0   0   0   0   0\n",
      "    0   0   3   6   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   5   3   0   0   0   0   1   4   2   1   0   0   0   0   0   0\n",
      "    0   9  25  49  80  89  79  74   7  10   2   0   2   0   0   5   2   1\n",
      "    0   0   3   0   3  15   0   0   1   0   0   0   7  11   1   2   3   1\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   1   3   1   0   0   0   0   4   0   0   0   1   3   1   0\n",
      "    0   0   0   0   6   0   0   0   0   0   1   1   3   0   0   6   4   0\n",
      "    0   0   0   0   0   0  11   6   1   0   0   1   0   0   0   0   2   3\n",
      "    1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   2   3   2   1   0   1   2   0   0   0   0   0   0   0\n",
      "    4   9   6   2   0   0   0  11   8  11   3   0   6   3   0   0   3   0\n",
      "    1   1   0   1   3   0   0   0   0   0   0   2   2   2   0   0   0   0\n",
      "    0   0   1   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   2   1   0   0   0   0   0   0   0   0   2   2   0   0   0   1\n",
      "    0   1   0   0   9   8   0   0   0   0   0   0   0   0   0   4   0   0\n",
      "    4   2   0   2   7   0   2   4   5   5   0   0   0   5   2   2   1   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# I print out what the test image looks like as an array, as I plan to convert all the test images into arrays.\n",
    "\n",
    "testArray = np.asarray(testImage)\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "print(testArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65583c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This for loop takes every image in the Alzheimer's Dataset file (which is already separated into test and \n",
    "# training sets) and puts it into X_train and X_test lists (I use lists rather than arrays because it is easier to\n",
    "# iterate to add items to a list, and then convert to arrays later). Meanwhile, according to which stage of \n",
    "# Alzheimer's it is, I put the stage in the Y_train or Y_test list, turning the stages into integers in order of\n",
    "# severity so they can be used in a regression, which I plan to use even though this is a classification problem\n",
    "# to account for how the class numbers are not arbitrary but in order of severity.\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "\n",
    "for testTrain in os.listdir('Desktop/Alzheimer_s Dataset'):\n",
    "    if testTrain != '.DS_Store':\n",
    "        for dementia in os.listdir('Desktop/Alzheimer_s Dataset/' + testTrain):\n",
    "            if dementia != '.DS_Store':\n",
    "                for file in os.listdir('Desktop/Alzheimer_s Dataset/' + testTrain + '/' + dementia):\n",
    "                    if testTrain == 'train':\n",
    "                        img = Image.open('Desktop/Alzheimer_s Dataset/' + testTrain + '/' + dementia + '/' + file)\n",
    "                        X_train.append(np.asarray(img))\n",
    "                        if dementia == 'MildDemented':\n",
    "                            Y_train.append(3.0)\n",
    "                        elif dementia == 'ModerateDemented':\n",
    "                            Y_train.append(4.0)\n",
    "                        elif dementia == 'NonDemented':\n",
    "                            Y_train.append(1.0)\n",
    "                        elif dementia == 'VeryMildDemented':\n",
    "                            Y_train.append(2.0)\n",
    "\n",
    "                    elif testTrain == 'test':\n",
    "                        img = Image.open('Desktop/Alzheimer_s Dataset/' + testTrain + '/' + dementia + '/' + file)\n",
    "                        X_test.append(np.asarray(img))\n",
    "                        if dementia == 'MildDemented':\n",
    "                            Y_test.append(3.0)\n",
    "                        elif dementia == 'ModerateDemented':\n",
    "                            Y_test.append(4.0)\n",
    "                        elif dementia == 'NonDemented':\n",
    "                            Y_test.append(1.0)\n",
    "                        elif dementia == 'VeryMildDemented':\n",
    "                            Y_test.append(2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a456100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5121\n",
      "5121\n",
      "1279\n",
      "1279\n",
      "2560\n",
      "1792\n",
      "717\n",
      "52\n",
      "640\n",
      "448\n",
      "179\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# Here I check my data set, seeing the length of the training and test sets. \n",
    "\n",
    "print(len(X_train))\n",
    "print(len(Y_train))\n",
    "print(len(X_test))\n",
    "print(len(Y_test))\n",
    "\n",
    "print(Y_train.count(1.0))\n",
    "print(Y_train.count(2.0))\n",
    "print(Y_train.count(3.0))\n",
    "print(Y_train.count(4.0))\n",
    "\n",
    "print(Y_test.count(1.0))\n",
    "print(Y_test.count(2.0))\n",
    "print(Y_test.count(3.0))\n",
    "print(Y_test.count(4.0))\n",
    "\n",
    "# The more severe the dementia is, the fewer images there are representing it, meaning that stratified partition\n",
    "# would be preferable over random partition when creeating validation folds if I don't undersample/oversample and I \n",
    "# should either use a loss function that weights the less common classes more heavily, undersample (not ideal\n",
    "# because there are VERY few of the rarest class) or oversample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb15168e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train is (5121, 208, 176)\n",
      "X_test is (1279, 208, 176)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "print('X_train is ' + str(X_train.shape))\n",
    "print('X_test is ' + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3dba3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train is now (5121, 36608)\n",
      "X_test is now (1279, 36608)\n"
     ]
    }
   ],
   "source": [
    "# I flatten the arrays because arrays must be flattened to be oversampled with imblearn, and it is also helpful\n",
    "# for making the data into a dataset that I can do Exploratory Data Analysis on. See below for why I flatten the \n",
    "# test data even though I am not doing analysis on it and you don't want to oversample test data.\n",
    "\n",
    "X_trainFlat = X_train.reshape((5121, 36608))\n",
    "X_testFlat = X_test.reshape((1279, 36608))\n",
    "\n",
    "\n",
    "print('X_train is now ' + str(X_trainFlat.shape))\n",
    "print('X_test is now ' + str(X_testFlat.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b83a00f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       0      1      2      3      4      5      6      7      8      9      \\\n",
       "0         0      0      0      0      0      0      0      0      0      0   \n",
       "1         0      0      0      0      0      0      0      0      0      0   \n",
       "2         0      0      0      0      0      0      0      0      0      0   \n",
       "3         0      0      0      0      0      0      0      0      0      0   \n",
       "4         0      0      0      0      0      0      0      0      0      0   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "5116      0      0      0      0      0      0      0      0      0      0   \n",
       "5117      0      0      0      0      0      0      0      0      0      0   \n",
       "5118      0      0      0      0      0      0      0      0      0      0   \n",
       "5119      0      0      0      0      0      0      0      0      0      0   \n",
       "5120      0      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "      ...  36598  36599  36600  36601  36602  36603  36604  36605  36606  \\\n",
       "0     ...      0      0      0      0      0      0      0      0      0   \n",
       "1     ...      0      0      0      0      0      0      0      0      0   \n",
       "2     ...      0      0      0      0      0      0      0      0      0   \n",
       "3     ...      0      0      0      0      0      0      0      0      0   \n",
       "4     ...      0      0      0      0      0      0      0      0      0   \n",
       "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "5116  ...      0      0      0      0      0      0      0      0      0   \n",
       "5117  ...      0      0      0      0      0      0      0      0      0   \n",
       "5118  ...      0      0      0      0      0      0      0      0      0   \n",
       "5119  ...      0      0      0      0      0      0      0      0      0   \n",
       "5120  ...      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "      36607  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "5116      0  \n",
       "5117      0  \n",
       "5118      0  \n",
       "5119      0  \n",
       "5120      0  \n",
       "\n",
       "[5121 rows x 36608 columns]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainDF = pd.DataFrame(X_trainFlat)\n",
    "X_trainDF.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6043e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>36598</th>\n",
       "      <th>36599</th>\n",
       "      <th>36600</th>\n",
       "      <th>36601</th>\n",
       "      <th>36602</th>\n",
       "      <th>36603</th>\n",
       "      <th>36604</th>\n",
       "      <th>36605</th>\n",
       "      <th>36606</th>\n",
       "      <th>36607</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5121.0</td>\n",
       "      <td>5121.0</td>\n",
       "      <td>5121.0</td>\n",
       "      <td>5121.0</td>\n",
       "      <td>5121.0</td>\n",
       "      <td>5121.0</td>\n",
       "      <td>5121.0</td>\n",
       "      <td>5121.0</td>\n",
       "      <td>5121.0</td>\n",
       "      <td>5121.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5121.0</td>\n",
       "      <td>5121.0</td>\n",
       "      <td>5121.0</td>\n",
       "      <td>5121.0</td>\n",
       "      <td>5121.0</td>\n",
       "      <td>5121.0</td>\n",
       "      <td>5121.0</td>\n",
       "      <td>5121.0</td>\n",
       "      <td>5121.0</td>\n",
       "      <td>5121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 36608 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1       2       3       4       5       6       7       8      \\\n",
       "count  5121.0  5121.0  5121.0  5121.0  5121.0  5121.0  5121.0  5121.0  5121.0   \n",
       "mean      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "std       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "min       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "50%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "75%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "max       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "        9      ...   36598   36599   36600   36601   36602   36603   36604  \\\n",
       "count  5121.0  ...  5121.0  5121.0  5121.0  5121.0  5121.0  5121.0  5121.0   \n",
       "mean      0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "std       0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "min       0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25%       0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "50%       0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "75%       0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "max       0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "        36605   36606   36607  \n",
       "count  5121.0  5121.0  5121.0  \n",
       "mean      0.0     0.0     0.0  \n",
       "std       0.0     0.0     0.0  \n",
       "min       0.0     0.0     0.0  \n",
       "25%       0.0     0.0     0.0  \n",
       "50%       0.0     0.0     0.0  \n",
       "75%       0.0     0.0     0.0  \n",
       "max       0.0     0.0     0.0  \n",
       "\n",
       "[8 rows x 36608 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('display.max_columns', 10000)\n",
    "X_trainDF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9294d0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x179d57c90>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A graph of how the average value of a pixel in the image changes over row and column.\n",
    "\n",
    "plt.plot(X_trainDF.columns, X_trainDF.describe().loc['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c1c6da14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_testOver is (2560, 36608)\n"
     ]
    }
   ],
   "source": [
    "# I chose to oversample because, as I said, undersampling would leave me with too little data, and it would be hard\n",
    "# to find a loss function that accounts for rarer classes while also accomplishing other things I want like \n",
    "# acknowledging the classes are ordinal. I oversample every class besides the majority class. This also means I \n",
    "# don't have to do a stratified K-fold. I don't oversample the test set because, while I need to train the model\n",
    "# on more of the rarer classes, the test set is for seeing how it performs on a data set just like the real world.\n",
    "\n",
    "over = RandomOverSampler(sampling_strategy = 'not majority', random_state = 100)\n",
    "X_trainOver, Y_trainOver = over.fit_resample(X_trainFlat, Y_train)\n",
    "\n",
    "# this last part is not actually going to be used for the final results, since the idea is to see how the model \n",
    "# performs on the actual imbalanced data you will see (so the test set should not be oversampled), this is just\n",
    "# a test I added later on to see if underperformance on the test set as opposed to the kFolds of the training set\n",
    "# was caused by normal overfit or a result of class imbalance leading to overpredicting more severe stages being\n",
    "# more likely. Since there isn't an option to resample the test data \n",
    "# without fitting (since oversample isn't designed to be used for test data), I tried two variants, one where the \n",
    "# original oversampler with another fit was used and one where a second oversampler was created.\n",
    "\n",
    "X_testOver, Y_testOver = over.fit_resample(X_testFlat, Y_test)\n",
    "\n",
    "over2 = RandomOverSampler(sampling_strategy = 'not majority', random_state = 103)\n",
    "X_testOver2, Y_testOver2 = over2.fit_resample(X_testFlat, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4bf7bf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trainOver is(10240, 36608)\n",
      "X_testOver is (2560, 36608)\n",
      "2560\n",
      "2560\n",
      "2560\n",
      "2560\n"
     ]
    }
   ],
   "source": [
    "# This is successful, in the training set each class is now equally common.\n",
    "\n",
    "print('X_trainOver is' + str(X_trainOver.shape))\n",
    "print('X_testOver is ' + str(X_testOver.shape))\n",
    "\n",
    "Y_trainList = np.ndarray.tolist(Y_trainOver)\n",
    "\n",
    "print(Y_trainList.count(1.0))\n",
    "print(Y_trainList.count(2.0))\n",
    "print(Y_trainList.count(3.0))\n",
    "print(Y_trainList.count(4.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ea30b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainOver = X_trainOver/255.0\n",
    "X_test = X_test/255.0\n",
    "X_testOver = X_testOver/255.0\n",
    "X_testOver2 = X_testOver2/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3b67370d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trainOver2 is(10240, 208, 176, 1)\n",
      "X_test is(1279, 208, 176, 1)\n",
      "X_testOver is(2560, 208, 176, 1)\n",
      "X_testOver2 is(2560, 208, 176, 1)\n"
     ]
    }
   ],
   "source": [
    "# made two X_trainOvers, X_trainOver2 is for immediate use in my neural network from scratch, having one color \n",
    "# channel to be compatible with that neural network, while X_trainOver is intended for use in the resNet neural \n",
    "# network, to be eventually changed to have three color channels that are all copies of each other because that is\n",
    "# necessary to be compatible with resNet.\n",
    "\n",
    "X_trainOver = X_trainOver.reshape(10240, 208, 176)\n",
    "X_trainOver2 = X_trainOver.reshape(10240, 208, 176, 1)\n",
    "\n",
    "X_test = X_test.reshape(1279, 208, 176, 1)\n",
    "X_testOver = X_testOver.reshape(2560, 208, 176, 1)\n",
    "X_testOver2 = X_testOver2.reshape(2560, 208, 176, 1)\n",
    "\n",
    "print('X_trainOver2 is' + str(X_trainOver2.shape))\n",
    "print('X_test is' + str(X_test.shape))\n",
    "print('X_testOver is' + str(X_testOver.shape))\n",
    "print('X_testOver2 is' + str(X_testOver2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b6e2fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/50\n",
      "32/32 - 3s - loss: 2.2519 - mse: 2.2519 - 3s/epoch - 101ms/step\n",
      "Epoch 2/50\n",
      "32/32 - 3s - loss: 1.3705 - mse: 1.3705 - 3s/epoch - 92ms/step\n",
      "Epoch 3/50\n",
      "32/32 - 3s - loss: 1.2921 - mse: 1.2921 - 3s/epoch - 95ms/step\n",
      "Epoch 4/50\n",
      "32/32 - 3s - loss: 1.2009 - mse: 1.2009 - 3s/epoch - 105ms/step\n",
      "Epoch 5/50\n",
      "32/32 - 4s - loss: 1.0688 - mse: 1.0688 - 4s/epoch - 111ms/step\n",
      "Epoch 6/50\n",
      "32/32 - 4s - loss: 0.9125 - mse: 0.9125 - 4s/epoch - 116ms/step\n",
      "Epoch 7/50\n",
      "32/32 - 4s - loss: 0.7457 - mse: 0.7457 - 4s/epoch - 114ms/step\n",
      "Epoch 8/50\n",
      "32/32 - 4s - loss: 0.6011 - mse: 0.6011 - 4s/epoch - 131ms/step\n",
      "Epoch 9/50\n",
      "32/32 - 4s - loss: 0.5132 - mse: 0.5132 - 4s/epoch - 116ms/step\n",
      "Epoch 10/50\n",
      "32/32 - 4s - loss: 0.4584 - mse: 0.4584 - 4s/epoch - 133ms/step\n",
      "Epoch 11/50\n",
      "32/32 - 4s - loss: 0.4306 - mse: 0.4306 - 4s/epoch - 114ms/step\n",
      "Epoch 12/50\n",
      "32/32 - 3s - loss: 0.4081 - mse: 0.4081 - 3s/epoch - 103ms/step\n",
      "Epoch 13/50\n",
      "32/32 - 3s - loss: 0.3899 - mse: 0.3899 - 3s/epoch - 107ms/step\n",
      "Epoch 14/50\n",
      "32/32 - 4s - loss: 0.3808 - mse: 0.3808 - 4s/epoch - 135ms/step\n",
      "Epoch 15/50\n",
      "32/32 - 4s - loss: 0.3726 - mse: 0.3726 - 4s/epoch - 120ms/step\n",
      "Epoch 16/50\n",
      "32/32 - 4s - loss: 0.3729 - mse: 0.3729 - 4s/epoch - 117ms/step\n",
      "Epoch 17/50\n",
      "32/32 - 4s - loss: 0.3540 - mse: 0.3540 - 4s/epoch - 114ms/step\n",
      "Epoch 18/50\n",
      "32/32 - 3s - loss: 0.3422 - mse: 0.3422 - 3s/epoch - 108ms/step\n",
      "Epoch 19/50\n",
      "32/32 - 4s - loss: 0.3463 - mse: 0.3463 - 4s/epoch - 114ms/step\n",
      "Epoch 20/50\n",
      "32/32 - 4s - loss: 0.3269 - mse: 0.3269 - 4s/epoch - 110ms/step\n",
      "Epoch 21/50\n",
      "32/32 - 4s - loss: 0.3189 - mse: 0.3189 - 4s/epoch - 130ms/step\n",
      "Epoch 22/50\n",
      "32/32 - 4s - loss: 0.3071 - mse: 0.3071 - 4s/epoch - 128ms/step\n",
      "Epoch 23/50\n",
      "32/32 - 4s - loss: 0.2993 - mse: 0.2993 - 4s/epoch - 122ms/step\n",
      "Epoch 24/50\n",
      "32/32 - 5s - loss: 0.2951 - mse: 0.2951 - 5s/epoch - 150ms/step\n",
      "Epoch 25/50\n",
      "32/32 - 5s - loss: 0.2931 - mse: 0.2931 - 5s/epoch - 150ms/step\n",
      "Epoch 26/50\n",
      "32/32 - 4s - loss: 0.2877 - mse: 0.2877 - 4s/epoch - 134ms/step\n",
      "Epoch 27/50\n",
      "32/32 - 4s - loss: 0.2736 - mse: 0.2736 - 4s/epoch - 125ms/step\n",
      "Epoch 28/50\n",
      "32/32 - 4s - loss: 0.2716 - mse: 0.2716 - 4s/epoch - 129ms/step\n",
      "Epoch 29/50\n",
      "32/32 - 4s - loss: 0.2672 - mse: 0.2672 - 4s/epoch - 134ms/step\n",
      "Epoch 30/50\n",
      "32/32 - 4s - loss: 0.2631 - mse: 0.2631 - 4s/epoch - 126ms/step\n",
      "Epoch 31/50\n",
      "32/32 - 4s - loss: 0.2796 - mse: 0.2796 - 4s/epoch - 127ms/step\n",
      "Epoch 32/50\n",
      "32/32 - 4s - loss: 0.2696 - mse: 0.2696 - 4s/epoch - 120ms/step\n",
      "Epoch 33/50\n",
      "32/32 - 4s - loss: 0.2506 - mse: 0.2506 - 4s/epoch - 127ms/step\n",
      "Epoch 34/50\n",
      "32/32 - 4s - loss: 0.2513 - mse: 0.2513 - 4s/epoch - 114ms/step\n",
      "Epoch 35/50\n",
      "32/32 - 4s - loss: 0.2429 - mse: 0.2429 - 4s/epoch - 123ms/step\n",
      "Epoch 36/50\n",
      "32/32 - 4s - loss: 0.2366 - mse: 0.2366 - 4s/epoch - 139ms/step\n",
      "Epoch 37/50\n",
      "32/32 - 5s - loss: 0.2434 - mse: 0.2434 - 5s/epoch - 149ms/step\n",
      "Epoch 38/50\n",
      "32/32 - 4s - loss: 0.2284 - mse: 0.2284 - 4s/epoch - 140ms/step\n",
      "Epoch 39/50\n",
      "32/32 - 5s - loss: 0.2220 - mse: 0.2220 - 5s/epoch - 147ms/step\n",
      "Epoch 40/50\n",
      "32/32 - 4s - loss: 0.2174 - mse: 0.2174 - 4s/epoch - 127ms/step\n",
      "Epoch 41/50\n",
      "32/32 - 4s - loss: 0.2153 - mse: 0.2153 - 4s/epoch - 128ms/step\n",
      "Epoch 42/50\n",
      "32/32 - 5s - loss: 0.2126 - mse: 0.2126 - 5s/epoch - 146ms/step\n",
      "Epoch 43/50\n",
      "32/32 - 4s - loss: 0.2117 - mse: 0.2117 - 4s/epoch - 134ms/step\n",
      "Epoch 44/50\n",
      "32/32 - 5s - loss: 0.2060 - mse: 0.2060 - 5s/epoch - 142ms/step\n",
      "Epoch 45/50\n",
      "32/32 - 4s - loss: 0.2114 - mse: 0.2114 - 4s/epoch - 119ms/step\n",
      "Epoch 46/50\n",
      "32/32 - 4s - loss: 0.2047 - mse: 0.2047 - 4s/epoch - 125ms/step\n",
      "Epoch 47/50\n",
      "32/32 - 4s - loss: 0.2089 - mse: 0.2089 - 4s/epoch - 129ms/step\n",
      "Epoch 48/50\n",
      "32/32 - 4s - loss: 0.1939 - mse: 0.1939 - 4s/epoch - 133ms/step\n",
      "Epoch 49/50\n",
      "32/32 - 5s - loss: 0.1884 - mse: 0.1884 - 5s/epoch - 143ms/step\n",
      "Epoch 50/50\n",
      "32/32 - 5s - loss: 0.1928 - mse: 0.1928 - 5s/epoch - 152ms/step\n",
      "Score for fold 1: mse of [0.19024229049682617, 0.19024229049682617]\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/50\n",
      "32/32 - 4s - loss: 2.2271 - mse: 2.2271 - 4s/epoch - 114ms/step\n",
      "Epoch 2/50\n",
      "32/32 - 3s - loss: 1.3099 - mse: 1.3099 - 3s/epoch - 109ms/step\n",
      "Epoch 3/50\n",
      "32/32 - 4s - loss: 1.1535 - mse: 1.1535 - 4s/epoch - 134ms/step\n",
      "Epoch 4/50\n",
      "32/32 - 4s - loss: 0.9894 - mse: 0.9894 - 4s/epoch - 138ms/step\n",
      "Epoch 5/50\n",
      "32/32 - 5s - loss: 0.7768 - mse: 0.7768 - 5s/epoch - 146ms/step\n",
      "Epoch 6/50\n",
      "32/32 - 4s - loss: 0.5559 - mse: 0.5559 - 4s/epoch - 139ms/step\n",
      "Epoch 7/50\n",
      "32/32 - 5s - loss: 0.4574 - mse: 0.4574 - 5s/epoch - 143ms/step\n",
      "Epoch 8/50\n",
      "32/32 - 5s - loss: 0.4159 - mse: 0.4159 - 5s/epoch - 160ms/step\n",
      "Epoch 9/50\n",
      "32/32 - 5s - loss: 0.3930 - mse: 0.3930 - 5s/epoch - 142ms/step\n",
      "Epoch 10/50\n",
      "32/32 - 4s - loss: 0.3675 - mse: 0.3675 - 4s/epoch - 138ms/step\n",
      "Epoch 11/50\n",
      "32/32 - 6s - loss: 0.3509 - mse: 0.3509 - 6s/epoch - 193ms/step\n",
      "Epoch 12/50\n",
      "32/32 - 6s - loss: 0.3444 - mse: 0.3444 - 6s/epoch - 195ms/step\n",
      "Epoch 13/50\n",
      "32/32 - 6s - loss: 0.3258 - mse: 0.3258 - 6s/epoch - 181ms/step\n",
      "Epoch 14/50\n",
      "32/32 - 5s - loss: 0.3115 - mse: 0.3115 - 5s/epoch - 171ms/step\n",
      "Epoch 15/50\n",
      "32/32 - 5s - loss: 0.3057 - mse: 0.3057 - 5s/epoch - 141ms/step\n",
      "Epoch 16/50\n",
      "32/32 - 5s - loss: 0.2974 - mse: 0.2974 - 5s/epoch - 152ms/step\n",
      "Epoch 17/50\n",
      "32/32 - 4s - loss: 0.2895 - mse: 0.2895 - 4s/epoch - 139ms/step\n",
      "Epoch 18/50\n",
      "32/32 - 5s - loss: 0.2779 - mse: 0.2779 - 5s/epoch - 141ms/step\n",
      "Epoch 19/50\n",
      "32/32 - 4s - loss: 0.2726 - mse: 0.2726 - 4s/epoch - 138ms/step\n",
      "Epoch 20/50\n",
      "32/32 - 5s - loss: 0.2655 - mse: 0.2655 - 5s/epoch - 161ms/step\n",
      "Epoch 21/50\n",
      "32/32 - 5s - loss: 0.2649 - mse: 0.2649 - 5s/epoch - 154ms/step\n",
      "Epoch 22/50\n",
      "32/32 - 5s - loss: 0.2469 - mse: 0.2469 - 5s/epoch - 156ms/step\n",
      "Epoch 23/50\n",
      "32/32 - 5s - loss: 0.2378 - mse: 0.2378 - 5s/epoch - 168ms/step\n",
      "Epoch 24/50\n",
      "32/32 - 5s - loss: 0.2300 - mse: 0.2300 - 5s/epoch - 170ms/step\n",
      "Epoch 25/50\n",
      "32/32 - 5s - loss: 0.2235 - mse: 0.2235 - 5s/epoch - 141ms/step\n",
      "Epoch 26/50\n",
      "32/32 - 5s - loss: 0.2200 - mse: 0.2200 - 5s/epoch - 162ms/step\n",
      "Epoch 27/50\n",
      "32/32 - 5s - loss: 0.2138 - mse: 0.2138 - 5s/epoch - 171ms/step\n",
      "Epoch 28/50\n",
      "32/32 - 6s - loss: 0.2127 - mse: 0.2127 - 6s/epoch - 173ms/step\n",
      "Epoch 29/50\n",
      "32/32 - 6s - loss: 0.2032 - mse: 0.2032 - 6s/epoch - 183ms/step\n",
      "Epoch 30/50\n",
      "32/32 - 5s - loss: 0.1990 - mse: 0.1990 - 5s/epoch - 146ms/step\n",
      "Epoch 31/50\n",
      "32/32 - 5s - loss: 0.1960 - mse: 0.1960 - 5s/epoch - 169ms/step\n",
      "Epoch 32/50\n",
      "32/32 - 5s - loss: 0.1876 - mse: 0.1876 - 5s/epoch - 145ms/step\n",
      "Epoch 33/50\n",
      "32/32 - 5s - loss: 0.1816 - mse: 0.1816 - 5s/epoch - 142ms/step\n",
      "Epoch 34/50\n",
      "32/32 - 4s - loss: 0.1779 - mse: 0.1779 - 4s/epoch - 139ms/step\n",
      "Epoch 35/50\n",
      "32/32 - 4s - loss: 0.1728 - mse: 0.1728 - 4s/epoch - 136ms/step\n",
      "Epoch 36/50\n",
      "32/32 - 4s - loss: 0.1696 - mse: 0.1696 - 4s/epoch - 138ms/step\n",
      "Epoch 37/50\n",
      "32/32 - 5s - loss: 0.1647 - mse: 0.1647 - 5s/epoch - 151ms/step\n",
      "Epoch 38/50\n",
      "32/32 - 4s - loss: 0.1633 - mse: 0.1633 - 4s/epoch - 139ms/step\n",
      "Epoch 39/50\n",
      "32/32 - 5s - loss: 0.1578 - mse: 0.1578 - 5s/epoch - 143ms/step\n",
      "Epoch 40/50\n",
      "32/32 - 4s - loss: 0.1518 - mse: 0.1518 - 4s/epoch - 131ms/step\n",
      "Epoch 41/50\n",
      "32/32 - 4s - loss: 0.1510 - mse: 0.1510 - 4s/epoch - 134ms/step\n",
      "Epoch 42/50\n",
      "32/32 - 5s - loss: 0.1482 - mse: 0.1482 - 5s/epoch - 149ms/step\n",
      "Epoch 43/50\n",
      "32/32 - 5s - loss: 0.1519 - mse: 0.1519 - 5s/epoch - 144ms/step\n",
      "Epoch 44/50\n",
      "32/32 - 5s - loss: 0.1412 - mse: 0.1412 - 5s/epoch - 142ms/step\n",
      "Epoch 45/50\n",
      "32/32 - 4s - loss: 0.1373 - mse: 0.1373 - 4s/epoch - 128ms/step\n",
      "Epoch 46/50\n",
      "32/32 - 4s - loss: 0.1373 - mse: 0.1373 - 4s/epoch - 132ms/step\n",
      "Epoch 47/50\n",
      "32/32 - 4s - loss: 0.1334 - mse: 0.1334 - 4s/epoch - 135ms/step\n",
      "Epoch 48/50\n",
      "32/32 - 4s - loss: 0.1290 - mse: 0.1290 - 4s/epoch - 138ms/step\n",
      "Epoch 49/50\n",
      "32/32 - 4s - loss: 0.1263 - mse: 0.1263 - 4s/epoch - 136ms/step\n",
      "Epoch 50/50\n",
      "32/32 - 4s - loss: 0.1241 - mse: 0.1241 - 4s/epoch - 135ms/step\n",
      "Score for fold 2: mse of [0.14694473147392273, 0.14694473147392273]\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/50\n",
      "32/32 - 3s - loss: 2.0056 - mse: 2.0056 - 3s/epoch - 102ms/step\n",
      "Epoch 2/50\n",
      "32/32 - 3s - loss: 1.2899 - mse: 1.2899 - 3s/epoch - 104ms/step\n",
      "Epoch 3/50\n",
      "32/32 - 4s - loss: 1.1481 - mse: 1.1481 - 4s/epoch - 120ms/step\n",
      "Epoch 4/50\n",
      "32/32 - 4s - loss: 0.9436 - mse: 0.9436 - 4s/epoch - 118ms/step\n",
      "Epoch 5/50\n",
      "32/32 - 4s - loss: 0.7272 - mse: 0.7272 - 4s/epoch - 119ms/step\n",
      "Epoch 6/50\n",
      "32/32 - 4s - loss: 0.5640 - mse: 0.5640 - 4s/epoch - 121ms/step\n",
      "Epoch 7/50\n",
      "32/32 - 4s - loss: 0.4735 - mse: 0.4735 - 4s/epoch - 124ms/step\n",
      "Epoch 8/50\n",
      "32/32 - 4s - loss: 0.4259 - mse: 0.4259 - 4s/epoch - 133ms/step\n",
      "Epoch 9/50\n",
      "32/32 - 4s - loss: 0.4002 - mse: 0.4002 - 4s/epoch - 121ms/step\n",
      "Epoch 10/50\n",
      "32/32 - 5s - loss: 0.3817 - mse: 0.3817 - 5s/epoch - 166ms/step\n",
      "Epoch 11/50\n",
      "32/32 - 4s - loss: 0.3685 - mse: 0.3685 - 4s/epoch - 134ms/step\n",
      "Epoch 12/50\n",
      "32/32 - 6s - loss: 0.3631 - mse: 0.3631 - 6s/epoch - 176ms/step\n",
      "Epoch 13/50\n",
      "32/32 - 4s - loss: 0.3492 - mse: 0.3492 - 4s/epoch - 132ms/step\n",
      "Epoch 14/50\n",
      "32/32 - 5s - loss: 0.3371 - mse: 0.3371 - 5s/epoch - 141ms/step\n",
      "Epoch 15/50\n",
      "32/32 - 4s - loss: 0.3278 - mse: 0.3278 - 4s/epoch - 138ms/step\n",
      "Epoch 16/50\n",
      "32/32 - 4s - loss: 0.3204 - mse: 0.3204 - 4s/epoch - 140ms/step\n",
      "Epoch 17/50\n",
      "32/32 - 4s - loss: 0.3222 - mse: 0.3222 - 4s/epoch - 138ms/step\n",
      "Epoch 18/50\n",
      "32/32 - 4s - loss: 0.3141 - mse: 0.3141 - 4s/epoch - 134ms/step\n",
      "Epoch 19/50\n",
      "32/32 - 5s - loss: 0.3003 - mse: 0.3003 - 5s/epoch - 143ms/step\n",
      "Epoch 20/50\n",
      "32/32 - 5s - loss: 0.2918 - mse: 0.2918 - 5s/epoch - 143ms/step\n",
      "Epoch 21/50\n",
      "32/32 - 5s - loss: 0.2909 - mse: 0.2909 - 5s/epoch - 161ms/step\n",
      "Epoch 22/50\n",
      "32/32 - 5s - loss: 0.3068 - mse: 0.3068 - 5s/epoch - 148ms/step\n",
      "Epoch 23/50\n",
      "32/32 - 6s - loss: 0.2875 - mse: 0.2875 - 6s/epoch - 178ms/step\n",
      "Epoch 24/50\n",
      "32/32 - 4s - loss: 0.2758 - mse: 0.2758 - 4s/epoch - 140ms/step\n",
      "Epoch 25/50\n",
      "32/32 - 4s - loss: 0.2659 - mse: 0.2659 - 4s/epoch - 128ms/step\n",
      "Epoch 26/50\n",
      "32/32 - 4s - loss: 0.2628 - mse: 0.2628 - 4s/epoch - 127ms/step\n",
      "Epoch 27/50\n",
      "32/32 - 4s - loss: 0.2604 - mse: 0.2604 - 4s/epoch - 125ms/step\n",
      "Epoch 28/50\n",
      "32/32 - 4s - loss: 0.2635 - mse: 0.2635 - 4s/epoch - 123ms/step\n",
      "Epoch 29/50\n",
      "32/32 - 4s - loss: 0.2527 - mse: 0.2527 - 4s/epoch - 120ms/step\n",
      "Epoch 30/50\n",
      "32/32 - 4s - loss: 0.2498 - mse: 0.2498 - 4s/epoch - 123ms/step\n",
      "Epoch 31/50\n",
      "32/32 - 5s - loss: 0.2447 - mse: 0.2447 - 5s/epoch - 147ms/step\n",
      "Epoch 32/50\n",
      "32/32 - 5s - loss: 0.2383 - mse: 0.2383 - 5s/epoch - 146ms/step\n",
      "Epoch 33/50\n",
      "32/32 - 4s - loss: 0.2378 - mse: 0.2378 - 4s/epoch - 136ms/step\n",
      "Epoch 34/50\n",
      "32/32 - 4s - loss: 0.2303 - mse: 0.2303 - 4s/epoch - 130ms/step\n",
      "Epoch 35/50\n",
      "32/32 - 4s - loss: 0.2235 - mse: 0.2235 - 4s/epoch - 133ms/step\n",
      "Epoch 36/50\n",
      "32/32 - 4s - loss: 0.2310 - mse: 0.2310 - 4s/epoch - 129ms/step\n",
      "Epoch 37/50\n",
      "32/32 - 5s - loss: 0.2180 - mse: 0.2180 - 5s/epoch - 143ms/step\n",
      "Epoch 38/50\n",
      "32/32 - 5s - loss: 0.2133 - mse: 0.2133 - 5s/epoch - 150ms/step\n",
      "Epoch 39/50\n",
      "32/32 - 5s - loss: 0.2144 - mse: 0.2144 - 5s/epoch - 147ms/step\n",
      "Epoch 40/50\n",
      "32/32 - 5s - loss: 0.2184 - mse: 0.2184 - 5s/epoch - 150ms/step\n",
      "Epoch 41/50\n",
      "32/32 - 6s - loss: 0.2048 - mse: 0.2048 - 6s/epoch - 175ms/step\n",
      "Epoch 42/50\n",
      "32/32 - 4s - loss: 0.2110 - mse: 0.2110 - 4s/epoch - 138ms/step\n",
      "Epoch 43/50\n",
      "32/32 - 5s - loss: 0.2019 - mse: 0.2019 - 5s/epoch - 153ms/step\n",
      "Epoch 44/50\n",
      "32/32 - 4s - loss: 0.1918 - mse: 0.1918 - 4s/epoch - 132ms/step\n",
      "Epoch 45/50\n",
      "32/32 - 4s - loss: 0.1892 - mse: 0.1892 - 4s/epoch - 138ms/step\n",
      "Epoch 46/50\n",
      "32/32 - 5s - loss: 0.1844 - mse: 0.1844 - 5s/epoch - 152ms/step\n",
      "Epoch 47/50\n",
      "32/32 - 5s - loss: 0.1827 - mse: 0.1827 - 5s/epoch - 165ms/step\n",
      "Epoch 48/50\n",
      "32/32 - 4s - loss: 0.1775 - mse: 0.1775 - 4s/epoch - 136ms/step\n",
      "Epoch 49/50\n",
      "32/32 - 4s - loss: 0.1758 - mse: 0.1758 - 4s/epoch - 127ms/step\n",
      "Epoch 50/50\n",
      "32/32 - 4s - loss: 0.1806 - mse: 0.1806 - 4s/epoch - 123ms/step\n",
      "Score for fold 3: mse of [0.18080876767635345, 0.18080876767635345]\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/50\n",
      "32/32 - 4s - loss: 2.5485 - mse: 2.5485 - 4s/epoch - 113ms/step\n",
      "Epoch 2/50\n",
      "32/32 - 3s - loss: 1.4029 - mse: 1.4029 - 3s/epoch - 105ms/step\n",
      "Epoch 3/50\n",
      "32/32 - 4s - loss: 1.3629 - mse: 1.3629 - 4s/epoch - 115ms/step\n",
      "Epoch 4/50\n",
      "32/32 - 4s - loss: 1.3215 - mse: 1.3215 - 4s/epoch - 119ms/step\n",
      "Epoch 5/50\n",
      "32/32 - 4s - loss: 1.2680 - mse: 1.2680 - 4s/epoch - 140ms/step\n",
      "Epoch 6/50\n",
      "32/32 - 5s - loss: 1.1693 - mse: 1.1693 - 5s/epoch - 154ms/step\n",
      "Epoch 7/50\n",
      "32/32 - 5s - loss: 0.9859 - mse: 0.9859 - 5s/epoch - 146ms/step\n",
      "Epoch 8/50\n",
      "32/32 - 5s - loss: 0.7944 - mse: 0.7944 - 5s/epoch - 145ms/step\n",
      "Epoch 9/50\n",
      "32/32 - 5s - loss: 0.6443 - mse: 0.6443 - 5s/epoch - 141ms/step\n",
      "Epoch 10/50\n",
      "32/32 - 4s - loss: 0.5382 - mse: 0.5382 - 4s/epoch - 135ms/step\n",
      "Epoch 11/50\n",
      "32/32 - 5s - loss: 0.4734 - mse: 0.4734 - 5s/epoch - 145ms/step\n",
      "Epoch 12/50\n",
      "32/32 - 5s - loss: 0.4318 - mse: 0.4318 - 5s/epoch - 147ms/step\n",
      "Epoch 13/50\n",
      "32/32 - 4s - loss: 0.4056 - mse: 0.4056 - 4s/epoch - 130ms/step\n",
      "Epoch 14/50\n",
      "32/32 - 4s - loss: 0.3890 - mse: 0.3890 - 4s/epoch - 138ms/step\n",
      "Epoch 15/50\n",
      "32/32 - 5s - loss: 0.3863 - mse: 0.3863 - 5s/epoch - 165ms/step\n",
      "Epoch 16/50\n",
      "32/32 - 5s - loss: 0.3641 - mse: 0.3641 - 5s/epoch - 170ms/step\n",
      "Epoch 17/50\n",
      "32/32 - 7s - loss: 0.3561 - mse: 0.3561 - 7s/epoch - 207ms/step\n",
      "Epoch 18/50\n",
      "32/32 - 5s - loss: 0.3472 - mse: 0.3472 - 5s/epoch - 152ms/step\n",
      "Epoch 19/50\n",
      "32/32 - 6s - loss: 0.3460 - mse: 0.3460 - 6s/epoch - 182ms/step\n",
      "Epoch 20/50\n",
      "32/32 - 5s - loss: 0.3478 - mse: 0.3478 - 5s/epoch - 160ms/step\n",
      "Epoch 21/50\n",
      "32/32 - 5s - loss: 0.3331 - mse: 0.3331 - 5s/epoch - 144ms/step\n",
      "Epoch 22/50\n",
      "32/32 - 4s - loss: 0.3395 - mse: 0.3395 - 4s/epoch - 136ms/step\n",
      "Epoch 23/50\n",
      "32/32 - 4s - loss: 0.3248 - mse: 0.3248 - 4s/epoch - 140ms/step\n",
      "Epoch 24/50\n",
      "32/32 - 5s - loss: 0.3162 - mse: 0.3162 - 5s/epoch - 142ms/step\n",
      "Epoch 25/50\n",
      "32/32 - 4s - loss: 0.3144 - mse: 0.3144 - 4s/epoch - 136ms/step\n",
      "Epoch 26/50\n",
      "32/32 - 4s - loss: 0.3017 - mse: 0.3017 - 4s/epoch - 134ms/step\n",
      "Epoch 27/50\n",
      "32/32 - 4s - loss: 0.3021 - mse: 0.3021 - 4s/epoch - 129ms/step\n",
      "Epoch 28/50\n",
      "32/32 - 4s - loss: 0.2998 - mse: 0.2998 - 4s/epoch - 132ms/step\n",
      "Epoch 29/50\n",
      "32/32 - 5s - loss: 0.3008 - mse: 0.3008 - 5s/epoch - 142ms/step\n",
      "Epoch 30/50\n",
      "32/32 - 4s - loss: 0.2887 - mse: 0.2887 - 4s/epoch - 132ms/step\n",
      "Epoch 31/50\n",
      "32/32 - 5s - loss: 0.2791 - mse: 0.2791 - 5s/epoch - 156ms/step\n",
      "Epoch 32/50\n",
      "32/32 - 4s - loss: 0.2799 - mse: 0.2799 - 4s/epoch - 134ms/step\n",
      "Epoch 33/50\n",
      "32/32 - 4s - loss: 0.2830 - mse: 0.2830 - 4s/epoch - 132ms/step\n",
      "Epoch 34/50\n",
      "32/32 - 4s - loss: 0.2708 - mse: 0.2708 - 4s/epoch - 139ms/step\n",
      "Epoch 35/50\n",
      "32/32 - 5s - loss: 0.2631 - mse: 0.2631 - 5s/epoch - 170ms/step\n",
      "Epoch 36/50\n",
      "32/32 - 4s - loss: 0.2670 - mse: 0.2670 - 4s/epoch - 127ms/step\n",
      "Epoch 37/50\n",
      "32/32 - 4s - loss: 0.2624 - mse: 0.2624 - 4s/epoch - 130ms/step\n",
      "Epoch 38/50\n",
      "32/32 - 5s - loss: 0.2729 - mse: 0.2729 - 5s/epoch - 161ms/step\n",
      "Epoch 39/50\n",
      "32/32 - 4s - loss: 0.2545 - mse: 0.2545 - 4s/epoch - 137ms/step\n",
      "Epoch 40/50\n",
      "32/32 - 4s - loss: 0.2481 - mse: 0.2481 - 4s/epoch - 132ms/step\n",
      "Epoch 41/50\n",
      "32/32 - 4s - loss: 0.2498 - mse: 0.2498 - 4s/epoch - 137ms/step\n",
      "Epoch 42/50\n",
      "32/32 - 4s - loss: 0.2451 - mse: 0.2451 - 4s/epoch - 129ms/step\n",
      "Epoch 43/50\n",
      "32/32 - 4s - loss: 0.2380 - mse: 0.2380 - 4s/epoch - 125ms/step\n",
      "Epoch 44/50\n",
      "32/32 - 4s - loss: 0.2352 - mse: 0.2352 - 4s/epoch - 133ms/step\n",
      "Epoch 45/50\n",
      "32/32 - 4s - loss: 0.2302 - mse: 0.2302 - 4s/epoch - 128ms/step\n",
      "Epoch 46/50\n",
      "32/32 - 4s - loss: 0.2375 - mse: 0.2375 - 4s/epoch - 130ms/step\n",
      "Epoch 47/50\n",
      "32/32 - 4s - loss: 0.2238 - mse: 0.2238 - 4s/epoch - 130ms/step\n",
      "Epoch 48/50\n",
      "32/32 - 4s - loss: 0.2235 - mse: 0.2235 - 4s/epoch - 128ms/step\n",
      "Epoch 49/50\n",
      "32/32 - 4s - loss: 0.2187 - mse: 0.2187 - 4s/epoch - 130ms/step\n",
      "Epoch 50/50\n",
      "32/32 - 4s - loss: 0.2318 - mse: 0.2318 - 4s/epoch - 123ms/step\n",
      "Score for fold 4: mse of [0.24397186934947968, 0.24397186934947968]\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/50\n",
      "32/32 - 4s - loss: 1.7785 - mse: 1.7785 - 4s/epoch - 113ms/step\n",
      "Epoch 2/50\n",
      "32/32 - 4s - loss: 1.1725 - mse: 1.1725 - 4s/epoch - 114ms/step\n",
      "Epoch 3/50\n",
      "32/32 - 4s - loss: 0.9423 - mse: 0.9423 - 4s/epoch - 116ms/step\n",
      "Epoch 4/50\n",
      "32/32 - 4s - loss: 0.7122 - mse: 0.7122 - 4s/epoch - 120ms/step\n",
      "Epoch 5/50\n",
      "32/32 - 7s - loss: 0.5426 - mse: 0.5426 - 7s/epoch - 205ms/step\n",
      "Epoch 6/50\n",
      "32/32 - 5s - loss: 0.4596 - mse: 0.4596 - 5s/epoch - 144ms/step\n",
      "Epoch 7/50\n",
      "32/32 - 4s - loss: 0.4220 - mse: 0.4220 - 4s/epoch - 122ms/step\n",
      "Epoch 8/50\n",
      "32/32 - 4s - loss: 0.3843 - mse: 0.3843 - 4s/epoch - 121ms/step\n",
      "Epoch 9/50\n",
      "32/32 - 4s - loss: 0.3616 - mse: 0.3616 - 4s/epoch - 133ms/step\n",
      "Epoch 10/50\n",
      "32/32 - 4s - loss: 0.3435 - mse: 0.3435 - 4s/epoch - 130ms/step\n",
      "Epoch 11/50\n",
      "32/32 - 4s - loss: 0.3305 - mse: 0.3305 - 4s/epoch - 136ms/step\n",
      "Epoch 12/50\n",
      "32/32 - 4s - loss: 0.3183 - mse: 0.3183 - 4s/epoch - 136ms/step\n",
      "Epoch 13/50\n",
      "32/32 - 4s - loss: 0.3117 - mse: 0.3117 - 4s/epoch - 138ms/step\n",
      "Epoch 14/50\n",
      "32/32 - 5s - loss: 0.3007 - mse: 0.3007 - 5s/epoch - 141ms/step\n",
      "Epoch 15/50\n",
      "32/32 - 4s - loss: 0.2935 - mse: 0.2935 - 4s/epoch - 135ms/step\n",
      "Epoch 16/50\n",
      "32/32 - 5s - loss: 0.2821 - mse: 0.2821 - 5s/epoch - 142ms/step\n",
      "Epoch 17/50\n",
      "32/32 - 4s - loss: 0.2735 - mse: 0.2735 - 4s/epoch - 140ms/step\n",
      "Epoch 18/50\n",
      "32/32 - 4s - loss: 0.2597 - mse: 0.2597 - 4s/epoch - 128ms/step\n",
      "Epoch 19/50\n",
      "32/32 - 4s - loss: 0.2522 - mse: 0.2522 - 4s/epoch - 131ms/step\n",
      "Epoch 20/50\n",
      "32/32 - 4s - loss: 0.2604 - mse: 0.2604 - 4s/epoch - 125ms/step\n",
      "Epoch 21/50\n",
      "32/32 - 4s - loss: 0.2473 - mse: 0.2473 - 4s/epoch - 126ms/step\n",
      "Epoch 22/50\n",
      "32/32 - 4s - loss: 0.2361 - mse: 0.2361 - 4s/epoch - 138ms/step\n",
      "Epoch 23/50\n",
      "32/32 - 4s - loss: 0.2375 - mse: 0.2375 - 4s/epoch - 136ms/step\n",
      "Epoch 24/50\n",
      "32/32 - 4s - loss: 0.2283 - mse: 0.2283 - 4s/epoch - 136ms/step\n",
      "Epoch 25/50\n",
      "32/32 - 4s - loss: 0.2185 - mse: 0.2185 - 4s/epoch - 133ms/step\n",
      "Epoch 26/50\n",
      "32/32 - 4s - loss: 0.2185 - mse: 0.2185 - 4s/epoch - 135ms/step\n",
      "Epoch 27/50\n",
      "32/32 - 5s - loss: 0.2053 - mse: 0.2053 - 5s/epoch - 169ms/step\n",
      "Epoch 28/50\n",
      "32/32 - 4s - loss: 0.2018 - mse: 0.2018 - 4s/epoch - 130ms/step\n",
      "Epoch 29/50\n",
      "32/32 - 5s - loss: 0.2006 - mse: 0.2006 - 5s/epoch - 141ms/step\n",
      "Epoch 30/50\n",
      "32/32 - 4s - loss: 0.1959 - mse: 0.1959 - 4s/epoch - 136ms/step\n",
      "Epoch 31/50\n",
      "32/32 - 5s - loss: 0.1875 - mse: 0.1875 - 5s/epoch - 141ms/step\n",
      "Epoch 32/50\n",
      "32/32 - 4s - loss: 0.1863 - mse: 0.1863 - 4s/epoch - 136ms/step\n",
      "Epoch 33/50\n",
      "32/32 - 4s - loss: 0.1832 - mse: 0.1832 - 4s/epoch - 138ms/step\n",
      "Epoch 34/50\n",
      "32/32 - 4s - loss: 0.1811 - mse: 0.1811 - 4s/epoch - 137ms/step\n",
      "Epoch 35/50\n",
      "32/32 - 4s - loss: 0.1768 - mse: 0.1768 - 4s/epoch - 136ms/step\n",
      "Epoch 36/50\n",
      "32/32 - 4s - loss: 0.1715 - mse: 0.1715 - 4s/epoch - 137ms/step\n",
      "Epoch 37/50\n",
      "32/32 - 4s - loss: 0.1642 - mse: 0.1642 - 4s/epoch - 132ms/step\n",
      "Epoch 38/50\n",
      "32/32 - 4s - loss: 0.1653 - mse: 0.1653 - 4s/epoch - 126ms/step\n",
      "Epoch 39/50\n",
      "32/32 - 4s - loss: 0.1571 - mse: 0.1571 - 4s/epoch - 138ms/step\n",
      "Epoch 40/50\n",
      "32/32 - 4s - loss: 0.1626 - mse: 0.1626 - 4s/epoch - 127ms/step\n",
      "Epoch 41/50\n",
      "32/32 - 4s - loss: 0.1484 - mse: 0.1484 - 4s/epoch - 125ms/step\n",
      "Epoch 42/50\n",
      "32/32 - 4s - loss: 0.1525 - mse: 0.1525 - 4s/epoch - 131ms/step\n",
      "Epoch 43/50\n",
      "32/32 - 4s - loss: 0.1419 - mse: 0.1419 - 4s/epoch - 126ms/step\n",
      "Epoch 44/50\n",
      "32/32 - 5s - loss: 0.1429 - mse: 0.1429 - 5s/epoch - 141ms/step\n",
      "Epoch 45/50\n",
      "32/32 - 4s - loss: 0.1379 - mse: 0.1379 - 4s/epoch - 132ms/step\n",
      "Epoch 46/50\n",
      "32/32 - 4s - loss: 0.1339 - mse: 0.1339 - 4s/epoch - 131ms/step\n",
      "Epoch 47/50\n",
      "32/32 - 4s - loss: 0.1285 - mse: 0.1285 - 4s/epoch - 126ms/step\n",
      "Epoch 48/50\n",
      "32/32 - 4s - loss: 0.1284 - mse: 0.1284 - 4s/epoch - 126ms/step\n",
      "Epoch 49/50\n",
      "32/32 - 4s - loss: 0.1253 - mse: 0.1253 - 4s/epoch - 139ms/step\n",
      "Epoch 50/50\n",
      "32/32 - 4s - loss: 0.1271 - mse: 0.1271 - 4s/epoch - 131ms/step\n",
      "Score for fold 5: mse of [0.14179131388664246, 0.14179131388664246]\n"
     ]
    }
   ],
   "source": [
    "# I use mean squared error over mean absolute error here (as both the loss and metric) because in cases like \n",
    "# diagnosing Alzheimer's I think it's especially important to avoid large errors, like mistaking someone with no\n",
    "# Alzheimer's at all for having moderate Alzheimer's.\n",
    "\n",
    "# I resize the data, dividing the height and width by 4, because the large images would mean so many variables that\n",
    "# the neural network would run extremely slowly.\n",
    "\n",
    "# def scheduler(epoch, lr):\n",
    "#     if epoch < 10:\n",
    "#         return lr\n",
    "#     else:\n",
    "#         return lr * tf.math.exp(-0.1)\n",
    "\n",
    "# optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.01)\n",
    "\n",
    "# callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 101)\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "for train, valid in folds.split(X_trainOver, Y_trainOver):\n",
    "    alzCNN = tf.keras.Sequential()\n",
    "    \n",
    "    alzCNN.add(tf.keras.layers.Resizing(height = 52, width = 44))\n",
    "\n",
    "    alzCNN.add(tf.keras.layers.Conv2D(filters=6,\n",
    "                                 kernel_size=(3, 3),\n",
    "                                 activation='relu',\n",
    "                                 input_shape=(52, 44, 1)))\n",
    "\n",
    "    alzCNN.add(tf.keras.layers.Conv2D(filters=12,\n",
    "                                 kernel_size=(3, 3),\n",
    "                                 activation='relu'))\n",
    "    \n",
    "    alzCNN.add(tf.keras.layers.Conv2D(filters=16,\n",
    "                                 kernel_size=(5, 5),\n",
    "                                 activation='relu'))\n",
    "\n",
    "    alzCNN.add(tf.keras.layers.AveragePooling2D())\n",
    "\n",
    "    alzCNN.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    alzCNN.add(tf.keras.layers.Dense(units=40,\n",
    "                       activation='relu'))\n",
    "    alzCNN.add(tf.keras.layers.Dense(units=20,\n",
    "                       activation='relu'))\n",
    "    alzCNN.add(tf.keras.layers.Dense(units=1,\n",
    "                       activation='linear'))\n",
    "    \n",
    "    alzCNN.compile(loss='mse',\n",
    "                optimizer='adam',\n",
    "                metrics=['mse'])\n",
    "    \n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    \n",
    "    alzCNN.fit(X_trainOver2[train], Y_trainOver[train],\n",
    "              batch_size=256,\n",
    "              epochs=50,\n",
    "              verbose=2,)\n",
    "    \n",
    "    score = alzCNN.evaluate(X_trainOver2[valid], Y_trainOver[valid], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: mse of {score}')\n",
    "        \n",
    "    fold_no = fold_no + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a540215a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alzCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m alzCNN\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'alzCNN' is not defined"
     ]
    }
   ],
   "source": [
    "alzCNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d11d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(Y_trainOver)\n",
    "\n",
    "# In my initial attempts at a neural network, the MSE loss was about the same as the variance, meaning you could \n",
    "# get just as good results by always guessing the mean. I started out with four dense layers going from 120 to 80 \n",
    "# to 20 to 1. Because I had so few images I reduced the number of dense layers, instead having three layers of 40, \n",
    "# 20 and 1. This did nothing to change the final MSE loss, which was between 1.26 and 1.28 for every fold. Raising\n",
    "# the learning rate to 0.01, including with a schedule to lower it after the tenth epoch, made the loss drop\n",
    "# far quicker, but after the tenth epoch or so, regardless of whether there was a schedule, the rate stopped\n",
    "# dropping, staying at about 1.24. I tried doing a scheduled learning rate that started at 0.01 and then \n",
    "# exponentially decreased, which got the MSE loss down to 1.24 but no lower. I also tried an SGD learning rate and\n",
    "# adding a third convolutional layer with 20 filters, none of them had any appreciable effect on the loss. This is\n",
    "# likely because there aren't enough images in my data set to train a neural network from scratch.\n",
    "\n",
    "# I then realized that I had accidentally made my second to last layer softmax instead of relu. Upon fixing this\n",
    "# and running the learning rate schedule I had run before (with three convolutional layers of (3,3) kernel size, I \n",
    "# got unusual results; the first fold's loss went as low as 0.0196 on the 47th epoch, settling at 0.0335 at the \n",
    "# 50th, but every other fold remained stuck at around 1.24-1.25 or the same as the variance. This seemed to be \n",
    "# towards how lucky I was with the randomized parameters at the start of the training; on the first fold this number  \n",
    "# was 3.89 but on later folds it varied from 14 to 209. However, this problem was fixed when I switched from the \n",
    "# custom schedule to constant learning rates. I also fixed the mistake that I had been looking at the training fold\n",
    "# scores and not the evaluation using the validation fold.\n",
    "\n",
    "# Here were my parameter search results for average evaluation score across the folds:\n",
    "\n",
    "# adam, 3 convolution layers of all 3,3 kernel size, stride 1: 0.0765\n",
    "# SGD, 3 convolution layers of all 3,3 kernel size, stride 1: 0.2787\n",
    "# adamax, 3 convolution layers of all 3,3 kernel size, stride 1: 0.1807\n",
    "# adam, 3 convolution layers of 3,3/3,3/5,5 kernel size, stride 1: 0.0569\n",
    "# adam, 3 convolution layers of all 3,3 kernel size, stride 1 for first two layers and stride 2 for the last layer:\n",
    "# 0.1422\n",
    "\n",
    "# I also did tests for 3,3/5,5/7,7 or 3,3/5,5/7,7 layers with adam and stride 1, and four layers,\n",
    "# however I accidentally did these while only looking at the training set loss and not on the evaluated score, \n",
    "# ignoring potential overfit. However,on all the tests I did do, lower training set loss always meant lower final \n",
    "# score, and I did not get better results than my 3,3/3,3/5,5 score in any of these, so I didn't repeat these tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0387d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10240, 208, 176, 3)\n"
     ]
    }
   ],
   "source": [
    "# Here I make a version of X_trainOver with three color channels that are actually the same thing repeated three\n",
    "# times - this is for compatibility with resNet, which requires three color channels.\n",
    "\n",
    "X_trainOver3 = np.repeat(X_trainOver[..., np.newaxis], 3, -1)\n",
    "print(X_trainOver3.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c689b905",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m------------------------------------------------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m resModel\u001b[38;5;241m.\u001b[39mfit(X_trainOver3[train], Y_trainOver[train],\n\u001b[1;32m     47\u001b[0m           batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     48\u001b[0m           epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     49\u001b[0m           verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,)\n\u001b[1;32m     51\u001b[0m fold_no \u001b[38;5;241m=\u001b[39m fold_no \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:1682\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1672\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_coordinator \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1673\u001b[0m         tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mcoordinator\u001b[38;5;241m.\u001b[39mClusterCoordinator(\n\u001b[1;32m   1674\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\n\u001b[1;32m   1675\u001b[0m         )\n\u001b[1;32m   1676\u001b[0m     )\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   1679\u001b[0m     \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1680\u001b[0m ):\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[0;32m-> 1682\u001b[0m     data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1683\u001b[0m         x\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m   1684\u001b[0m         y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m   1685\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m   1686\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1687\u001b[0m         steps_per_epoch\u001b[38;5;241m=\u001b[39msteps_per_epoch,\n\u001b[1;32m   1688\u001b[0m         initial_epoch\u001b[38;5;241m=\u001b[39minitial_epoch,\n\u001b[1;32m   1689\u001b[0m         epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m   1690\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[1;32m   1691\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39mclass_weight,\n\u001b[1;32m   1692\u001b[0m         max_queue_size\u001b[38;5;241m=\u001b[39mmax_queue_size,\n\u001b[1;32m   1693\u001b[0m         workers\u001b[38;5;241m=\u001b[39mworkers,\n\u001b[1;32m   1694\u001b[0m         use_multiprocessing\u001b[38;5;241m=\u001b[39muse_multiprocessing,\n\u001b[1;32m   1695\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1696\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[1;32m   1697\u001b[0m     )\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1678\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1285\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[1;32m   1284\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m adapter_cls(\n\u001b[1;32m   1286\u001b[0m     x,\n\u001b[1;32m   1287\u001b[0m     y,\n\u001b[1;32m   1288\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1289\u001b[0m     steps\u001b[38;5;241m=\u001b[39msteps_per_epoch,\n\u001b[1;32m   1290\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs \u001b[38;5;241m-\u001b[39m initial_epoch,\n\u001b[1;32m   1291\u001b[0m     sample_weights\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m   1292\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[1;32m   1293\u001b[0m     max_queue_size\u001b[38;5;241m=\u001b[39mmax_queue_size,\n\u001b[1;32m   1294\u001b[0m     workers\u001b[38;5;241m=\u001b[39mworkers,\n\u001b[1;32m   1295\u001b[0m     use_multiprocessing\u001b[38;5;241m=\u001b[39muse_multiprocessing,\n\u001b[1;32m   1296\u001b[0m     distribution_strategy\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy(),\n\u001b[1;32m   1297\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1298\u001b[0m     pss_evaluation_shards\u001b[38;5;241m=\u001b[39mpss_evaluation_shards,\n\u001b[1;32m   1299\u001b[0m )\n\u001b[1;32m   1301\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:253\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    242\u001b[0m     x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    251\u001b[0m ):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 253\u001b[0m     x, y, sample_weights \u001b[38;5;241m=\u001b[39m _process_tensorlike((x, y, sample_weights))\n\u001b[1;32m    254\u001b[0m     sample_weight_modes \u001b[38;5;241m=\u001b[39m broadcast_sample_weight_modes(\n\u001b[1;32m    255\u001b[0m         sample_weights, sample_weight_modes\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# If sample_weights are not specified for an output use 1.0 as weights.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1163\u001b[0m, in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m-> 1163\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(_convert_single_tensor, inputs)\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mlist_to_tuple(inputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest.py:624\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnest.map_structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_structure\u001b[39m(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    540\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m    ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 624\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m nest_util\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    625\u001b[0m       nest_util\u001b[38;5;241m.\u001b[39mModality\u001b[38;5;241m.\u001b[39mCORE, func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    626\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1054\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \n\u001b[1;32m    959\u001b[0m \u001b[38;5;124;03m- For Modality.CORE: Refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;124;03m  ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mCORE:\n\u001b[0;32m-> 1054\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_map_structure(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[1;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_map_structure(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1094\u001b[0m, in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1090\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1093\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m-> 1094\u001b[0m     [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m   1095\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[1;32m   1096\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1094\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1089\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1090\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1093\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m-> 1094\u001b[0m     [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m   1095\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[1;32m   1096\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1158\u001b[0m, in \u001b[0;36m_process_tensorlike.<locals>._convert_single_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39mfloating):\n\u001b[1;32m   1157\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mfloatx()\n\u001b[0;32m-> 1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_scipy_sparse(x):\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion.py:160\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m\u001b[38;5;241m.\u001b[39mtf_export(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[1;32m     98\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     99\u001b[0m ):\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_tensor_v2(\n\u001b[1;32m    161\u001b[0m       value, dtype\u001b[38;5;241m=\u001b[39mdtype, dtype_hint\u001b[38;5;241m=\u001b[39mdtype_hint, name\u001b[38;5;241m=\u001b[39mname\n\u001b[1;32m    162\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion.py:168\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# preferred_dtype = preferred_dtype or dtype_hint\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(\n\u001b[1;32m    169\u001b[0m     value, dtype, name, preferred_dtype\u001b[38;5;241m=\u001b[39mdtype_hint\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m conversion_func(value, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname, as_ref\u001b[38;5;241m=\u001b[39mas_ref)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:324\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    322\u001b[0m                                          as_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    323\u001b[0m   _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m--> 324\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m constant(v, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:263\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    264\u001b[0m                         allow_broadcast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:275\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    274\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 275\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    277\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    279\u001b[0m )\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:285\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    284\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m   t \u001b[38;5;241m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    286\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:86\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     83\u001b[0m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[1;32m     84\u001b[0m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[1;32m     85\u001b[0m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m   value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[1;32m     88\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Here I attempt transfer learning with resNet, which could correct the problem of me having very few images by \n",
    "# using another neural network with similar images, freezing the early layers, and using the final layers to \n",
    "# customize it for my particular data set. Unfortunately, while I managed to debug all the errors in this code,\n",
    "# my computer doesn't seem to have the processing power to run such a large neural network and it crashes my kernel\n",
    "# before getting into an epoch every time I tried. Since my \"homemade\" neural network still performed well above\n",
    "# the variance, I accepted this, though I kept the code to show that I am capable of running ResNet (if I had more\n",
    "# time I would have tried to figure out how to offload the processing onto something else, I tried to do this with\n",
    "# google Colab but the code ran slower there than on Jupyter.\n",
    "\n",
    "folds2 = KFold(n_splits = 5, shuffle = True, random_state = 102)\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "for train, valid in folds2.split(X_trainOver3, Y_trainOver):\n",
    "\n",
    "    resModel = tf.keras.Sequential()\n",
    "    \n",
    "\n",
    "    pretrained_model = tf.keras.applications.ResNet101(\n",
    "        include_top=False,\n",
    "        weights= 'imagenet',\n",
    "        input_tensor=None,\n",
    "        input_shape=(52, 44, 3),\n",
    "        pooling='avg')\n",
    "\n",
    "    for layer in pretrained_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    resModel.add(tf.keras.layers.Resizing(height = 52, width = 44))\n",
    "    \n",
    "    resModel.add(pretrained_model)\n",
    "    \n",
    "#     resModel.add(tf.keras.layers.Conv2D(filters=16,\n",
    "#                                      kernel_size=(3, 3),\n",
    "#                                      activation='relu'))\n",
    "    \n",
    "#     resModel.add(tf.keras.layers.AveragePooling2D())\n",
    "\n",
    "    resModel.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#     resModel.add(tf.keras.layers.Dense(units=40,\n",
    "#                        activation='relu'))\n",
    "    resModel.add(tf.keras.layers.Dense(units=20,\n",
    "                       activation='relu'))\n",
    "    resModel.add(tf.keras.layers.Dense(units=1,\n",
    "                       activation='linear'))\n",
    "\n",
    "    resModel.compile(loss='mse',\n",
    "                optimizer='adam',\n",
    "                metrics=['mse'])\n",
    "    \n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    \n",
    "    resModel.fit(X_trainOver3[train], Y_trainOver[train],\n",
    "              batch_size=256,\n",
    "              epochs=50,\n",
    "              verbose=2,)\n",
    "        \n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7a0737f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "40/40 - 4s - loss: 0.1239 - mse: 0.1239 - 4s/epoch - 93ms/step\n",
      "Epoch 2/50\n",
      "40/40 - 4s - loss: 0.1267 - mse: 0.1267 - 4s/epoch - 107ms/step\n",
      "Epoch 3/50\n",
      "40/40 - 4s - loss: 0.1156 - mse: 0.1156 - 4s/epoch - 111ms/step\n",
      "Epoch 4/50\n",
      "40/40 - 5s - loss: 0.1176 - mse: 0.1176 - 5s/epoch - 123ms/step\n",
      "Epoch 5/50\n",
      "40/40 - 4s - loss: 0.1152 - mse: 0.1152 - 4s/epoch - 104ms/step\n",
      "Epoch 6/50\n",
      "40/40 - 4s - loss: 0.1085 - mse: 0.1085 - 4s/epoch - 105ms/step\n",
      "Epoch 7/50\n",
      "40/40 - 4s - loss: 0.1029 - mse: 0.1029 - 4s/epoch - 106ms/step\n",
      "Epoch 8/50\n",
      "40/40 - 4s - loss: 0.1138 - mse: 0.1138 - 4s/epoch - 110ms/step\n",
      "Epoch 9/50\n",
      "40/40 - 4s - loss: 0.1033 - mse: 0.1033 - 4s/epoch - 100ms/step\n",
      "Epoch 10/50\n",
      "40/40 - 4s - loss: 0.0985 - mse: 0.0985 - 4s/epoch - 97ms/step\n",
      "Epoch 11/50\n",
      "40/40 - 4s - loss: 0.0934 - mse: 0.0934 - 4s/epoch - 92ms/step\n",
      "Epoch 12/50\n",
      "40/40 - 4s - loss: 0.0907 - mse: 0.0907 - 4s/epoch - 93ms/step\n",
      "Epoch 13/50\n",
      "40/40 - 4s - loss: 0.0930 - mse: 0.0930 - 4s/epoch - 96ms/step\n",
      "Epoch 14/50\n",
      "40/40 - 4s - loss: 0.0879 - mse: 0.0879 - 4s/epoch - 102ms/step\n",
      "Epoch 15/50\n",
      "40/40 - 4s - loss: 0.0864 - mse: 0.0864 - 4s/epoch - 109ms/step\n",
      "Epoch 16/50\n",
      "40/40 - 4s - loss: 0.0825 - mse: 0.0825 - 4s/epoch - 103ms/step\n",
      "Epoch 17/50\n",
      "40/40 - 5s - loss: 0.0863 - mse: 0.0863 - 5s/epoch - 114ms/step\n",
      "Epoch 18/50\n",
      "40/40 - 6s - loss: 0.0822 - mse: 0.0822 - 6s/epoch - 140ms/step\n",
      "Epoch 19/50\n",
      "40/40 - 5s - loss: 0.0763 - mse: 0.0763 - 5s/epoch - 119ms/step\n",
      "Epoch 20/50\n",
      "40/40 - 4s - loss: 0.0780 - mse: 0.0780 - 4s/epoch - 106ms/step\n",
      "Epoch 21/50\n",
      "40/40 - 5s - loss: 0.0819 - mse: 0.0819 - 5s/epoch - 124ms/step\n",
      "Epoch 22/50\n",
      "40/40 - 5s - loss: 0.0764 - mse: 0.0764 - 5s/epoch - 117ms/step\n",
      "Epoch 23/50\n",
      "40/40 - 5s - loss: 0.0681 - mse: 0.0681 - 5s/epoch - 119ms/step\n",
      "Epoch 24/50\n",
      "40/40 - 5s - loss: 0.0661 - mse: 0.0661 - 5s/epoch - 117ms/step\n",
      "Epoch 25/50\n",
      "40/40 - 4s - loss: 0.0662 - mse: 0.0662 - 4s/epoch - 111ms/step\n",
      "Epoch 26/50\n",
      "40/40 - 5s - loss: 0.0617 - mse: 0.0617 - 5s/epoch - 118ms/step\n",
      "Epoch 27/50\n",
      "40/40 - 5s - loss: 0.0604 - mse: 0.0604 - 5s/epoch - 134ms/step\n",
      "Epoch 28/50\n",
      "40/40 - 4s - loss: 0.0597 - mse: 0.0597 - 4s/epoch - 111ms/step\n",
      "Epoch 29/50\n",
      "40/40 - 4s - loss: 0.0564 - mse: 0.0564 - 4s/epoch - 108ms/step\n",
      "Epoch 30/50\n",
      "40/40 - 4s - loss: 0.0554 - mse: 0.0554 - 4s/epoch - 111ms/step\n",
      "Epoch 31/50\n",
      "40/40 - 5s - loss: 0.0540 - mse: 0.0540 - 5s/epoch - 117ms/step\n",
      "Epoch 32/50\n",
      "40/40 - 4s - loss: 0.0492 - mse: 0.0492 - 4s/epoch - 103ms/step\n",
      "Epoch 33/50\n",
      "40/40 - 4s - loss: 0.0493 - mse: 0.0493 - 4s/epoch - 103ms/step\n",
      "Epoch 34/50\n",
      "40/40 - 4s - loss: 0.0549 - mse: 0.0549 - 4s/epoch - 104ms/step\n",
      "Epoch 35/50\n",
      "40/40 - 4s - loss: 0.0494 - mse: 0.0494 - 4s/epoch - 103ms/step\n",
      "Epoch 36/50\n",
      "40/40 - 4s - loss: 0.0438 - mse: 0.0438 - 4s/epoch - 105ms/step\n",
      "Epoch 37/50\n",
      "40/40 - 4s - loss: 0.0427 - mse: 0.0427 - 4s/epoch - 102ms/step\n",
      "Epoch 38/50\n",
      "40/40 - 4s - loss: 0.0415 - mse: 0.0415 - 4s/epoch - 102ms/step\n",
      "Epoch 39/50\n",
      "40/40 - 4s - loss: 0.0412 - mse: 0.0412 - 4s/epoch - 104ms/step\n",
      "Epoch 40/50\n",
      "40/40 - 4s - loss: 0.0400 - mse: 0.0400 - 4s/epoch - 102ms/step\n",
      "Epoch 41/50\n",
      "40/40 - 4s - loss: 0.0374 - mse: 0.0374 - 4s/epoch - 104ms/step\n",
      "Epoch 42/50\n",
      "40/40 - 4s - loss: 0.0367 - mse: 0.0367 - 4s/epoch - 102ms/step\n",
      "Epoch 43/50\n",
      "40/40 - 4s - loss: 0.0358 - mse: 0.0358 - 4s/epoch - 103ms/step\n",
      "Epoch 44/50\n",
      "40/40 - 4s - loss: 0.0359 - mse: 0.0359 - 4s/epoch - 106ms/step\n",
      "Epoch 45/50\n",
      "40/40 - 4s - loss: 0.0318 - mse: 0.0318 - 4s/epoch - 103ms/step\n",
      "Epoch 46/50\n",
      "40/40 - 4s - loss: 0.0328 - mse: 0.0328 - 4s/epoch - 106ms/step\n",
      "Epoch 47/50\n",
      "40/40 - 4s - loss: 0.0330 - mse: 0.0330 - 4s/epoch - 103ms/step\n",
      "Epoch 48/50\n",
      "40/40 - 4s - loss: 0.0355 - mse: 0.0355 - 4s/epoch - 104ms/step\n",
      "Epoch 49/50\n",
      "40/40 - 4s - loss: 0.0296 - mse: 0.0296 - 4s/epoch - 103ms/step\n",
      "Epoch 50/50\n",
      "40/40 - 4s - loss: 0.0269 - mse: 0.0269 - 4s/epoch - 104ms/step\n",
      "MSE = [0.6790823340415955, 0.6790823340415955]\n"
     ]
    }
   ],
   "source": [
    "#Final run through the test data:\n",
    "    \n",
    "alzCNN.fit(X_trainOver2, Y_trainOver,\n",
    "            batch_size=256,\n",
    "            epochs=50,\n",
    "            verbose=2,)\n",
    "    \n",
    "score = alzCNN.evaluate(X_test, Y_test, verbose=0)\n",
    "print(f'MSE = {score}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8072c203",
   "metadata": {},
   "source": [
    "Although the loss rent down to 0.0269 at the end, the final MSE ended up being 0.679. It seems odd that there would be this much of an overfitting problem when I used kFold, so I wondered if this was because of using oversampling on the training data but not on the test data. I thus decided, as a test (not my final result, since the goal is to see how well you can perform on the actual data) to create a version of X_test and Y_test with oversampling and see if the model performs better when evaluated based on that. \n",
    "\n",
    "I also noticed that since I already had trained this neural network on the kFolds, it started out with a much lower\n",
    "loss than it did when I started training on the kFold. I think this could be a cause of overfit, so I also tried to run a different, identical neural network that had not been trained, so I was getting an actual 50 epochs instead of an effective 100, so I would not overfit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d286a6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "40/40 - 7s - loss: 1.8300 - mse: 1.8300 - 7s/epoch - 180ms/step\n",
      "Epoch 2/50\n",
      "40/40 - 7s - loss: 1.1255 - mse: 1.1255 - 7s/epoch - 163ms/step\n",
      "Epoch 3/50\n",
      "40/40 - 6s - loss: 0.7247 - mse: 0.7247 - 6s/epoch - 140ms/step\n",
      "Epoch 4/50\n",
      "40/40 - 6s - loss: 0.4184 - mse: 0.4184 - 6s/epoch - 139ms/step\n",
      "Epoch 5/50\n",
      "40/40 - 6s - loss: 0.3530 - mse: 0.3530 - 6s/epoch - 140ms/step\n",
      "Epoch 6/50\n",
      "40/40 - 6s - loss: 0.3213 - mse: 0.3213 - 6s/epoch - 143ms/step\n",
      "Epoch 7/50\n",
      "40/40 - 6s - loss: 0.2827 - mse: 0.2827 - 6s/epoch - 147ms/step\n",
      "Epoch 8/50\n",
      "40/40 - 6s - loss: 0.2696 - mse: 0.2696 - 6s/epoch - 148ms/step\n",
      "Epoch 9/50\n",
      "40/40 - 6s - loss: 0.2576 - mse: 0.2576 - 6s/epoch - 151ms/step\n",
      "Epoch 10/50\n",
      "40/40 - 6s - loss: 0.2300 - mse: 0.2300 - 6s/epoch - 151ms/step\n",
      "Epoch 11/50\n",
      "40/40 - 6s - loss: 0.2213 - mse: 0.2213 - 6s/epoch - 153ms/step\n",
      "Epoch 12/50\n",
      "40/40 - 6s - loss: 0.2175 - mse: 0.2175 - 6s/epoch - 153ms/step\n",
      "Epoch 13/50\n",
      "40/40 - 6s - loss: 0.2069 - mse: 0.2069 - 6s/epoch - 155ms/step\n",
      "Epoch 14/50\n",
      "40/40 - 7s - loss: 0.1925 - mse: 0.1925 - 7s/epoch - 177ms/step\n",
      "Epoch 15/50\n",
      "40/40 - 7s - loss: 0.1808 - mse: 0.1808 - 7s/epoch - 173ms/step\n",
      "Epoch 16/50\n",
      "40/40 - 7s - loss: 0.1725 - mse: 0.1725 - 7s/epoch - 163ms/step\n",
      "Epoch 17/50\n",
      "40/40 - 7s - loss: 0.1667 - mse: 0.1667 - 7s/epoch - 165ms/step\n",
      "Epoch 18/50\n",
      "40/40 - 7s - loss: 0.1528 - mse: 0.1528 - 7s/epoch - 166ms/step\n",
      "Epoch 19/50\n",
      "40/40 - 7s - loss: 0.1482 - mse: 0.1482 - 7s/epoch - 165ms/step\n",
      "Epoch 20/50\n",
      "40/40 - 7s - loss: 0.1594 - mse: 0.1594 - 7s/epoch - 167ms/step\n",
      "Epoch 21/50\n",
      "40/40 - 7s - loss: 0.1426 - mse: 0.1426 - 7s/epoch - 168ms/step\n",
      "Epoch 22/50\n",
      "40/40 - 7s - loss: 0.1306 - mse: 0.1306 - 7s/epoch - 186ms/step\n",
      "Epoch 23/50\n",
      "40/40 - 7s - loss: 0.1273 - mse: 0.1273 - 7s/epoch - 169ms/step\n",
      "Epoch 24/50\n",
      "40/40 - 7s - loss: 0.1172 - mse: 0.1172 - 7s/epoch - 168ms/step\n",
      "Epoch 25/50\n",
      "40/40 - 7s - loss: 0.1318 - mse: 0.1318 - 7s/epoch - 170ms/step\n",
      "Epoch 26/50\n",
      "40/40 - 8s - loss: 0.1148 - mse: 0.1148 - 8s/epoch - 188ms/step\n",
      "Epoch 27/50\n",
      "40/40 - 7s - loss: 0.1091 - mse: 0.1091 - 7s/epoch - 173ms/step\n",
      "Epoch 28/50\n",
      "40/40 - 7s - loss: 0.0987 - mse: 0.0987 - 7s/epoch - 184ms/step\n",
      "Epoch 29/50\n",
      "40/40 - 7s - loss: 0.1003 - mse: 0.1003 - 7s/epoch - 180ms/step\n",
      "Epoch 30/50\n",
      "40/40 - 7s - loss: 0.0961 - mse: 0.0961 - 7s/epoch - 180ms/step\n",
      "Epoch 31/50\n",
      "40/40 - 7s - loss: 0.0903 - mse: 0.0903 - 7s/epoch - 179ms/step\n",
      "Epoch 32/50\n",
      "40/40 - 7s - loss: 0.0886 - mse: 0.0886 - 7s/epoch - 173ms/step\n",
      "Epoch 33/50\n",
      "40/40 - 7s - loss: 0.0914 - mse: 0.0914 - 7s/epoch - 176ms/step\n",
      "Epoch 34/50\n",
      "40/40 - 7s - loss: 0.1150 - mse: 0.1150 - 7s/epoch - 173ms/step\n",
      "Epoch 35/50\n",
      "40/40 - 7s - loss: 0.0774 - mse: 0.0774 - 7s/epoch - 175ms/step\n",
      "Epoch 36/50\n",
      "40/40 - 7s - loss: 0.0782 - mse: 0.0782 - 7s/epoch - 185ms/step\n",
      "Epoch 37/50\n",
      "40/40 - 7s - loss: 0.0761 - mse: 0.0761 - 7s/epoch - 177ms/step\n",
      "Epoch 38/50\n",
      "40/40 - 7s - loss: 0.0758 - mse: 0.0758 - 7s/epoch - 171ms/step\n",
      "Epoch 39/50\n",
      "40/40 - 7s - loss: 0.0763 - mse: 0.0763 - 7s/epoch - 170ms/step\n",
      "Epoch 40/50\n",
      "40/40 - 7s - loss: 0.0692 - mse: 0.0692 - 7s/epoch - 173ms/step\n",
      "Epoch 41/50\n",
      "40/40 - 7s - loss: 0.0565 - mse: 0.0565 - 7s/epoch - 179ms/step\n",
      "Epoch 42/50\n",
      "40/40 - 7s - loss: 0.0513 - mse: 0.0513 - 7s/epoch - 171ms/step\n",
      "Epoch 43/50\n",
      "40/40 - 7s - loss: 0.0549 - mse: 0.0549 - 7s/epoch - 178ms/step\n",
      "Epoch 44/50\n",
      "40/40 - 7s - loss: 0.0556 - mse: 0.0556 - 7s/epoch - 173ms/step\n",
      "Epoch 45/50\n",
      "40/40 - 7s - loss: 0.0440 - mse: 0.0440 - 7s/epoch - 171ms/step\n",
      "Epoch 46/50\n",
      "40/40 - 7s - loss: 0.0484 - mse: 0.0484 - 7s/epoch - 170ms/step\n",
      "Epoch 47/50\n",
      "40/40 - 7s - loss: 0.0485 - mse: 0.0485 - 7s/epoch - 171ms/step\n",
      "Epoch 48/50\n",
      "40/40 - 7s - loss: 0.0412 - mse: 0.0412 - 7s/epoch - 171ms/step\n",
      "Epoch 49/50\n",
      "40/40 - 7s - loss: 0.0372 - mse: 0.0372 - 7s/epoch - 180ms/step\n",
      "Epoch 50/50\n",
      "40/40 - 7s - loss: 0.0367 - mse: 0.0367 - 7s/epoch - 178ms/step\n",
      "MSE = [0.4971499443054199, 0.4971499443054199]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "alzCNN2 = tf.keras.Sequential()\n",
    "    \n",
    "alzCNN2.add(tf.keras.layers.Resizing(height = 52, width = 44))\n",
    "\n",
    "alzCNN2.add(tf.keras.layers.Conv2D(filters=6,\n",
    "                                kernel_size=(3, 3),\n",
    "                                activation='relu',\n",
    "                                input_shape=(52, 44, 1)))\n",
    "\n",
    "alzCNN2.add(tf.keras.layers.Conv2D(filters=12,\n",
    "                                kernel_size=(3, 3),\n",
    "                                activation='relu'))\n",
    "    \n",
    "alzCNN2.add(tf.keras.layers.Conv2D(filters=16,\n",
    "                                kernel_size=(5, 5),\n",
    "                                activation='relu'))\n",
    "\n",
    "alzCNN2.add(tf.keras.layers.AveragePooling2D())\n",
    "\n",
    "alzCNN2.add(tf.keras.layers.Flatten())\n",
    "\n",
    "alzCNN2.add(tf.keras.layers.Dense(units=40,\n",
    "                    activation='relu'))\n",
    "alzCNN2.add(tf.keras.layers.Dense(units=20,\n",
    "                    activation='relu'))\n",
    "alzCNN2.add(tf.keras.layers.Dense(units=1,\n",
    "                    activation='linear'))\n",
    "    \n",
    "alzCNN2.compile(loss='mse',\n",
    "            optimizer='adam',\n",
    "            metrics=['mse'])\n",
    "    \n",
    "alzCNN2.fit(X_trainOver2, Y_trainOver,\n",
    "            batch_size=256,\n",
    "            epochs=50,\n",
    "            verbose=2,)\n",
    "    \n",
    "score = alzCNN2.evaluate(X_test, Y_test, verbose=0)\n",
    "print(f'MSE = {score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a17e4f",
   "metadata": {},
   "source": [
    "Training a completely new neural network gets me a final result of 0.497 MSE, compared to a final loss on the training set of 0.0367, so there is still a lot of overfit although it was decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c6aab0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "40/40 - 6s - loss: 1.9753 - mse: 1.9753 - 6s/epoch - 152ms/step\n",
      "Epoch 2/50\n",
      "40/40 - 6s - loss: 1.2694 - mse: 1.2694 - 6s/epoch - 139ms/step\n",
      "Epoch 3/50\n",
      "40/40 - 6s - loss: 0.8692 - mse: 0.8692 - 6s/epoch - 138ms/step\n",
      "Epoch 4/50\n",
      "40/40 - 6s - loss: 0.5161 - mse: 0.5161 - 6s/epoch - 138ms/step\n",
      "Epoch 5/50\n",
      "40/40 - 6s - loss: 0.4334 - mse: 0.4334 - 6s/epoch - 138ms/step\n",
      "Epoch 6/50\n",
      "40/40 - 6s - loss: 0.3756 - mse: 0.3756 - 6s/epoch - 139ms/step\n",
      "Epoch 7/50\n",
      "40/40 - 6s - loss: 0.3431 - mse: 0.3431 - 6s/epoch - 139ms/step\n",
      "Epoch 8/50\n",
      "40/40 - 6s - loss: 0.3312 - mse: 0.3312 - 6s/epoch - 141ms/step\n",
      "Epoch 9/50\n",
      "40/40 - 6s - loss: 0.2991 - mse: 0.2991 - 6s/epoch - 143ms/step\n",
      "Epoch 10/50\n",
      "40/40 - 6s - loss: 0.2720 - mse: 0.2720 - 6s/epoch - 145ms/step\n",
      "Epoch 11/50\n",
      "40/40 - 7s - loss: 0.2609 - mse: 0.2609 - 7s/epoch - 168ms/step\n",
      "Epoch 12/50\n",
      "40/40 - 7s - loss: 0.2624 - mse: 0.2624 - 7s/epoch - 169ms/step\n",
      "Epoch 13/50\n",
      "40/40 - 6s - loss: 0.2339 - mse: 0.2339 - 6s/epoch - 154ms/step\n",
      "Epoch 14/50\n",
      "40/40 - 6s - loss: 0.2149 - mse: 0.2149 - 6s/epoch - 155ms/step\n",
      "Epoch 15/50\n",
      "40/40 - 6s - loss: 0.2069 - mse: 0.2069 - 6s/epoch - 156ms/step\n",
      "Epoch 16/50\n",
      "40/40 - 6s - loss: 0.2062 - mse: 0.2062 - 6s/epoch - 156ms/step\n",
      "Epoch 17/50\n",
      "40/40 - 6s - loss: 0.1846 - mse: 0.1846 - 6s/epoch - 157ms/step\n",
      "Epoch 18/50\n",
      "40/40 - 6s - loss: 0.1754 - mse: 0.1754 - 6s/epoch - 159ms/step\n",
      "Epoch 19/50\n",
      "40/40 - 6s - loss: 0.1832 - mse: 0.1832 - 6s/epoch - 160ms/step\n",
      "Epoch 20/50\n",
      "40/40 - 7s - loss: 0.1567 - mse: 0.1567 - 7s/epoch - 173ms/step\n",
      "Epoch 21/50\n",
      "40/40 - 7s - loss: 0.1518 - mse: 0.1518 - 7s/epoch - 165ms/step\n",
      "Epoch 22/50\n",
      "40/40 - 7s - loss: 0.1548 - mse: 0.1548 - 7s/epoch - 167ms/step\n",
      "Epoch 23/50\n",
      "40/40 - 7s - loss: 0.1403 - mse: 0.1403 - 7s/epoch - 174ms/step\n",
      "Epoch 24/50\n",
      "40/40 - 7s - loss: 0.1334 - mse: 0.1334 - 7s/epoch - 175ms/step\n",
      "Epoch 25/50\n",
      "40/40 - 7s - loss: 0.1225 - mse: 0.1225 - 7s/epoch - 171ms/step\n",
      "Epoch 26/50\n",
      "40/40 - 7s - loss: 0.1150 - mse: 0.1150 - 7s/epoch - 166ms/step\n",
      "Epoch 27/50\n",
      "40/40 - 7s - loss: 0.1125 - mse: 0.1125 - 7s/epoch - 168ms/step\n",
      "Epoch 28/50\n",
      "40/40 - 7s - loss: 0.1178 - mse: 0.1178 - 7s/epoch - 167ms/step\n",
      "Epoch 29/50\n",
      "40/40 - 7s - loss: 0.0972 - mse: 0.0972 - 7s/epoch - 168ms/step\n",
      "Epoch 30/50\n",
      "40/40 - 8s - loss: 0.0912 - mse: 0.0912 - 8s/epoch - 204ms/step\n",
      "Epoch 31/50\n",
      "40/40 - 7s - loss: 0.0850 - mse: 0.0850 - 7s/epoch - 184ms/step\n",
      "Epoch 32/50\n",
      "40/40 - 7s - loss: 0.0906 - mse: 0.0906 - 7s/epoch - 170ms/step\n",
      "Epoch 33/50\n",
      "40/40 - 7s - loss: 0.1017 - mse: 0.1017 - 7s/epoch - 166ms/step\n",
      "Epoch 34/50\n",
      "40/40 - 7s - loss: 0.0712 - mse: 0.0712 - 7s/epoch - 166ms/step\n",
      "Epoch 35/50\n",
      "40/40 - 7s - loss: 0.0720 - mse: 0.0720 - 7s/epoch - 168ms/step\n",
      "Epoch 36/50\n",
      "40/40 - 7s - loss: 0.0748 - mse: 0.0748 - 7s/epoch - 170ms/step\n",
      "Epoch 37/50\n",
      "40/40 - 7s - loss: 0.0625 - mse: 0.0625 - 7s/epoch - 170ms/step\n",
      "Epoch 38/50\n",
      "40/40 - 7s - loss: 0.0612 - mse: 0.0612 - 7s/epoch - 171ms/step\n",
      "Epoch 39/50\n",
      "40/40 - 7s - loss: 0.0600 - mse: 0.0600 - 7s/epoch - 171ms/step\n",
      "Epoch 40/50\n",
      "40/40 - 7s - loss: 0.0519 - mse: 0.0519 - 7s/epoch - 169ms/step\n",
      "Epoch 41/50\n",
      "40/40 - 7s - loss: 0.0491 - mse: 0.0491 - 7s/epoch - 168ms/step\n",
      "Epoch 42/50\n",
      "40/40 - 7s - loss: 0.0592 - mse: 0.0592 - 7s/epoch - 168ms/step\n",
      "Epoch 43/50\n",
      "40/40 - 7s - loss: 0.0470 - mse: 0.0470 - 7s/epoch - 168ms/step\n",
      "Epoch 44/50\n",
      "40/40 - 7s - loss: 0.0408 - mse: 0.0408 - 7s/epoch - 168ms/step\n",
      "Epoch 45/50\n",
      "40/40 - 7s - loss: 0.0431 - mse: 0.0431 - 7s/epoch - 168ms/step\n",
      "Epoch 46/50\n",
      "40/40 - 7s - loss: 0.0425 - mse: 0.0425 - 7s/epoch - 168ms/step\n",
      "Epoch 47/50\n",
      "40/40 - 7s - loss: 0.0355 - mse: 0.0355 - 7s/epoch - 181ms/step\n",
      "Epoch 48/50\n",
      "40/40 - 7s - loss: 0.0349 - mse: 0.0349 - 7s/epoch - 170ms/step\n",
      "Epoch 49/50\n",
      "40/40 - 7s - loss: 0.0396 - mse: 0.0396 - 7s/epoch - 167ms/step\n",
      "Epoch 50/50\n",
      "40/40 - 7s - loss: 0.0471 - mse: 0.0471 - 7s/epoch - 168ms/step\n",
      "MSE = [0.7594112157821655, 0.7594112157821655]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "alzCNN3 = tf.keras.Sequential()\n",
    "    \n",
    "alzCNN3.add(tf.keras.layers.Resizing(height = 52, width = 44))\n",
    "\n",
    "alzCNN3.add(tf.keras.layers.Conv2D(filters=6,\n",
    "                                kernel_size=(3, 3),\n",
    "                                activation='relu',\n",
    "                                input_shape=(52, 44, 1)))\n",
    "\n",
    "alzCNN3.add(tf.keras.layers.Conv2D(filters=12,\n",
    "                                kernel_size=(3, 3),\n",
    "                                activation='relu'))\n",
    "    \n",
    "alzCNN3.add(tf.keras.layers.Conv2D(filters=16,\n",
    "                                kernel_size=(5, 5),\n",
    "                                activation='relu'))\n",
    "\n",
    "alzCNN3.add(tf.keras.layers.AveragePooling2D())\n",
    "\n",
    "alzCNN3.add(tf.keras.layers.Flatten())\n",
    "\n",
    "alzCNN3.add(tf.keras.layers.Dense(units=40,\n",
    "                    activation='relu'))\n",
    "alzCNN3.add(tf.keras.layers.Dense(units=20,\n",
    "                    activation='relu'))\n",
    "alzCNN3.add(tf.keras.layers.Dense(units=1,\n",
    "                    activation='linear'))\n",
    "    \n",
    "alzCNN3.compile(loss='mse',\n",
    "            optimizer='adam',\n",
    "            metrics=['mse'])\n",
    "    \n",
    "alzCNN3.fit(X_trainOver2, Y_trainOver,\n",
    "            batch_size=256,\n",
    "            epochs=50,\n",
    "            verbose=2,)\n",
    "    \n",
    "score = alzCNN3.evaluate(X_testOver, Y_testOver, verbose=0)\n",
    "print(f'MSE = {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fedecb",
   "metadata": {},
   "source": [
    "Oversampling the training data actually leads to MORE overfit, at 0.759 MSE (maybe due to how resampling the severe cases from just 12 original samples leads to generated data that's wildly different from the original 52)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de213d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "40/40 - 6s - loss: 1.9078 - mse: 1.9078 - 6s/epoch - 147ms/step\n",
      "Epoch 2/50\n",
      "40/40 - 6s - loss: 0.9878 - mse: 0.9878 - 6s/epoch - 139ms/step\n",
      "Epoch 3/50\n",
      "40/40 - 6s - loss: 0.4983 - mse: 0.4983 - 6s/epoch - 139ms/step\n",
      "Epoch 4/50\n",
      "40/40 - 6s - loss: 0.4046 - mse: 0.4046 - 6s/epoch - 139ms/step\n",
      "Epoch 5/50\n",
      "40/40 - 6s - loss: 0.3411 - mse: 0.3411 - 6s/epoch - 140ms/step\n",
      "Epoch 6/50\n",
      "40/40 - 6s - loss: 0.3095 - mse: 0.3095 - 6s/epoch - 143ms/step\n",
      "Epoch 7/50\n",
      "40/40 - 6s - loss: 0.2772 - mse: 0.2772 - 6s/epoch - 145ms/step\n",
      "Epoch 8/50\n",
      "40/40 - 6s - loss: 0.2668 - mse: 0.2668 - 6s/epoch - 147ms/step\n",
      "Epoch 9/50\n",
      "40/40 - 6s - loss: 0.2366 - mse: 0.2366 - 6s/epoch - 149ms/step\n",
      "Epoch 10/50\n",
      "40/40 - 6s - loss: 0.2318 - mse: 0.2318 - 6s/epoch - 150ms/step\n",
      "Epoch 11/50\n",
      "40/40 - 6s - loss: 0.2085 - mse: 0.2085 - 6s/epoch - 152ms/step\n",
      "Epoch 12/50\n",
      "40/40 - 6s - loss: 0.1938 - mse: 0.1938 - 6s/epoch - 153ms/step\n",
      "Epoch 13/50\n",
      "40/40 - 6s - loss: 0.1747 - mse: 0.1747 - 6s/epoch - 154ms/step\n",
      "Epoch 14/50\n",
      "40/40 - 6s - loss: 0.1622 - mse: 0.1622 - 6s/epoch - 155ms/step\n",
      "Epoch 15/50\n",
      "40/40 - 6s - loss: 0.1516 - mse: 0.1516 - 6s/epoch - 156ms/step\n",
      "Epoch 16/50\n",
      "40/40 - 7s - loss: 0.1402 - mse: 0.1402 - 7s/epoch - 182ms/step\n",
      "Epoch 17/50\n",
      "40/40 - 7s - loss: 0.1303 - mse: 0.1303 - 7s/epoch - 180ms/step\n",
      "Epoch 18/50\n",
      "40/40 - 6s - loss: 0.1214 - mse: 0.1214 - 6s/epoch - 161ms/step\n",
      "Epoch 19/50\n",
      "40/40 - 7s - loss: 0.1103 - mse: 0.1103 - 7s/epoch - 163ms/step\n",
      "Epoch 20/50\n",
      "40/40 - 7s - loss: 0.1611 - mse: 0.1611 - 7s/epoch - 166ms/step\n",
      "Epoch 21/50\n",
      "40/40 - 7s - loss: 0.1343 - mse: 0.1343 - 7s/epoch - 164ms/step\n",
      "Epoch 22/50\n",
      "40/40 - 6s - loss: 0.0919 - mse: 0.0919 - 6s/epoch - 161ms/step\n",
      "Epoch 23/50\n",
      "40/40 - 7s - loss: 0.0865 - mse: 0.0865 - 7s/epoch - 170ms/step\n",
      "Epoch 24/50\n",
      "40/40 - 7s - loss: 0.0835 - mse: 0.0835 - 7s/epoch - 166ms/step\n",
      "Epoch 25/50\n",
      "40/40 - 8s - loss: 0.0733 - mse: 0.0733 - 8s/epoch - 206ms/step\n",
      "Epoch 26/50\n",
      "40/40 - 7s - loss: 0.0713 - mse: 0.0713 - 7s/epoch - 187ms/step\n",
      "Epoch 27/50\n",
      "40/40 - 7s - loss: 0.0634 - mse: 0.0634 - 7s/epoch - 177ms/step\n",
      "Epoch 28/50\n",
      "40/40 - 7s - loss: 0.0607 - mse: 0.0607 - 7s/epoch - 167ms/step\n",
      "Epoch 29/50\n",
      "40/40 - 7s - loss: 0.0580 - mse: 0.0580 - 7s/epoch - 167ms/step\n",
      "Epoch 30/50\n",
      "40/40 - 7s - loss: 0.0543 - mse: 0.0543 - 7s/epoch - 172ms/step\n",
      "Epoch 31/50\n",
      "40/40 - 7s - loss: 0.0586 - mse: 0.0586 - 7s/epoch - 176ms/step\n",
      "Epoch 32/50\n",
      "40/40 - 7s - loss: 0.0450 - mse: 0.0450 - 7s/epoch - 187ms/step\n",
      "Epoch 33/50\n",
      "40/40 - 7s - loss: 0.0465 - mse: 0.0465 - 7s/epoch - 185ms/step\n",
      "Epoch 34/50\n",
      "40/40 - 7s - loss: 0.0438 - mse: 0.0438 - 7s/epoch - 177ms/step\n",
      "Epoch 35/50\n",
      "40/40 - 7s - loss: 0.0361 - mse: 0.0361 - 7s/epoch - 169ms/step\n",
      "Epoch 36/50\n",
      "40/40 - 7s - loss: 0.0374 - mse: 0.0374 - 7s/epoch - 169ms/step\n",
      "Epoch 37/50\n",
      "40/40 - 7s - loss: 0.0512 - mse: 0.0512 - 7s/epoch - 171ms/step\n",
      "Epoch 38/50\n",
      "40/40 - 7s - loss: 0.0331 - mse: 0.0331 - 7s/epoch - 172ms/step\n",
      "Epoch 39/50\n",
      "40/40 - 7s - loss: 0.0270 - mse: 0.0270 - 7s/epoch - 169ms/step\n",
      "Epoch 40/50\n",
      "40/40 - 7s - loss: 0.0289 - mse: 0.0289 - 7s/epoch - 169ms/step\n",
      "Epoch 41/50\n",
      "40/40 - 7s - loss: 0.0273 - mse: 0.0273 - 7s/epoch - 168ms/step\n",
      "Epoch 42/50\n",
      "40/40 - 7s - loss: 0.0223 - mse: 0.0223 - 7s/epoch - 186ms/step\n",
      "Epoch 43/50\n",
      "40/40 - 7s - loss: 0.0222 - mse: 0.0222 - 7s/epoch - 173ms/step\n",
      "Epoch 44/50\n",
      "40/40 - 7s - loss: 0.0194 - mse: 0.0194 - 7s/epoch - 174ms/step\n",
      "Epoch 45/50\n",
      "40/40 - 7s - loss: 0.0212 - mse: 0.0212 - 7s/epoch - 173ms/step\n",
      "Epoch 46/50\n",
      "40/40 - 7s - loss: 0.0185 - mse: 0.0185 - 7s/epoch - 174ms/step\n",
      "Epoch 47/50\n",
      "40/40 - 7s - loss: 0.0170 - mse: 0.0170 - 7s/epoch - 170ms/step\n",
      "Epoch 48/50\n",
      "40/40 - 7s - loss: 0.0176 - mse: 0.0176 - 7s/epoch - 169ms/step\n",
      "Epoch 49/50\n",
      "40/40 - 7s - loss: 0.0132 - mse: 0.0132 - 7s/epoch - 171ms/step\n",
      "Epoch 50/50\n",
      "40/40 - 7s - loss: 0.0127 - mse: 0.0127 - 7s/epoch - 175ms/step\n",
      "MSE = [0.6322012543678284, 0.6322012543678284]\n"
     ]
    }
   ],
   "source": [
    "alzCNN4 = tf.keras.Sequential()\n",
    "    \n",
    "alzCNN4.add(tf.keras.layers.Resizing(height = 52, width = 44))\n",
    "\n",
    "alzCNN4.add(tf.keras.layers.Conv2D(filters=6,\n",
    "                                kernel_size=(3, 3),\n",
    "                                activation='relu',\n",
    "                                input_shape=(52, 44, 1)))\n",
    "\n",
    "alzCNN4.add(tf.keras.layers.Conv2D(filters=12,\n",
    "                                kernel_size=(3, 3),\n",
    "                                activation='relu'))\n",
    "    \n",
    "alzCNN4.add(tf.keras.layers.Conv2D(filters=16,\n",
    "                                kernel_size=(5, 5),\n",
    "                                activation='relu'))\n",
    "\n",
    "alzCNN4.add(tf.keras.layers.AveragePooling2D())\n",
    "\n",
    "alzCNN4.add(tf.keras.layers.Flatten())\n",
    "\n",
    "alzCNN4.add(tf.keras.layers.Dense(units=40,\n",
    "                    activation='relu'))\n",
    "alzCNN4.add(tf.keras.layers.Dense(units=20,\n",
    "                    activation='relu'))\n",
    "alzCNN4.add(tf.keras.layers.Dense(units=1,\n",
    "                    activation='linear'))\n",
    "    \n",
    "alzCNN4.compile(loss='mse',\n",
    "            optimizer='adam',\n",
    "            metrics=['mse'])\n",
    "    \n",
    "alzCNN4.fit(X_trainOver2, Y_trainOver,\n",
    "            batch_size=256,\n",
    "            epochs=50,\n",
    "            verbose=2,)\n",
    "    \n",
    "score = alzCNN4.evaluate(X_testOver2, Y_testOver2, verbose=0)\n",
    "print(f'MSE = {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2631e272",
   "metadata": {},
   "source": [
    "Making an entirely new oversampler gives me an MSE of 0.632, again a bit higher than with the default test set. I next tried running a neural network on the normal test set, but with 30 epochs instead of 50 to see if reducing the empoch number reduced overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8aef568e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "40/40 - 6s - loss: 2.2851 - mse: 2.2851 - 6s/epoch - 153ms/step\n",
      "Epoch 2/30\n",
      "40/40 - 6s - loss: 1.2671 - mse: 1.2671 - 6s/epoch - 139ms/step\n",
      "Epoch 3/30\n",
      "40/40 - 6s - loss: 0.9311 - mse: 0.9311 - 6s/epoch - 139ms/step\n",
      "Epoch 4/30\n",
      "40/40 - 6s - loss: 0.5625 - mse: 0.5625 - 6s/epoch - 140ms/step\n",
      "Epoch 5/30\n",
      "40/40 - 6s - loss: 0.4058 - mse: 0.4058 - 6s/epoch - 143ms/step\n",
      "Epoch 6/30\n",
      "40/40 - 6s - loss: 0.3729 - mse: 0.3729 - 6s/epoch - 146ms/step\n",
      "Epoch 7/30\n",
      "40/40 - 6s - loss: 0.3414 - mse: 0.3414 - 6s/epoch - 148ms/step\n",
      "Epoch 8/30\n",
      "40/40 - 6s - loss: 0.3127 - mse: 0.3127 - 6s/epoch - 151ms/step\n",
      "Epoch 9/30\n",
      "40/40 - 6s - loss: 0.2918 - mse: 0.2918 - 6s/epoch - 151ms/step\n",
      "Epoch 10/30\n",
      "40/40 - 6s - loss: 0.2866 - mse: 0.2866 - 6s/epoch - 153ms/step\n",
      "Epoch 11/30\n",
      "40/40 - 7s - loss: 0.2500 - mse: 0.2500 - 7s/epoch - 171ms/step\n",
      "Epoch 12/30\n",
      "40/40 - 7s - loss: 0.2412 - mse: 0.2412 - 7s/epoch - 163ms/step\n",
      "Epoch 13/30\n",
      "40/40 - 7s - loss: 0.2280 - mse: 0.2280 - 7s/epoch - 164ms/step\n",
      "Epoch 14/30\n",
      "40/40 - 6s - loss: 0.2187 - mse: 0.2187 - 6s/epoch - 160ms/step\n",
      "Epoch 15/30\n",
      "40/40 - 6s - loss: 0.2052 - mse: 0.2052 - 6s/epoch - 161ms/step\n",
      "Epoch 16/30\n",
      "40/40 - 7s - loss: 0.2023 - mse: 0.2023 - 7s/epoch - 163ms/step\n",
      "Epoch 17/30\n",
      "40/40 - 7s - loss: 0.1833 - mse: 0.1833 - 7s/epoch - 164ms/step\n",
      "Epoch 18/30\n",
      "40/40 - 6s - loss: 0.1717 - mse: 0.1717 - 6s/epoch - 162ms/step\n",
      "Epoch 19/30\n",
      "40/40 - 7s - loss: 0.1710 - mse: 0.1710 - 7s/epoch - 163ms/step\n",
      "Epoch 20/30\n",
      "40/40 - 7s - loss: 0.1638 - mse: 0.1638 - 7s/epoch - 169ms/step\n",
      "Epoch 21/30\n",
      "40/40 - 7s - loss: 0.1540 - mse: 0.1540 - 7s/epoch - 167ms/step\n",
      "Epoch 22/30\n",
      "40/40 - 7s - loss: 0.1423 - mse: 0.1423 - 7s/epoch - 164ms/step\n",
      "Epoch 23/30\n",
      "40/40 - 7s - loss: 0.1375 - mse: 0.1375 - 7s/epoch - 179ms/step\n",
      "Epoch 24/30\n",
      "40/40 - 7s - loss: 0.1338 - mse: 0.1338 - 7s/epoch - 182ms/step\n",
      "Epoch 25/30\n",
      "40/40 - 7s - loss: 0.1318 - mse: 0.1318 - 7s/epoch - 170ms/step\n",
      "Epoch 26/30\n",
      "40/40 - 7s - loss: 0.1192 - mse: 0.1192 - 7s/epoch - 170ms/step\n",
      "Epoch 27/30\n",
      "40/40 - 7s - loss: 0.1164 - mse: 0.1164 - 7s/epoch - 172ms/step\n",
      "Epoch 28/30\n",
      "40/40 - 7s - loss: 0.1059 - mse: 0.1059 - 7s/epoch - 187ms/step\n",
      "Epoch 29/30\n",
      "40/40 - 7s - loss: 0.1031 - mse: 0.1031 - 7s/epoch - 173ms/step\n",
      "Epoch 30/30\n",
      "40/40 - 7s - loss: 0.1047 - mse: 0.1047 - 7s/epoch - 168ms/step\n",
      "MSE = [0.5361826419830322, 0.5361826419830322]\n"
     ]
    }
   ],
   "source": [
    "alzCNN5 = tf.keras.Sequential()\n",
    "    \n",
    "alzCNN5.add(tf.keras.layers.Resizing(height = 52, width = 44))\n",
    "\n",
    "alzCNN5.add(tf.keras.layers.Conv2D(filters=6,\n",
    "                                kernel_size=(3, 3),\n",
    "                                activation='relu',\n",
    "                                input_shape=(52, 44, 1)))\n",
    "\n",
    "alzCNN5.add(tf.keras.layers.Conv2D(filters=12,\n",
    "                                kernel_size=(3, 3),\n",
    "                                activation='relu'))\n",
    "    \n",
    "alzCNN5.add(tf.keras.layers.Conv2D(filters=16,\n",
    "                                kernel_size=(5, 5),\n",
    "                                activation='relu'))\n",
    "\n",
    "alzCNN5.add(tf.keras.layers.AveragePooling2D())\n",
    "\n",
    "alzCNN5.add(tf.keras.layers.Flatten())\n",
    "\n",
    "alzCNN5.add(tf.keras.layers.Dense(units=40,\n",
    "                    activation='relu'))\n",
    "alzCNN5.add(tf.keras.layers.Dense(units=20,\n",
    "                    activation='relu'))\n",
    "alzCNN5.add(tf.keras.layers.Dense(units=1,\n",
    "                    activation='linear'))\n",
    "    \n",
    "alzCNN5.compile(loss='mse',\n",
    "            optimizer='adam',\n",
    "            metrics=['mse'])\n",
    "    \n",
    "alzCNN5.fit(X_trainOver2, Y_trainOver,\n",
    "            batch_size=256,\n",
    "            epochs=30,\n",
    "            verbose=2,)\n",
    "    \n",
    "score = alzCNN5.evaluate(X_test, Y_test, verbose=0)\n",
    "print(f'MSE = {score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d3a6c5",
   "metadata": {},
   "source": [
    "It seems decreasing the epochs doesn't help: I get a MSE of 0.536, which is higher than I got with 50 epochs and the same settings.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "While I did perform significantly better than the variance on the test data, this is still not ideal due to the large amount of overfitting in spite of using a kFold strategy to avoid overfitting. This may be due to the very small number of images in the data sets of more severe classes of Alzheimer's - while oversampling avoids the system not knowing how to categorize them after all, it is still very likely that, for instance, the 52 moderate Alzheimer's cases in the training data and the oversampled examples created from those 52 differ noticeably from the 12 in the training set, and the oversampling would in fact contribute further to overfit (as while the folds are supposed to show you how the model will perform when encountering entirely new data, since the oversamples are created from the original data it is NOT entirely new data, and is thus not indicative of the performance on the test set). If I had more time, I would want to not use oversampling and instead use a custom loss function that weighs larger numbers in the regression more heavily, to avoid this problem (I would have to use a custom loss function because the only inbuilt loss function that accounts for imbalance in the data, the focal loss, is classification-based, and making this a classification problem rather than regression would ignore that the \"classes\" are a gradient of severity rather than being in no particular order)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
